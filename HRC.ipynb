{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:42:40.149158Z",
     "start_time": "2025-01-22T00:42:39.239988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# General modules\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Language models\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "client = openai.Client()\n",
    "\n",
    "chat_model_name = 'gpt-4o-mini'\n",
    "embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "\n",
    "# Paths\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC'\n",
    "steam_title = 'Community'\n",
    "\n",
    "\n",
    "\n",
    "path_input = os.path.join(root_dir, steam_title, \"Transcript_pinehaven_stream.txt\")\n",
    "path_db_prepared = os.path.join(root_dir, steam_title, \"db_prepared.json\")\n",
    "path_db_translated = os.path.join(root_dir, steam_title, \"db_translated.json\")\n",
    "path_db_analysed = os.path.join(root_dir, steam_title, \"db_analysed.json\")\n",
    "path_db_embedded = os.path.join(root_dir, steam_title, \"db_embedded.json\")\n",
    "path_db_clustered = os.path.join(root_dir, steam_title, \"db_clustered.json\")\n",
    "path_db_final = os.path.join(root_dir, steam_title, \"db_final.json\")"
   ],
   "id": "dfc8a8587566289e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:57:18.859586Z",
     "start_time": "2025-01-22T00:57:18.856586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helper.utils import *\n",
    "\n",
    "configure_api(client, chat_model_name)"
   ],
   "id": "799973c4c41da7b8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transcript Preparation",
   "id": "f83f2c921bee4f35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the transcript\n",
    "with open(path_input, 'r', encoding='utf-8') as file:\n",
    "    transcript = file.readlines()"
   ],
   "id": "afed59bedab40cd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# concatenate every 100 lines + get rid of '\\n'\n",
    "\n",
    "transcript_joined = []\n",
    "for i in range(0, len(transcript), 40):\n",
    "    transcript_joined.append(' '.join([line.strip() for line in transcript[i:i+100]]))\n"
   ],
   "id": "e71c4d258aff18c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(transcript_joined)",
   "id": "baa94793cfceca2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(transcript_joined[5])",
   "id": "92d8de7739c44733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis",
   "id": "9a4d0b31b6f052f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.utils import *\n",
    "from helper.prompt_templates import *\n",
    "\n",
    "configure_api(client, chat_model_name)\n",
    "\n",
    "all_entries = []\n",
    "\n",
    "for i in range(0, len(transcript_joined)):\n",
    "    logger.info(f\"Processing text {i}\")\n",
    "\n",
    "    transcript = transcript_joined[i]\n",
    "\n",
    "    prompt_influencer = prompt_template_influencer.format(transcript=transcript)\n",
    "    response = api_settings[\"client\"].chat.completions.create(\n",
    "        model=api_settings[\"model\"],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in extracting video game topics from Youtube Transcripts.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_influencer},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # Dynamically handle varying keys at the root of the response\n",
    "    if isinstance(response_json, dict):\n",
    "        for key, value in response_json.items():\n",
    "            if isinstance(value, list):  # Ensure the value is a list\n",
    "                all_entries.extend(value)\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected format for key '{key}' in response {i}\")\n",
    "    else:\n",
    "        logger.warning(f\"Unexpected response structure for text {i}: {response_json}\")\n",
    "\n",
    "# save the entries\n",
    "with open(path_db_prepared, \"w\") as output_file:\n",
    "    json.dump(all_entries, output_file, indent=4)"
   ],
   "id": "50d2353ad353e5c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# generat unique ID\n",
    "\n",
    "from helper.utils import *\n",
    "\n",
    "# A unique ID is generated in the new column / key \"response_ID\"\n",
    "data = read_json(path_db_prepared)\n",
    "data_prepared = generate_ID(data)\n",
    "save_to_json(data_prepared, path_db_prepared)"
   ],
   "id": "1f236c93e32c4cb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentiment Analysis",
   "id": "e9eadf2c98e80fc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_entries = []\n",
    "data_prepared = read_json(path_db_prepared)\n",
    "\n",
    "for i in range(0, len(data_prepared)):\n",
    "    entry = data_prepared[i]\n",
    "    logger.info(f\"Process Sentiment for text {i}\")\n",
    "    try:\n",
    "        prompt_sentiment = prompt_template_sentiment.format(\n",
    "            review=entry[\"Context\"], topic=entry[\"Topic\"]\n",
    "        )\n",
    "        response = api_settings[\"client\"].chat.completions.create(\n",
    "            model=api_settings[\"model\"],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert for sentiment analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_sentiment},\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        sentiment = response.choices[0].message.content.strip()\n",
    "        # rename keys\n",
    "        entry[\"topic\"] = entry[\"Topic\"]\n",
    "        entry[\"sentiment\"] = sentiment\n",
    "        entry[\"category\"] = entry[\"Category\"]\n",
    "        entry[\"sentence\"] = entry[\"Context\"]\n",
    "        entry.pop(\"Context\")\n",
    "        entry.pop(\"Category\")\n",
    "        entry.pop(\"Topic\")\n",
    "\n",
    "        all_entries.append(entry)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing sentiment for topic '{entry['Topic']}' (Entry ID {entry['response_ID']}): {e}\")\n",
    "        raise\n",
    "\n"
   ],
   "id": "d8acd2d7c6c1f9f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the entries\n",
    "with open(path_db_analysed, \"w\") as output_file:\n",
    "    json.dump(all_entries, output_file, indent=4)"
   ],
   "id": "137b73b925f6e3f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding",
   "id": "550e6761353bf508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.embedding import *\n",
    "\n",
    "embed_key = \"topic\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "data = read_json(path_db_analysed)\n",
    "embed_model = initialize_embedding_model(embed_model_name)\n",
    "\n",
    "def process_embedding(data, embed_key):\n",
    "    for i in range(0, len(data)):\n",
    "        if i % 10 == 0:\n",
    "            logger.info(f\"Processing entry {i}\")\n",
    "        entry = data[i]\n",
    "        text = entry[embed_key]\n",
    "        embedding = embed_text(text, embed_model)\n",
    "        entry[\"embedding\"] = embedding\n",
    "    return data\n",
    "\n",
    "data_embedded = process_embedding(data, embed_key)\n",
    "\n",
    "# Save the embedded data\n",
    "with open(path_db_embedded, \"w\") as output_file:\n",
    "    json.dump(data_embedded, output_file, indent=4)\n"
   ],
   "id": "7e69d2b853315b0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering",
   "id": "2b8f576120da7fa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.cluster_analysis import *\n",
    "\n",
    "# Adjustable parameters\n",
    "dimensionality_methods = ['UMAP','PCA', 'tSNE']\n",
    "hdbscan_params = {\"min_cluster_size\": 7, \"min_samples\": 2, \"cluster_selection_epsilon\": 0.4}\n",
    "\n",
    "# Load data\n",
    "df_total = load_embedded_data(path_db_embedded)\n",
    "mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "# Apply HDBSCAN\n",
    "df_total = apply_hdbscan(\n",
    "    df_total,\n",
    "    mat,\n",
    "    dimensionality_methods,\n",
    "    hdbscan_params=hdbscan_params,\n",
    "    include_2d=True,\n",
    "    include_3d=True\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_df_as_json(df_total, path_db_clustered)"
   ],
   "id": "944d01d2bd2a71f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster Naming",
   "id": "14f0ce0528f48d48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.cluster_naming import *\n",
    "\n",
    "# Parameters\n",
    "dimensionality_methods = [\"UMAP\",'PCA', \"tSNE\"]\n",
    "clustering_algorithms = [\"hdbscan\"]  # No KMeans here\n",
    "max_centers = 10\n",
    "\n",
    "#kmeans_clusters = [15, 20, 25, 50]  # Number of clusters for KMeans\n",
    "\n",
    "# Load data\n",
    "df_total = load_json_into_df(path_db_clustered)\n",
    "\n",
    "# Process clusters and generate names\n",
    "df_total = process_clusters(\n",
    "    df_total,\n",
    "    dimensionality_methods,\n",
    "    clustering_algorithms,\n",
    "    max_centers,\n",
    "    api_settings) # insert kmeans_clusters in the function when needed\n",
    "\n",
    "\n",
    "# Save results\n",
    "save_data_for_streamlit(df_total, path_db_final)"
   ],
   "id": "d4bc70643727aec2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HRC Steam reviews",
   "id": "8d05f207f0403fdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:50:17.715447Z",
     "start_time": "2025-01-22T00:50:17.535222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# General modules\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "client = openai.Client()\n",
    "\n",
    "chat_model_name = 'gpt-4o-mini'\n",
    "embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Paths\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC'\n",
    "steam_title = 'Community'\n",
    "\n",
    "# Paths\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC'\n",
    "steam_title = 'Steam'\n",
    "\n",
    "\n",
    "\n",
    "path_db_prepared = os.path.join(root_dir, steam_title, \"db_prepared.json\")\n",
    "path_db_translated = os.path.join(root_dir, steam_title, \"db_translated.json\")\n",
    "path_db_analysed = os.path.join(root_dir, steam_title, \"db_analysed.json\")\n",
    "path_db_embedded = os.path.join(root_dir, steam_title, \"db_embedded.json\")\n",
    "path_db_clustered = os.path.join(root_dir, steam_title, \"db_clustered.json\")\n",
    "path_db_final = os.path.join(root_dir, steam_title, \"db_final.json\")"
   ],
   "id": "e7a316bc3d948d3f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:57:32.034089Z",
     "start_time": "2025-01-22T00:57:32.030861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helper.utils import *\n",
    "\n",
    "configure_api(client, chat_model_name)"
   ],
   "id": "50e700f1c9aed619",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Redshift query",
   "id": "97922a3bd896f454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:50:32.888418Z",
     "start_time": "2025-01-21T20:50:19.145246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# My imports\n",
    "from helper.redshift_conector_standalone import *\n",
    "\n",
    "# https://store.steampowered.com/app/1166860/Rival_Stars_Horse_Racing_Desktop_Edition/\n",
    "\n",
    "# SQL Query Redshift\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM steam_review\n",
    "where app_id_name = '1166860_Rival_Stars_Horse_Racing_Desktop_Edition'\n",
    "\"\"\"\n",
    "logger.info(f\"Query Redshift with: {sql_query}\")\n",
    "\n",
    "try:\n",
    "    results_json, results_df = fetch_query_results(sql_query)\n",
    "    # Print the first row of the DataFrame\n",
    "    logger.info(\"Successfully fetched query results, with shape: %s\", results_df.shape)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error fetching query results: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save the json\n",
    "parsed_json = json.loads(results_json)\n",
    "\n",
    "# 2) Then pretty-print with indentation\n",
    "save_to_json(parsed_json, path_db_prepared)"
   ],
   "id": "d5a8d3b0805d2c5c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 09:50:19,868 - INFO - Query Redshift with: \n",
      "SELECT *\n",
      "FROM steam_review\n",
      "where app_id_name = '1166860_Rival_Stars_Horse_Racing_Desktop_Edition'\n",
      "\n",
      "2025-01-22 09:50:32,716 - INFO - Successfully fetched query results, with shape: (3235, 14)\n",
      "2025-01-22 09:50:32,886 - INFO - Data successfully saved to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\Steam\\db_prepared.json\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Translation",
   "id": "583f05073249d498"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.data_analysis import translate_reviews\n",
    "\n",
    "id_column = \"recommendationid\"              # The column that contains unique IDs\n",
    "text_col = \"review_text\"                    # The column that contains the text to be translated\n",
    "language_col = \"language\"                   # The column that contains the language tag\n",
    "\n",
    "data_translated = translate_reviews(df=results_df,\n",
    "                                    file_path=path_db_translated,\n",
    "                                    id_column=id_column,\n",
    "                                    text_column=text_col,\n",
    "                                    language_column='language')\n",
    "\n",
    "# Save the translated data\n",
    "save_df_as_json(data_translated, path_db_translated)"
   ],
   "id": "771f651d87e073ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Analysis",
   "id": "29079ec413d0653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from helper.utils import *\n",
    "from helper.prompt_templates import *\n",
    "from helper.data_analysis import normalize_topics_key, process_entry\n",
    "\n",
    "# Configure API\n",
    "configure_api(client, chat_model_name)\n",
    "\n",
    "data_prepared = read_json(path_db_translated)\n",
    "\n",
    "id_column = \"recommendationid\"              # The column that contains unique IDs\n",
    "columns_of_interest = [\"review_text\"]       # The column(s) that are going to be analyzed\n",
    "all_entries = []                            # List to store all processed entries\n",
    "processed_ids = set()                       # Set to store IDs of processed entries\n",
    "\n",
    "# If the analyzed file already exists, load it\n",
    "if os.path.exists(path_db_analysed):\n",
    "    all_entries = read_json(path_db_analysed)\n",
    "    processed_ids = {entry[id_column] for entry in all_entries}  # set for O(1) membership checks\n",
    "\n",
    "# Process all unprocessed entries\n",
    "for i, entry in enumerate(data_prepared):\n",
    "    current_id = entry[id_column]\n",
    "\n",
    "    # If we've already processed this entry, skip it\n",
    "    if current_id in processed_ids:\n",
    "        logger.info(f\"Skipping entry {i} (ID: {current_id}) - already processed.\")\n",
    "        continue\n",
    "\n",
    "    # Otherwise, process and append\n",
    "    process_entry(\n",
    "        entry,\n",
    "        id_column,\n",
    "        prompt_template_topic,\n",
    "        prompt_template_sentiment,\n",
    "        api_settings,\n",
    "        columns_of_interest\n",
    "    )\n",
    "    all_entries.append(entry)\n",
    "    processed_ids.add(current_id)  # mark as processed\n",
    "\n",
    "    # Save intermediate progress every 10 entries\n",
    "    if (i % 10) == 0 and i != 0:\n",
    "        save_to_json(all_entries, path_db_analysed)\n",
    "        logger.info(f\"Progress saved at index {i}.\")\n",
    "\n",
    "# Final save after the loop\n",
    "save_to_json(all_entries, path_db_analysed)\n",
    "logger.info(\"All entries processed and final results saved.\")\n"
   ],
   "id": "c7011c034c5be72c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding",
   "id": "b2c091c74428d8c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = read_json(path_db_analysed)",
   "id": "72c7a3ea4cb7f0e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data[0]['topics']",
   "id": "836e44abe4f659d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.embedding import *\n",
    "\n",
    "embed_key = \"topic\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "data = read_json(path_db_analysed)\n",
    "embed_model = initialize_embedding_model(embed_model_name)\n",
    "\n",
    "def process_embedding(data, embed_key):\n",
    "    for i in range(0, len(data)):\n",
    "        if i % 10 == 0:\n",
    "            logger.info(f\"Processing entry {i}\")\n",
    "\n",
    "        for d_topic in data[i][\"topics\"]:\n",
    "            if isinstance(d_topic, dict):\n",
    "                d_topic[\"embedding\"] = embed_text(d_topic[embed_key], embed_model)\n",
    "    return data\n",
    "\n",
    "data_embedded = process_embedding(data, embed_key)\n",
    "\n",
    "# Flatten\n",
    "def flatten_data(data):\n",
    "    flattened = []\n",
    "    for entry in data:\n",
    "        base_copy = dict(entry)\n",
    "        topics = base_copy.pop(\"topics\", [])\n",
    "\n",
    "        for topic in topics:\n",
    "            new_entry = dict(base_copy)\n",
    "            new_entry.update(topic)\n",
    "            flattened.append(new_entry)\n",
    "    return flattened\n",
    "\n",
    "data_flattened = flatten_data(data_embedded)\n",
    "\n",
    "\n",
    "# Save the embedded data\n",
    "with open(path_db_embedded, \"w\") as output_file:\n",
    "    json.dump(data_flattened, output_file, indent=4)\n"
   ],
   "id": "8188d5fddf552289"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering",
   "id": "1d4735db2113350d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:46:19.833831Z",
     "start_time": "2025-01-22T00:45:15.064879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helper.cluster_analysis import *\n",
    "from helper.utils import *\n",
    "\n",
    "# Adjustable parameters\n",
    "dimensionality_methods = ['UMAP','PCA', 'tSNE']\n",
    "hdbscan_params = {\"min_cluster_size\": 30, \"min_samples\": 15, \"cluster_selection_epsilon\": 0.4}\n",
    "\n",
    "# Load data\n",
    "df_total = load_embedded_data(path_db_embedded)\n",
    "mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "# Apply HDBSCAN\n",
    "df_total = apply_hdbscan(\n",
    "    df_total,\n",
    "    mat,\n",
    "    dimensionality_methods,\n",
    "    hdbscan_params=hdbscan_params,\n",
    "    include_2d=True,\n",
    "    include_3d=True\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_df_as_json(df_total, path_db_clustered)"
   ],
   "id": "8b66465c56b175c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-22 13:45:19,273 - INFO - Loading data from S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\Steam\\db_embedded.json\n",
      "2025-01-22 13:45:20,671 - INFO - Loaded 5797 valid entries with embeddings.\n",
      "2025-01-22 13:45:20,734 - INFO - Applying HDBSCAN with: {'min_cluster_size': 30, 'min_samples': 15, 'cluster_selection_epsilon': 0.4}\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-01-22 13:45:27,849 - INFO - Found 35 clusters.\n",
      "2025-01-22 13:45:27,849 - INFO - Applying UMAP with 2 components.\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-01-22 13:45:42,677 - INFO - Applying UMAP with 3 components.\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-01-22 13:45:43,879 - INFO - Applying PCA with 2 components.\n",
      "2025-01-22 13:45:43,943 - INFO - Applying PCA with 3 components.\n",
      "2025-01-22 13:45:44,002 - INFO - Applying tSNE with 2 components.\n",
      "2025-01-22 13:45:44,002 - INFO - Perplexity not provided, setting to 30 based on sample size.\n",
      "2025-01-22 13:45:53,389 - INFO - Applying tSNE with 3 components.\n",
      "2025-01-22 13:45:53,391 - INFO - Perplexity not provided, setting to 30 based on sample size.\n",
      "2025-01-22 13:46:13,471 - INFO - HDBSCAN clustering and dimensionality reduction completed.\n",
      "2025-01-22 13:46:13,471 - INFO - Saving data to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\Steam\\db_clustered.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster Naming",
   "id": "d4dc85b0197b8122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T00:58:34.222598Z",
     "start_time": "2025-01-22T00:58:11.453232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helper.cluster_naming import *\n",
    "\n",
    "# Parameters\n",
    "dimensionality_methods = [\"UMAP\",'PCA', \"tSNE\"]\n",
    "clustering_algorithms = [\"hdbscan\"]  # No KMeans here\n",
    "max_centers = 10\n",
    "api_settings = {\"client\": client, \"model\": chat_model_name}\n",
    "\n",
    "#kmeans_clusters = [15, 20, 25, 50]  # Number of clusters for KMeans\n",
    "\n",
    "# Load data\n",
    "df_total = load_json_into_df(path_db_clustered)\n",
    "\n",
    "df_total = process_clusters(\n",
    "    df_total,\n",
    "    dimensionality_methods,\n",
    "    clustering_algorithms,\n",
    "    max_centers,\n",
    "    api_settings) # insert kmeans_clusters in the function when needed\n",
    "\n",
    "\n",
    "# Save results\n",
    "save_data_for_streamlit(df_total, path_db_final)"
   ],
   "id": "37ae3f861d8411c5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 13:58:11,455 - INFO - Loading data from S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\Steam\\db_clustered.json\n",
      "2025-01-22 13:58:12,909 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 28\n",
      "2025-01-22 13:58:14,039 - INFO - Generated cluster name: Game Enthusiasm and Enjoyment\n",
      "2025-01-22 13:58:14,040 - INFO - Tokens used so far: Prompt Tokens: 144, Completion Tokens: 7\n",
      "2025-01-22 13:58:14,044 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 27\n",
      "2025-01-22 13:58:14,536 - INFO - Generated cluster name: Equestrian Game Enthusiasm\n",
      "2025-01-22 13:58:14,537 - INFO - Tokens used so far: Prompt Tokens: 315, Completion Tokens: 13\n",
      "2025-01-22 13:58:14,540 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 5\n",
      "2025-01-22 13:58:15,264 - INFO - Generated cluster name: Customization in Equestrian Gameplay\n",
      "2025-01-22 13:58:15,266 - INFO - Tokens used so far: Prompt Tokens: 508, Completion Tokens: 20\n",
      "2025-01-22 13:58:15,271 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 24\n",
      "2025-01-22 13:58:15,818 - INFO - Generated cluster name: Value Perception of Video Games\n",
      "2025-01-22 13:58:15,819 - INFO - Tokens used so far: Prompt Tokens: 709, Completion Tokens: 27\n",
      "2025-01-22 13:58:15,824 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 8\n",
      "2025-01-22 13:58:16,360 - INFO - Generated cluster name: PC vs. Mobile Gaming Preferences\n",
      "2025-01-22 13:58:16,361 - INFO - Tokens used so far: Prompt Tokens: 906, Completion Tokens: 33\n",
      "2025-01-22 13:58:16,367 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 17\n",
      "2025-01-22 13:58:16,876 - INFO - Generated cluster name: Better Than Mobile Experience\n",
      "2025-01-22 13:58:16,877 - INFO - Tokens used so far: Prompt Tokens: 1102, Completion Tokens: 38\n",
      "2025-01-22 13:58:16,881 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 0\n",
      "2025-01-22 13:58:17,519 - INFO - Generated cluster name: Horse Lovers' Gaming Paradise\n",
      "2025-01-22 13:58:17,523 - INFO - Tokens used so far: Prompt Tokens: 1290, Completion Tokens: 43\n",
      "2025-01-22 13:58:17,532 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 30\n",
      "2025-01-22 13:58:18,012 - INFO - Generated cluster name: Horse Breeding Features and Fun\n",
      "2025-01-22 13:58:18,014 - INFO - Tokens used so far: Prompt Tokens: 1477, Completion Tokens: 50\n",
      "2025-01-22 13:58:18,019 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 16\n",
      "2025-01-22 13:58:18,756 - INFO - Generated cluster name: Horse Racing Experience and Engagement\n",
      "2025-01-22 13:58:18,757 - INFO - Tokens used so far: Prompt Tokens: 1714, Completion Tokens: 56\n",
      "2025-01-22 13:58:18,762 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 19\n",
      "2025-01-22 13:58:19,291 - INFO - Generated cluster name: Outstanding Game Experience\n",
      "2025-01-22 13:58:19,294 - INFO - Tokens used so far: Prompt Tokens: 1883, Completion Tokens: 60\n",
      "2025-01-22 13:58:19,304 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 11\n",
      "2025-01-22 13:58:19,832 - INFO - Generated cluster name: \"Visually Stunning Horse Games\"\n",
      "2025-01-22 13:58:19,834 - INFO - Tokens used so far: Prompt Tokens: 2055, Completion Tokens: 68\n",
      "2025-01-22 13:58:19,840 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 18\n",
      "2025-01-22 13:58:20,401 - INFO - Generated cluster name: Addictive Gaming Experiences\n",
      "2025-01-22 13:58:20,401 - INFO - Tokens used so far: Prompt Tokens: 2210, Completion Tokens: 73\n",
      "2025-01-22 13:58:20,404 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 22\n",
      "2025-01-22 13:58:21,028 - INFO - Generated cluster name: Equestrian Game Features and Feedback\n",
      "2025-01-22 13:58:21,030 - INFO - Tokens used so far: Prompt Tokens: 2425, Completion Tokens: 79\n",
      "2025-01-22 13:58:21,038 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 2\n",
      "2025-01-22 13:58:21,510 - INFO - Generated cluster name: Highly Recommended Horse Game\n",
      "2025-01-22 13:58:21,511 - INFO - Tokens used so far: Prompt Tokens: 2578, Completion Tokens: 84\n",
      "2025-01-22 13:58:21,518 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 14\n",
      "2025-01-22 13:58:22,031 - INFO - Generated cluster name: Game Difficulty and Progression\n",
      "2025-01-22 13:58:22,032 - INFO - Tokens used so far: Prompt Tokens: 2789, Completion Tokens: 90\n",
      "2025-01-22 13:58:22,036 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 6\n",
      "2025-01-22 13:58:22,536 - INFO - Generated cluster name: Update Disparities: Mobile vs. PC\n",
      "2025-01-22 13:58:22,536 - INFO - Tokens used so far: Prompt Tokens: 2998, Completion Tokens: 100\n",
      "2025-01-22 13:58:22,539 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 10\n",
      "2025-01-22 13:58:22,998 - INFO - Generated cluster name: Realistic Gaming Experience\n",
      "2025-01-22 13:58:22,998 - INFO - Tokens used so far: Prompt Tokens: 3160, Completion Tokens: 105\n",
      "2025-01-22 13:58:23,002 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 26\n",
      "2025-01-22 13:58:23,520 - INFO - Generated cluster name: Game Pricing and Value Opinions\n",
      "2025-01-22 13:58:23,521 - INFO - Tokens used so far: Prompt Tokens: 3411, Completion Tokens: 111\n",
      "2025-01-22 13:58:23,524 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 23\n",
      "2025-01-22 13:58:24,102 - INFO - Generated cluster name: Relaxing Horse Adventure Experience\n",
      "2025-01-22 13:58:24,104 - INFO - Tokens used so far: Prompt Tokens: 3583, Completion Tokens: 117\n",
      "2025-01-22 13:58:24,109 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 15\n",
      "2025-01-22 13:58:24,643 - INFO - Generated cluster name: Addictive and Engaging Gameplay\n",
      "2025-01-22 13:58:24,643 - INFO - Tokens used so far: Prompt Tokens: 3801, Completion Tokens: 124\n",
      "2025-01-22 13:58:24,646 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 7\n",
      "2025-01-22 13:58:25,236 - INFO - Generated cluster name: Ongoing Game Improvement and Updates\n",
      "2025-01-22 13:58:25,237 - INFO - Tokens used so far: Prompt Tokens: 3997, Completion Tokens: 131\n",
      "2025-01-22 13:58:25,240 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 33\n",
      "2025-01-22 13:58:25,903 - INFO - Generated cluster name: Diverse Horse Breeds and Breeding\n",
      "2025-01-22 13:58:25,905 - INFO - Tokens used so far: Prompt Tokens: 4191, Completion Tokens: 140\n",
      "2025-01-22 13:58:25,915 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 31\n",
      "2025-01-22 13:58:26,588 - INFO - Generated cluster name: Horse Breeding Mechanics and Gameplay\n",
      "2025-01-22 13:58:26,590 - INFO - Tokens used so far: Prompt Tokens: 4405, Completion Tokens: 147\n",
      "2025-01-22 13:58:26,597 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 25\n",
      "2025-01-22 13:58:27,298 - INFO - Generated cluster name: Horse Racing Game Reviews\n",
      "2025-01-22 13:58:27,300 - INFO - Tokens used so far: Prompt Tokens: 4569, Completion Tokens: 152\n",
      "2025-01-22 13:58:27,306 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 3\n",
      "2025-01-22 13:58:27,788 - INFO - Generated cluster name: Game Recommendations & Enthusiasm\n",
      "2025-01-22 13:58:27,790 - INFO - Tokens used so far: Prompt Tokens: 4711, Completion Tokens: 158\n",
      "2025-01-22 13:58:27,796 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 32\n",
      "2025-01-22 13:58:28,244 - INFO - Generated cluster name: Customizable Horse Breeding Experience\n",
      "2025-01-22 13:58:28,247 - INFO - Tokens used so far: Prompt Tokens: 4930, Completion Tokens: 165\n",
      "2025-01-22 13:58:28,252 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 13\n",
      "2025-01-22 13:58:28,730 - INFO - Generated cluster name: Repetitive Gameplay Challenges\n",
      "2025-01-22 13:58:28,731 - INFO - Tokens used so far: Prompt Tokens: 5164, Completion Tokens: 171\n",
      "2025-01-22 13:58:28,737 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 20\n",
      "2025-01-22 13:58:29,308 - INFO - Generated cluster name: Highly Recommended Games\n",
      "2025-01-22 13:58:29,309 - INFO - Tokens used so far: Prompt Tokens: 5339, Completion Tokens: 175\n",
      "2025-01-22 13:58:29,315 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 29\n",
      "2025-01-22 13:58:30,009 - INFO - Generated cluster name: Horse Breeding Systems and Gameplay\n",
      "2025-01-22 13:58:30,011 - INFO - Tokens used so far: Prompt Tokens: 5511, Completion Tokens: 182\n",
      "2025-01-22 13:58:30,017 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 4\n",
      "2025-01-22 13:58:30,750 - INFO - Generated cluster name: Engaging and Wholesome Storylines\n",
      "2025-01-22 13:58:30,753 - INFO - Tokens used so far: Prompt Tokens: 5685, Completion Tokens: 191\n",
      "2025-01-22 13:58:30,760 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 21\n",
      "2025-01-22 13:58:31,190 - INFO - Generated cluster name: Gameplay Variety and Repetition\n",
      "2025-01-22 13:58:31,191 - INFO - Tokens used so far: Prompt Tokens: 5868, Completion Tokens: 197\n",
      "2025-01-22 13:58:31,197 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 12\n",
      "2025-01-22 13:58:31,646 - INFO - Generated cluster name: Game Length and Engagement\n",
      "2025-01-22 13:58:31,648 - INFO - Tokens used so far: Prompt Tokens: 6054, Completion Tokens: 201\n",
      "2025-01-22 13:58:31,651 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 1\n",
      "2025-01-22 13:58:32,387 - INFO - Generated cluster name: Horse Game Recommendations\n",
      "2025-01-22 13:58:32,389 - INFO - Tokens used so far: Prompt Tokens: 6265, Completion Tokens: 205\n",
      "2025-01-22 13:58:32,395 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 9\n",
      "2025-01-22 13:58:33,108 - INFO - Generated cluster name: Stunning Graphics and Gameplay\n",
      "2025-01-22 13:58:33,109 - INFO - Tokens used so far: Prompt Tokens: 6434, Completion Tokens: 211\n",
      "2025-01-22 13:58:33,115 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 28\n",
      "2025-01-22 13:58:33,120 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 27\n",
      "2025-01-22 13:58:33,123 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 5\n",
      "2025-01-22 13:58:33,125 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 24\n",
      "2025-01-22 13:58:33,128 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 8\n",
      "2025-01-22 13:58:33,130 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 17\n",
      "2025-01-22 13:58:33,133 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 0\n",
      "2025-01-22 13:58:33,135 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 30\n",
      "2025-01-22 13:58:33,137 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 16\n",
      "2025-01-22 13:58:33,139 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 19\n",
      "2025-01-22 13:58:33,144 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 11\n",
      "2025-01-22 13:58:33,146 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 18\n",
      "2025-01-22 13:58:33,148 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 22\n",
      "2025-01-22 13:58:33,151 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 2\n",
      "2025-01-22 13:58:33,153 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 14\n",
      "2025-01-22 13:58:33,155 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 6\n",
      "2025-01-22 13:58:33,158 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 10\n",
      "2025-01-22 13:58:33,161 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 26\n",
      "2025-01-22 13:58:33,163 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 23\n",
      "2025-01-22 13:58:33,165 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 15\n",
      "2025-01-22 13:58:33,168 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 7\n",
      "2025-01-22 13:58:33,171 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 33\n",
      "2025-01-22 13:58:33,174 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 31\n",
      "2025-01-22 13:58:33,176 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 25\n",
      "2025-01-22 13:58:33,178 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 3\n",
      "2025-01-22 13:58:33,181 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 32\n",
      "2025-01-22 13:58:33,184 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 13\n",
      "2025-01-22 13:58:33,186 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 20\n",
      "2025-01-22 13:58:33,188 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 29\n",
      "2025-01-22 13:58:33,190 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 4\n",
      "2025-01-22 13:58:33,191 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 21\n",
      "2025-01-22 13:58:33,193 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 12\n",
      "2025-01-22 13:58:33,195 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 1\n",
      "2025-01-22 13:58:33,197 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 9\n",
      "2025-01-22 13:58:33,201 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 28\n",
      "2025-01-22 13:58:33,205 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 27\n",
      "2025-01-22 13:58:33,208 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 5\n",
      "2025-01-22 13:58:33,210 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 24\n",
      "2025-01-22 13:58:33,214 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 8\n",
      "2025-01-22 13:58:33,216 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 17\n",
      "2025-01-22 13:58:33,219 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 0\n",
      "2025-01-22 13:58:33,221 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 30\n",
      "2025-01-22 13:58:33,223 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 16\n",
      "2025-01-22 13:58:33,225 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 19\n",
      "2025-01-22 13:58:33,230 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 11\n",
      "2025-01-22 13:58:33,232 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 18\n",
      "2025-01-22 13:58:33,233 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 22\n",
      "2025-01-22 13:58:33,237 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 2\n",
      "2025-01-22 13:58:33,239 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 14\n",
      "2025-01-22 13:58:33,242 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 6\n",
      "2025-01-22 13:58:33,244 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 10\n",
      "2025-01-22 13:58:33,248 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 26\n",
      "2025-01-22 13:58:33,250 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 23\n",
      "2025-01-22 13:58:33,253 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 15\n",
      "2025-01-22 13:58:33,255 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 7\n",
      "2025-01-22 13:58:33,257 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 33\n",
      "2025-01-22 13:58:33,260 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 31\n",
      "2025-01-22 13:58:33,263 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 25\n",
      "2025-01-22 13:58:33,265 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 3\n",
      "2025-01-22 13:58:33,267 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 32\n",
      "2025-01-22 13:58:33,269 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 13\n",
      "2025-01-22 13:58:33,271 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 20\n",
      "2025-01-22 13:58:33,273 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 29\n",
      "2025-01-22 13:58:33,275 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 4\n",
      "2025-01-22 13:58:33,277 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 21\n",
      "2025-01-22 13:58:33,278 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 12\n",
      "2025-01-22 13:58:33,280 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 1\n",
      "2025-01-22 13:58:33,282 - INFO - Found 10 Topics for hdbscan_cluster_id ID: 9\n",
      "2025-01-22 13:58:33,285 - INFO - Cluster naming process completed.\n",
      "2025-01-22 13:58:33,285 - INFO - Saving updated data to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\Steam\\db_final.json\n",
      "2025-01-22 13:58:34,220 - INFO - Data saved successfully.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combining the data",
   "id": "2ad44626cfd97e37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:03:28.103536Z",
     "start_time": "2025-01-22T02:03:27.923316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# General modules\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "client = openai.Client()\n",
    "\n",
    "chat_model_name = 'gpt-4o-mini'\n",
    "embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Paths\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC'\n",
    "influencer = 'Community'\n",
    "steam_reviews = 'Steam'\n",
    "\n",
    "influencer_data = os.path.join(root_dir, influencer, \"db_analysed.json\")\n",
    "steam_data = os.path.join(root_dir, steam_reviews, \"db_analysed.json\")\n",
    "\n",
    "path_db_analysed = os.path.join(root_dir, \"db_analysed.json\")\n",
    "path_db_embedded = os.path.join(root_dir, \"db_embedded.json\")\n",
    "path_db_final = os.path.join(root_dir, \"db_final.json\")\n"
   ],
   "id": "49c2144aa5e59572",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:03:29.924839Z",
     "start_time": "2025-01-22T02:03:29.894885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# flatten steam data\n",
    "\n",
    "def flatten_data(data):\n",
    "    flattened = []\n",
    "    for entry in data:\n",
    "        base_copy = dict(entry)\n",
    "        topics = base_copy.pop(\"topics\", [])\n",
    "\n",
    "        for topic in topics:\n",
    "            new_entry = dict(base_copy)\n",
    "            new_entry.update(topic)\n",
    "            flattened.append(new_entry)\n",
    "    return flattened\n",
    "\n",
    "steam_data_flattened = flatten_data(steam_data)\n",
    "with open(path_db_analysed, \"w\") as output_file:\n",
    "    json.dump(steam_data_flattened, output_file, indent=4)"
   ],
   "id": "e3aace07684d28f5",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m             flattened\u001B[38;5;241m.\u001B[39mappend(new_entry)\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m flattened\n\u001B[1;32m---> 15\u001B[0m steam_data_flattened \u001B[38;5;241m=\u001B[39m \u001B[43mflatten_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43msteam_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path_db_analysed, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m output_file:\n\u001B[0;32m     17\u001B[0m     json\u001B[38;5;241m.\u001B[39mdump(steam_data_flattened, output_file, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n",
      "Cell \u001B[1;32mIn[46], line 6\u001B[0m, in \u001B[0;36mflatten_data\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m      4\u001B[0m flattened \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[1;32m----> 6\u001B[0m     base_copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mentry\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     topics \u001B[38;5;241m=\u001B[39m base_copy\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtopics\u001B[39m\u001B[38;5;124m\"\u001B[39m, [])\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m topic \u001B[38;5;129;01min\u001B[39;00m topics:\n",
      "\u001B[1;31mValueError\u001B[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:03:33.552541Z",
     "start_time": "2025-01-22T02:03:33.525630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the pre embedded data\n",
    "influencer_data = read_json(influencer_data)\n",
    "steam_data = read_json(steam_data)"
   ],
   "id": "5c0fdfd4267c3644",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\Community\\db_analysed.json' was not found.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'S:\\\\SID\\\\Analytics\\\\Working Files\\\\Individual\\\\Florian\\\\Projects\\\\DataScience\\\\cluster_analysis\\\\Data\\\\HRC\\\\Community\\\\db_analysed.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load the pre embedded data\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m influencer_data \u001B[38;5;241m=\u001B[39m \u001B[43mread_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43minfluencer_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m steam_data \u001B[38;5;241m=\u001B[39m read_json(steam_data)\n",
      "File \u001B[1;32mS:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\helper\\utils.py:142\u001B[0m, in \u001B[0;36mread_json\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;124;03mReads a JSON file and returns its contents as a Python object.\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;124;03m    object: The contents of the JSON file as a Python data structure (e.g., dict or list).\u001B[39;00m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m    143\u001B[0m         data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'S:\\\\SID\\\\Analytics\\\\Working Files\\\\Individual\\\\Florian\\\\Projects\\\\DataScience\\\\cluster_analysis\\\\Data\\\\HRC\\\\Community\\\\db_analysed.json'"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T01:34:51.066162Z",
     "start_time": "2025-01-22T01:34:51.059068Z"
    }
   },
   "cell_type": "code",
   "source": "steam_data[1]",
   "id": "dff76e2f0ab8bfd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app_id_name': '1166860_Rival_Stars_Horse_Racing_Desktop_Edition',\n",
       " 'recommendationid': 178743676,\n",
       " 'playtime_at_review_minutes': 1016,\n",
       " 'last_played': 1731228389,\n",
       " 'review_text': \"definitely a favourite go-to game of mine, I can't really fault anything in it. I DO wish you could care for your horses (e.g, feed them, water them, wash them, etc) but nonetheless it's a great game\",\n",
       " 'timestamp_updated': 1731227648,\n",
       " 'voted_up': True,\n",
       " 'votes_up': 9,\n",
       " 'votes_funny': 0,\n",
       " 'weighted_vote_score': 0.65208226442337,\n",
       " 'steam_purchase': True,\n",
       " 'received_for_free': False,\n",
       " 'written_during_early_access': False,\n",
       " 'language': 'english',\n",
       " 'topic': 'Horse Care Mechanics',\n",
       " 'sentiment': 'Negative',\n",
       " 'category': 'request',\n",
       " 'sentence': 'I DO wish you could care for your horses (e.g., feed them, water them, wash them, etc).'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T01:34:57.405164Z",
     "start_time": "2025-01-22T01:34:57.397770Z"
    }
   },
   "cell_type": "code",
   "source": "influencer_data[1]",
   "id": "cb95b92d93a53716",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_ID': 2,\n",
       " 'topic': 'Rival Stars Update',\n",
       " 'sentiment': 'Positive',\n",
       " 'category': 'fact',\n",
       " 'sentence': 'There has been an update on Rival stars as you saw by the thumbnail and the title um they have added show jumping.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add tags to all JSON entires in the influencer data and the steam data",
   "id": "471787d213b6ab3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T01:36:45.548327Z",
     "start_time": "2025-01-22T01:36:45.541882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for entry in influencer_data:\n",
    "    entry[\"data_source\"] = \"influencer\""
   ],
   "id": "2fd70e6e25ae050",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T01:37:03.820569Z",
     "start_time": "2025-01-22T01:37:03.804093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for entry in steam_data:\n",
    "    entry[\"data_source\"] = \"steam\""
   ],
   "id": "8b2be3a38a342ce4",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combine the data",
   "id": "26fcea37583ab8fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T02:02:33.288621Z",
     "start_time": "2025-01-22T02:02:33.256029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "combined_data = influencer_data + steam_data\n",
    "\n",
    "save_to_json(combined_data, path_db_analysed)"
   ],
   "id": "2d331bed20b3738e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 15:02:33,284 - INFO - Data successfully saved to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\db_analysed.json\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T01:38:01.435534Z",
     "start_time": "2025-01-22T01:38:01.423348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Influencer data: {len(influencer_data)}')\n",
    "print(f'Steam data: {len(steam_data)}')\n",
    "print(f'Combined data: {len(combined_data)}')"
   ],
   "id": "5fd0a920b9f1e60d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influencer data: 753\n",
      "Steam data: 5797\n",
      "Combined data: 6550\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedd the combined data",
   "id": "4ac705261630e43a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.embedding import *\n",
    "\n",
    "embed_key = \"topic\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "data = read_json(path_db_analysed)\n",
    "embed_model = initialize_embedding_model(embed_model_name)\n",
    "\n",
    "def process_embedding(data, embed_key):\n",
    "    for i in range(0, len(data)):\n",
    "        if i % 10 == 0:\n",
    "            logger.info(f\"Processing entry {i}\")\n",
    "\n",
    "        for d_topic in data[i][\"topics\"]:\n",
    "            if isinstance(d_topic, dict):\n",
    "                d_topic[\"embedding\"] = embed_text(d_topic[embed_key], embed_model)\n",
    "    return data\n",
    "\n",
    "data_embedded = process_embedding(data, embed_key)\n",
    "\n",
    "# Flatten\n",
    "def flatten_data(data):\n",
    "    flattened = []\n",
    "    for entry in data:\n",
    "        base_copy = dict(entry)\n",
    "        topics = base_copy.pop(\"topics\", [])\n",
    "\n",
    "        for topic in topics:\n",
    "            new_entry = dict(base_copy)\n",
    "            new_entry.update(topic)\n",
    "            flattened.append(new_entry)\n",
    "    return flattened\n",
    "\n",
    "data_flattened = flatten_data(data_embedded)\n",
    "\n",
    "# Save the embedded data\n",
    "with open(path_db_embedded, \"w\") as output_file:\n",
    "    json.dump(data_flattened, output_file, indent=4)"
   ],
   "id": "a36d52d9f213303e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
