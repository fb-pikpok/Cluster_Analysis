{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:48:33.244273Z",
     "start_time": "2025-01-21T20:48:32.834467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# General modules\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Language models\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "client = openai.Client()\n",
    "\n",
    "chat_model_name = 'gpt-4o-mini'\n",
    "embed_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Paths\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC'\n",
    "steam_title = 'Community'\n",
    "\n",
    "\n",
    "\n",
    "path_input = os.path.join(root_dir, steam_title, \"Transcript_pinehaven_stream.txt\")\n",
    "path_db_prepared = os.path.join(root_dir, steam_title, \"db_prepared.json\")\n",
    "path_db_translated = os.path.join(root_dir, steam_title, \"db_translated.json\")\n",
    "path_db_analysed = os.path.join(root_dir, steam_title, \"db_analysed.json\")\n",
    "path_db_embedded = os.path.join(root_dir, steam_title, \"db_embedded.json\")\n",
    "path_db_clustered = os.path.join(root_dir, steam_title, \"db_clustered.json\")\n",
    "path_db_final = os.path.join(root_dir, steam_title, \"db_final.json\")"
   ],
   "id": "dfc8a8587566289e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transcript Preparation",
   "id": "f83f2c921bee4f35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the transcript\n",
    "with open(path_input, 'r', encoding='utf-8') as file:\n",
    "    transcript = file.readlines()"
   ],
   "id": "afed59bedab40cd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# concatenate every 100 lines + get rid of '\\n'\n",
    "\n",
    "transcript_joined = []\n",
    "for i in range(0, len(transcript), 40):\n",
    "    transcript_joined.append(' '.join([line.strip() for line in transcript[i:i+100]]))\n"
   ],
   "id": "e71c4d258aff18c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(transcript_joined)",
   "id": "baa94793cfceca2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(transcript_joined[5])",
   "id": "92d8de7739c44733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis",
   "id": "9a4d0b31b6f052f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.utils import *\n",
    "from helper.prompt_templates import *\n",
    "\n",
    "configure_api(client, chat_model_name)\n",
    "\n",
    "all_entries = []\n",
    "\n",
    "for i in range(0, len(transcript_joined)):\n",
    "    logger.info(f\"Processing text {i}\")\n",
    "\n",
    "    transcript = transcript_joined[i]\n",
    "\n",
    "    prompt_influencer = prompt_template_influencer.format(transcript=transcript)\n",
    "    response = api_settings[\"client\"].chat.completions.create(\n",
    "        model=api_settings[\"model\"],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in extracting video game topics from Youtube Transcripts.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_influencer},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    response_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # Dynamically handle varying keys at the root of the response\n",
    "    if isinstance(response_json, dict):\n",
    "        for key, value in response_json.items():\n",
    "            if isinstance(value, list):  # Ensure the value is a list\n",
    "                all_entries.extend(value)\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected format for key '{key}' in response {i}\")\n",
    "    else:\n",
    "        logger.warning(f\"Unexpected response structure for text {i}: {response_json}\")\n",
    "\n",
    "# save the entries\n",
    "with open(path_db_prepared, \"w\") as output_file:\n",
    "    json.dump(all_entries, output_file, indent=4)"
   ],
   "id": "50d2353ad353e5c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# generat unique ID\n",
    "\n",
    "from helper.utils import *\n",
    "\n",
    "# A unique ID is generated in the new column / key \"response_ID\"\n",
    "data = read_json(path_db_prepared)\n",
    "data_prepared = generate_ID(data)\n",
    "save_to_json(data_prepared, path_db_prepared)"
   ],
   "id": "1f236c93e32c4cb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentiment Analysis",
   "id": "e9eadf2c98e80fc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_entries = []\n",
    "data_prepared = read_json(path_db_prepared)\n",
    "\n",
    "for i in range(0, len(data_prepared)):\n",
    "    entry = data_prepared[i]\n",
    "    logger.info(f\"Process Sentiment for text {i}\")\n",
    "    try:\n",
    "        prompt_sentiment = prompt_template_sentiment.format(\n",
    "            review=entry[\"Context\"], topic=entry[\"Topic\"]\n",
    "        )\n",
    "        response = api_settings[\"client\"].chat.completions.create(\n",
    "            model=api_settings[\"model\"],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert for sentiment analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_sentiment},\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        sentiment = response.choices[0].message.content.strip()\n",
    "        # rename keys\n",
    "        entry[\"topic\"] = entry[\"Topic\"]\n",
    "        entry[\"sentiment\"] = sentiment\n",
    "        entry[\"category\"] = entry[\"Category\"]\n",
    "        entry[\"sentence\"] = entry[\"Context\"]\n",
    "        entry.pop(\"Context\")\n",
    "        entry.pop(\"Category\")\n",
    "        entry.pop(\"Topic\")\n",
    "\n",
    "        all_entries.append(entry)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing sentiment for topic '{entry['Topic']}' (Entry ID {entry['response_ID']}): {e}\")\n",
    "        raise\n",
    "\n"
   ],
   "id": "d8acd2d7c6c1f9f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the entries\n",
    "with open(path_db_analysed, \"w\") as output_file:\n",
    "    json.dump(all_entries, output_file, indent=4)"
   ],
   "id": "137b73b925f6e3f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding",
   "id": "550e6761353bf508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.embedding import *\n",
    "\n",
    "embed_key = \"topic\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "data = read_json(path_db_analysed)\n",
    "embed_model = initialize_embedding_model(embed_model_name)\n",
    "\n",
    "def process_embedding(data, embed_key):\n",
    "    for i in range(0, len(data)):\n",
    "        if i % 10 == 0:\n",
    "            logger.info(f\"Processing entry {i}\")\n",
    "        entry = data[i]\n",
    "        text = entry[embed_key]\n",
    "        embedding = embed_text(text, embed_model)\n",
    "        entry[\"embedding\"] = embedding\n",
    "    return data\n",
    "\n",
    "data_embedded = process_embedding(data, embed_key)\n",
    "\n",
    "# Save the embedded data\n",
    "with open(path_db_embedded, \"w\") as output_file:\n",
    "    json.dump(data_embedded, output_file, indent=4)\n"
   ],
   "id": "7e69d2b853315b0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering",
   "id": "2b8f576120da7fa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.cluster_analysis import *\n",
    "\n",
    "# Adjustable parameters\n",
    "dimensionality_methods = ['UMAP','PCA', 'tSNE']\n",
    "hdbscan_params = {\"min_cluster_size\": 7, \"min_samples\": 2, \"cluster_selection_epsilon\": 0.4}\n",
    "\n",
    "# Load data\n",
    "df_total = load_embedded_data(path_db_embedded)\n",
    "mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "# Apply HDBSCAN\n",
    "df_total = apply_hdbscan(\n",
    "    df_total,\n",
    "    mat,\n",
    "    dimensionality_methods,\n",
    "    hdbscan_params=hdbscan_params,\n",
    "    include_2d=True,\n",
    "    include_3d=True\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_df_as_json(df_total, path_db_clustered)"
   ],
   "id": "944d01d2bd2a71f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster Naming",
   "id": "14f0ce0528f48d48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.cluster_naming import *\n",
    "\n",
    "# Parameters\n",
    "dimensionality_methods = [\"UMAP\",'PCA', \"tSNE\"]\n",
    "clustering_algorithms = [\"hdbscan\"]  # No KMeans here\n",
    "max_centers = 10\n",
    "\n",
    "#kmeans_clusters = [15, 20, 25, 50]  # Number of clusters for KMeans\n",
    "\n",
    "# Load data\n",
    "df_total = load_json_into_df(path_db_clustered)\n",
    "\n",
    "# Process clusters and generate names\n",
    "df_total = process_clusters(\n",
    "    df_total,\n",
    "    dimensionality_methods,\n",
    "    clustering_algorithms,\n",
    "    max_centers,\n",
    "    api_settings) # insert kmeans_clusters in the function when needed\n",
    "\n",
    "\n",
    "# Save results\n",
    "save_data_for_streamlit(df_total, path_db_final)"
   ],
   "id": "d4bc70643727aec2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HRC Steam reviews",
   "id": "8d05f207f0403fdd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:48:53.144271Z",
     "start_time": "2025-01-21T20:48:53.136802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC'\n",
    "steam_title = 'Steam'\n",
    "\n",
    "\n",
    "path_input = os.path.join(root_dir, steam_title, \"Transcript_pinehaven_stream.txt\")\n",
    "path_db_prepared = os.path.join(root_dir, steam_title, \"db_prepared.json\")\n",
    "path_db_translated = os.path.join(root_dir, steam_title, \"db_translated.json\")\n",
    "path_db_analysed = os.path.join(root_dir, steam_title, \"db_analysed.json\")\n",
    "path_db_embedded = os.path.join(root_dir, steam_title, \"db_embedded.json\")\n",
    "path_db_clustered = os.path.join(root_dir, steam_title, \"db_clustered.json\")\n",
    "path_db_final = os.path.join(root_dir, steam_title, \"db_final.json\")"
   ],
   "id": "e7a316bc3d948d3f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Redshift query",
   "id": "97922a3bd896f454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T20:50:32.888418Z",
     "start_time": "2025-01-21T20:50:19.145246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# My imports\n",
    "from helper.redshift_conector_standalone import *\n",
    "\n",
    "# https://store.steampowered.com/app/1166860/Rival_Stars_Horse_Racing_Desktop_Edition/\n",
    "\n",
    "# SQL Query Redshift\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM steam_review\n",
    "where app_id_name = '1166860_Rival_Stars_Horse_Racing_Desktop_Edition'\n",
    "\"\"\"\n",
    "logger.info(f\"Query Redshift with: {sql_query}\")\n",
    "\n",
    "try:\n",
    "    results_json, results_df = fetch_query_results(sql_query)\n",
    "    # Print the first row of the DataFrame\n",
    "    logger.info(\"Successfully fetched query results, with shape: %s\", results_df.shape)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error fetching query results: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save the json\n",
    "parsed_json = json.loads(results_json)\n",
    "\n",
    "# 2) Then pretty-print with indentation\n",
    "save_to_json(parsed_json, path_db_prepared)"
   ],
   "id": "d5a8d3b0805d2c5c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 09:50:19,868 - INFO - Query Redshift with: \n",
      "SELECT *\n",
      "FROM steam_review\n",
      "where app_id_name = '1166860_Rival_Stars_Horse_Racing_Desktop_Edition'\n",
      "\n",
      "2025-01-22 09:50:32,716 - INFO - Successfully fetched query results, with shape: (3235, 14)\n",
      "2025-01-22 09:50:32,886 - INFO - Data successfully saved to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\HRC\\Steam\\db_prepared.json\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Translation",
   "id": "583f05073249d498"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.data_analysis import translate_reviews\n",
    "\n",
    "id_column = \"recommendationid\"              # The column that contains unique IDs\n",
    "text_col = \"review_text\"                    # The column that contains the text to be translated\n",
    "language_col = \"language\"                   # The column that contains the language tag\n",
    "\n",
    "data_translated = translate_reviews(df=results_df,\n",
    "                                    file_path=path_db_translated,\n",
    "                                    id_column=id_column,\n",
    "                                    text_column=text_col,\n",
    "                                    language_column='language')\n",
    "\n",
    "# Save the translated data\n",
    "save_df_as_json(data_translated, path_db_translated)"
   ],
   "id": "771f651d87e073ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Analysis",
   "id": "29079ec413d0653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from helper.utils import *\n",
    "from helper.prompt_templates import *\n",
    "from helper.data_analysis import normalize_topics_key, process_entry\n",
    "\n",
    "# Configure API\n",
    "configure_api(client, chat_model_name)\n",
    "\n",
    "data_prepared = read_json(path_db_translated)\n",
    "\n",
    "id_column = \"recommendationid\"              # The column that contains unique IDs\n",
    "columns_of_interest = [\"review_text\"]       # The column(s) that are going to be analyzed\n",
    "all_entries = []                            # List to store all processed entries\n",
    "processed_ids = set()                       # Set to store IDs of processed entries\n",
    "\n",
    "# If the analyzed file already exists, load it\n",
    "if os.path.exists(path_db_analysed):\n",
    "    all_entries = read_json(path_db_analysed)\n",
    "    processed_ids = {entry[id_column] for entry in all_entries}  # set for O(1) membership checks\n",
    "\n",
    "# Process all unprocessed entries\n",
    "for i, entry in enumerate(data_prepared):\n",
    "    current_id = entry[id_column]\n",
    "\n",
    "    # If we've already processed this entry, skip it\n",
    "    if current_id in processed_ids:\n",
    "        logger.info(f\"Skipping entry {i} (ID: {current_id}) - already processed.\")\n",
    "        continue\n",
    "\n",
    "    # Otherwise, process and append\n",
    "    process_entry(\n",
    "        entry,\n",
    "        id_column,\n",
    "        prompt_template_topic,\n",
    "        prompt_template_sentiment,\n",
    "        api_settings,\n",
    "        columns_of_interest\n",
    "    )\n",
    "    all_entries.append(entry)\n",
    "    processed_ids.add(current_id)  # mark as processed\n",
    "\n",
    "    # Save intermediate progress every 10 entries\n",
    "    if (i % 10) == 0 and i != 0:\n",
    "        save_to_json(all_entries, path_db_analysed)\n",
    "        logger.info(f\"Progress saved at index {i}.\")\n",
    "\n",
    "# Final save after the loop\n",
    "save_to_json(all_entries, path_db_analysed)\n",
    "logger.info(\"All entries processed and final results saved.\")\n"
   ],
   "id": "c7011c034c5be72c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding",
   "id": "b2c091c74428d8c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = read_json(path_db_analysed)",
   "id": "72c7a3ea4cb7f0e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data[0]['topics']",
   "id": "836e44abe4f659d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.embedding import *\n",
    "\n",
    "embed_key = \"topic\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "data = read_json(path_db_analysed)\n",
    "embed_model = initialize_embedding_model(embed_model_name)\n",
    "\n",
    "def process_embedding(data, embed_key):\n",
    "    for i in range(0, len(data)):\n",
    "        if i % 10 == 0:\n",
    "            logger.info(f\"Processing entry {i}\")\n",
    "\n",
    "        for d_topic in data[i][\"topics\"]:\n",
    "            if isinstance(d_topic, dict):\n",
    "                d_topic[\"embedding\"] = embed_text(d_topic[embed_key], embed_model)\n",
    "    return data\n",
    "\n",
    "data_embedded = process_embedding(data, embed_key)\n",
    "\n",
    "# Flatten\n",
    "def flatten_data(data):\n",
    "    flattened = []\n",
    "    for entry in data:\n",
    "        base_copy = dict(entry)\n",
    "        topics = base_copy.pop(\"topics\", [])\n",
    "\n",
    "        for topic in topics:\n",
    "            new_entry = dict(base_copy)\n",
    "            new_entry.update(topic)\n",
    "            flattened.append(new_entry)\n",
    "    return flattened\n",
    "\n",
    "data_flattened = flatten_data(data_embedded)\n",
    "\n",
    "\n",
    "# Save the embedded data\n",
    "with open(path_db_embedded, \"w\") as output_file:\n",
    "    json.dump(data_flattened, output_file, indent=4)\n"
   ],
   "id": "8188d5fddf552289"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering",
   "id": "1d4735db2113350d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.cluster_analysis import *\n",
    "\n",
    "# Adjustable parameters\n",
    "dimensionality_methods = ['UMAP','PCA', 'tSNE']\n",
    "hdbscan_params = {\"min_cluster_size\": 15, \"min_samples\": 5, \"cluster_selection_epsilon\": 0.4}\n",
    "\n",
    "# Load data\n",
    "df_total = load_embedded_data(path_db_embedded)\n",
    "mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "# Apply HDBSCAN\n",
    "df_total = apply_hdbscan(\n",
    "    df_total,\n",
    "    mat,\n",
    "    dimensionality_methods,\n",
    "    hdbscan_params=hdbscan_params,\n",
    "    include_2d=True,\n",
    "    include_3d=True\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_df_as_json(df_total, path_db_clustered)"
   ],
   "id": "8b66465c56b175c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster Naming",
   "id": "d4dc85b0197b8122"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from helper.cluster_naming import *\n",
    "\n",
    "# Parameters\n",
    "dimensionality_methods = [\"UMAP\",'PCA', \"tSNE\"]\n",
    "clustering_algorithms = [\"hdbscan\"]  # No KMeans here\n",
    "max_centers = 10\n",
    "\n",
    "#kmeans_clusters = [15, 20, 25, 50]  # Number of clusters for KMeans\n",
    "\n",
    "# Load data\n",
    "df_total = load_json_into_df(path_db_clustered)\n",
    "\n",
    "df_total = process_clusters(\n",
    "    df_total,\n",
    "    dimensionality_methods,\n",
    "    clustering_algorithms,\n",
    "    max_centers,\n",
    "    api_settings) # insert kmeans_clusters in the function when needed\n",
    "\n",
    "\n",
    "# Save results\n",
    "save_data_for_streamlit(df_total, path_db_final)"
   ],
   "id": "37ae3f861d8411c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
