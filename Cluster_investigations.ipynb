{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:13:41.691674800Z",
     "start_time": "2025-03-05T02:10:33.869624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# General modules\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from helper.utils import *\n",
    "\n",
    "# Setup API keys\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "client = openai.Client()\n",
    "\n",
    "# Specify models\n",
    "chat_model_name = 'gpt-4o-mini'\n",
    "openai_embedding_model = \"text-embedding-3-small\"\n",
    "local_embedding_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "configure_api(client, chat_model_name)\n",
    "\n",
    "# Specify paths for storing (backup) data\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data/'\n",
    "data_source = 'Cluster_tests'\n",
    "\n",
    "path_db_prepared = os.path.join(root_dir, data_source, \"db_prepared.json\")          #backup\n",
    "path_db_translated = os.path.join(root_dir, data_source, \"db_translated.json\")      #backup\n",
    "path_db_analysed = os.path.join(root_dir, data_source, \"db_analysed.json\")          #backup\n",
    "path_db_embedded = os.path.join(root_dir, data_source, \"db_embedded.json\")          #backup\n",
    "path_db_clustered = os.path.join(root_dir, data_source, \"db_clustered.json\")        #backup\n",
    "path_db_final = os.path.join(root_dir, data_source, \"db_final.json\")                #final file"
   ],
   "id": "3be6788d5567bf8d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-05T02:10:48.712704Z",
     "start_time": "2025-03-05T02:10:42.302235Z"
    }
   },
   "source": [
    "# Reduce dimensions a priori\n",
    "# The clustering does not perform to good. Some datapoints that clearly should be in a cluster based on eyeballing and their topic name but they are not. Rather often some points in a dense cluster are categorized as noise.\n",
    "# I will try to improve this by first perform a dimension reduction and then perform clustering. Reason being, that in high dimensions the data might be too sparse for the clustering algorithm to work properly.\n",
    "\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "data = read_json(path_db_embedded)\n",
    "# Gather all embeddings\n",
    "embeddings = [entry['embedding'] for entry in data]\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# Perform UMAP\n",
    "# X_embedded = umap.UMAP(n_components=40).fit_transform(X) # 40 dimensions\n",
    "\n",
    "# Perform t-SNE\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "# Store the updated embeddings in the data\n",
    "for i, entry in enumerate(data):\n",
    "    entry['embedding'] = X_embedded[i].tolist()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:11:08.308386Z",
     "start_time": "2025-03-05T02:10:54.880256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helper.cluster_analysis import *\n",
    "from helper.utils import *\n",
    "\n",
    "# Adjustable parameters\n",
    "dimensionality_methods = ['UMAP', 'PCA', 'tSNE']\n",
    "hdbscan_params = {\"min_cluster_size\": 10, \"min_samples\": 1, \"cluster_selection_epsilon\": 0.15}           #, \"min_samples\": 2, \"cluster_selection_epsilon\": 0.15\n",
    "\n",
    "data = read_json(path_db_embedded)\n",
    "df = pd.DataFrame(data)\n",
    "df = df[df['embedding'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "print(f\"Loaded {len(df)} valid entries with embeddings.\")\n",
    "\n",
    "# Extract embeddings\n",
    "mat = np.array(df['embedding'].tolist())\n",
    "\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(**hdbscan_params)\n",
    "cluster_labels = hdbscan_clusterer.fit_predict(mat)\n",
    "\n",
    "reduction_results = {}\n",
    "\n",
    "for method in dimensionality_methods:\n",
    "    coords_2d = dimensionality_reduction(mat, method, n_components=2)\n",
    "    reduction_results[f'hdbscan_{method}_2D'] = {\n",
    "        'x': coords_2d[:, 0],\n",
    "        'y': coords_2d[:, 1]\n",
    "    }\n",
    "\n",
    "# 3D Reduction\n",
    "    coords_3d = dimensionality_reduction(mat, method, n_components=3)\n",
    "    reduction_results[f'hdbscan_{method}_3D'] = {\n",
    "        'x': coords_3d[:, 0],\n",
    "        'y': coords_3d[:, 1],\n",
    "        'z': coords_3d[:, 2]\n",
    "    }\n",
    "\n",
    "# Add dimensional coordinates to DataFrame\n",
    "for method_dim, coords in reduction_results.items():\n",
    "    for axis, values in coords.items():\n",
    "        df[f'{method_dim}_{axis}'] = values\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "df['hdbscan_id'] = cluster_labels\n"
   ],
   "id": "6078fd17205f3c39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 991 valid entries with embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-03-05 15:10:56,805 - INFO - Applying UMAP with 2 components.\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-03-05 15:11:03,882 - INFO - Applying UMAP with 3 components.\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-03-05 15:11:04,464 - INFO - Applying PCA with 2 components.\n",
      "2025-03-05 15:11:04,491 - INFO - Applying PCA with 3 components.\n",
      "2025-03-05 15:11:04,519 - INFO - Applying tSNE with 2 components.\n",
      "2025-03-05 15:11:04,521 - INFO - Perplexity not provided, setting to 30 based on sample size.\n",
      "2025-03-05 15:11:05,716 - INFO - Applying tSNE with 3 components.\n",
      "2025-03-05 15:11:05,718 - INFO - Perplexity not provided, setting to 30 based on sample size.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:56:47.954990Z",
     "start_time": "2025-03-04T23:56:47.949052Z"
    }
   },
   "cell_type": "code",
   "source": "df['hdbscan_id'].value_counts()",
   "id": "37086e23ced6a8d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hdbscan_id\n",
       " 4    459\n",
       "-1    443\n",
       " 0     26\n",
       " 1     19\n",
       " 2     19\n",
       " 5     15\n",
       " 3     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:56:56.196217Z",
     "start_time": "2025-03-04T23:56:52.472983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the clustered data\n",
    "# NOTE: either cluster algorithm is optional as well as the dimension reduction. Generally you want to perform HDBSCAN though with reduced dimensions and use kmeans only if you have an idea of the number of clusters up front.\n",
    "save_df_as_json(df, path_db_clustered)"
   ],
   "id": "a115f91be58b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 12:56:52,472 - INFO - Saving data to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data/Cluster_tests\\db_clustered.json\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:13:41.688669Z",
     "start_time": "2025-03-06T01:13:36.097899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from helper.cluster_naming import *\n",
    "\n",
    "api_settings = {\"client\": client, \"model\": chat_model_name}\n",
    "\n",
    "def name_clusters(\n",
    "    df,\n",
    "    cluster_columns,\n",
    "    embedding_col=\"embedding\",\n",
    "    text_col=\"sentence\",\n",
    "    top_k=25,\n",
    "    skip_noise_label=-1\n",
    "):\n",
    "    for col in cluster_columns:\n",
    "        # Prepare a dict to store {cluster_id -> cluster_name}\n",
    "        cluster_id_to_name = {}\n",
    "        logger.info((f'Preparing to name clusters in column \"{col}\"'))\n",
    "\n",
    "        # Get unique cluster IDs from this column\n",
    "        cluster_ids = df[col].unique()\n",
    "\n",
    "        for cluster_id in cluster_ids:\n",
    "\n",
    "            if skip_noise_label is not None and cluster_id == skip_noise_label:\n",
    "                continue\n",
    "\n",
    "            # Select rows belonging to this cluster\n",
    "            cluster_data = df[df[col] == cluster_id]\n",
    "            if cluster_data.empty:\n",
    "                continue\n",
    "\n",
    "            # Compute centroid of embeddings\n",
    "            embeddings = np.array(cluster_data[embedding_col].tolist())\n",
    "            centroid = embeddings.mean(axis=0, dtype=np.float32, keepdims=True)\n",
    "\n",
    "            # Find top_k closest points to centroid\n",
    "            distances = cosine_distances(centroid, embeddings).flatten()\n",
    "            closest_indices = np.argsort(distances)[:top_k]\n",
    "            representative_texts = cluster_data.iloc[closest_indices][text_col].tolist()\n",
    "\n",
    "            # Call your naming function\n",
    "            cluster_name = generate_cluster_name(representative_texts, api_settings)\n",
    "            cluster_id_to_name[cluster_id] = cluster_name\n",
    "\n",
    "        # Create a new column with the cluster name for each row\n",
    "        name_col = f\"{col}_name\"\n",
    "        df[name_col] = df[col].apply(lambda cid: cluster_id_to_name.get(cid, \"Noise\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "data = read_json(path_db_clustered)  # data is probably a list of dicts\n",
    "df = pd.DataFrame(data)              # Convert to DataFrame\n",
    "\n",
    "cluster_columns = ['hdbscan_id']\n",
    "\n",
    "df_named = name_clusters(\n",
    "    df,\n",
    "    cluster_columns,\n",
    "    embedding_col=\"embedding\",\n",
    "    text_col=\"sentence\",\n",
    "    top_k=10,\n",
    "    skip_noise_label=-1  # for HDBSCAN noise\n",
    ")\n"
   ],
   "id": "604ad74a7809a30e",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpairwise\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m cosine_distances\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhelper\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcluster_naming\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m----> 6\u001B[0m api_settings \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclient\u001B[39m\u001B[38;5;124m\"\u001B[39m: client, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mchat_model_name\u001B[49m}\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mname_clusters\u001B[39m(\n\u001B[0;32m      9\u001B[0m     df,\n\u001B[0;32m     10\u001B[0m     cluster_columns,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m     skip_noise_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     15\u001B[0m ):\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m cluster_columns:\n\u001B[0;32m     17\u001B[0m         \u001B[38;5;66;03m# Prepare a dict to store {cluster_id -> cluster_name}\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'chat_model_name' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:57:37.586775Z",
     "start_time": "2025-03-04T23:57:37.508321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# rename pp_timestamp to timestamp_updated\n",
    "df_named.rename(columns={'pp_timestamp':'timestamp_updated'}, inplace=True)\n",
    "\n",
    "# refactor timestamp\n",
    "def convert_timestamp_string_to_unix_ms(date_str):\n",
    "    \"\"\"\n",
    "    Parse a date string of the form 'MM/DD/YYYY HH:MM:SS AM/PM'\n",
    "    and convert it to a Unix timestamp in milliseconds.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_str, \"%m/%d/%Y %I:%M:%S %p\")\n",
    "    return int(dt.timestamp())\n",
    "\n",
    "# We assume 'timestamp_updated' is the column to convert\n",
    "def parse_or_preserve(val):\n",
    "    \"\"\"\n",
    "    If the value is a string, try converting it.\n",
    "    If it's already numeric (maybe you have mixed data?), leave it as is.\n",
    "    \"\"\"\n",
    "    if isinstance(val, str):\n",
    "        return convert_timestamp_string_to_unix_ms(val)\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "# Apply the function to each row in the 'timestamp_updated' column\n",
    "df_named[\"timestamp_updated\"] = df_named[\"timestamp_updated\"].apply(parse_or_preserve)\n",
    "\n",
    "\n",
    "# Optionally get rid of the embeddings to save space\n",
    "df_named.drop(columns=['embedding'], inplace=True)\n",
    "save_data_for_streamlit(df_named, path_db_final)"
   ],
   "id": "37c74fae1cef5e40",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 12:57:37,515 - INFO - Saving updated data to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data/Cluster_tests\\db_final.json\n",
      "2025-03-05 12:57:37,584 - INFO - Data saved successfully.\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
