{
 "cells": [
  {
   "cell_type": "code",
   "id": "d0dc548a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:14:00.418748Z",
     "start_time": "2025-03-06T01:13:58.611256Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import json\n",
    "\n",
    "#from Example_steam_reviews import path_db_embedded\n",
    "from sqlalchemy.util import counter\n",
    "\n",
    "# from helper.utils import *\n",
    "# personally would not recommend this importing style, \n",
    "# it's recommend to use \"from package import foo1, foo2\", or \"import package as p\"\n",
    "from helper.utils import configure_api\n",
    "from helper.data_pipeline import gather_data, translate_data, analyse_data, embed_data\n",
    "\n",
    "\n",
    "# load_dotenv()  # \n",
    "d = dotenv_values()\n",
    "for k in d.keys():\n",
    "    os.environ[k] = d[k]\n",
    "\n",
    "# General modules\n",
    "\n",
    "# Setup API keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_api_key\n",
    "client = openai.Client()\n",
    "\n",
    "# Specify models\n",
    "chat_model_name = 'gpt-4o-mini'\n",
    "openai_embedding_model = \"text-embedding-3-small\"\n",
    "local_embedding_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "configure_api(client, chat_model_name)\n",
    "\n",
    "# Specify paths for storing (backup) data\n",
    "root_dir = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\Backup'\n",
    "\n",
    "# Setup the logger\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)      # Supress API HTTP request logs\n",
    "\n",
    "# path_db_prepared = os.path.join(root_dir, data_source, \"db_prepared.json\")          #backup\n",
    "# path_db_translated = os.path.join(root_dir, data_source, \"db_translated.json\")      #backup\n",
    "# path_db_analysed = os.path.join(root_dir, data_source, \"db_analysed.json\")          #backup\n",
    "# path_db_embedded = os.path.join(root_dir, data_source, \"db_embedded.json\")          #backup\n",
    "\n",
    "\n",
    "# path_db_clustered = os.path.join(root_dir, data_source, \"db_clustered.json\")        #backup\n",
    "# path_db_final = os.path.join(root_dir, data_source, \"db_final.json\")                #final file"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d5d88a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:43:49,502 - INFO - Query Redshift with: \n",
      "    SELECT *\n",
      "    FROM steam_review\n",
      "    where app_id_name = '1166860_Rival_Stars_Horse_Racing_Desktop_Edition'\n",
      "    limit 5\n",
      "    \n",
      "2025-02-18 15:43:50,643 - INFO - Successfully fetched query results, with shape: (5, 14)\n",
      "2025-02-18 15:43:50,649 - INFO - Data successfully saved to C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Steam\\db_prepared.json\n",
      "2025-02-18 15:43:50,656 - INFO - 'language' column already exists. Skipping language detection.\n",
      "2025-02-18 15:43:50,657 - INFO - Loading existing reviews from: C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Steam\\db_translated.json\n",
      "2025-02-18 15:43:50,659 - INFO - Found 0 new reviews to process.\n",
      "2025-02-18 15:43:50,660 - INFO - No new reviews to add. All IDs already exist.\n",
      "2025-02-18 15:43:50,661 - INFO - Translation completed. Total reviews translated: 0\n",
      "2025-02-18 15:43:50,666 - INFO - Skipping entry 0 (ID: 182030680) - already processed.\n",
      "2025-02-18 15:43:50,667 - INFO - Skipping entry 1 (ID: 177147219) - already processed.\n",
      "2025-02-18 15:43:50,667 - INFO - Skipping entry 2 (ID: 176912723) - already processed.\n",
      "2025-02-18 15:43:50,668 - INFO - Skipping entry 3 (ID: 172886364) - already processed.\n",
      "2025-02-18 15:43:50,668 - INFO - Skipping entry 4 (ID: 171277133) - already processed.\n",
      "2025-02-18 15:43:50,671 - INFO - Data successfully saved to C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Steam\\db_analysed.json\n",
      "2025-02-18 15:43:50,671 - INFO - All entries processed and final results saved.\n",
      "2025-02-18 15:43:50,684 - INFO - Processing entry 0\n",
      "2025-02-18 15:44:05,804 - INFO - Data successfully saved to C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Steam\\db_embedded.json\n"
     ]
    }
   ],
   "source": [
    "# Steam Reviews\n",
    "from helper.redshift_conector_standalone import fetch_query_results\n",
    "\n",
    "\n",
    "data_source = 'Steam'\n",
    "longname = 'com.pikpok.hrc'\n",
    "id_column = 'recommendationid'\n",
    "text_column = 'review_text'\n",
    "timestamp_column = 'timestamp_updated'\n",
    "language_column = 'language'\n",
    "embed_key = \"sentence\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "\n",
    "def steam_query_function():\n",
    "    # SQL Query Redshift\n",
    "    sql_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM steam_review\n",
    "    where app_id_name = '1166860_Rival_Stars_Horse_Racing_Desktop_Edition'\n",
    "    limit 5\n",
    "    \"\"\"\n",
    "    logger.info(f\"Query Redshift with: {sql_query}\")\n",
    "\n",
    "    try:\n",
    "        results_json, results_df = fetch_query_results(sql_query)\n",
    "        # Print the first row of the DataFrame\n",
    "        logger.info(\"Successfully fetched query results, with shape: %s\", results_df.shape)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching query results: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return results_json\n",
    "\n",
    "\n",
    "path_dir = os.path.join(root_dir, data_source)\n",
    "if not os.path.exists(path_dir):\n",
    "    os.mkdir(path_dir)\n",
    "    \n",
    "gather_data(root_dir, data_source,  \n",
    "            query_function=steam_query_function, \n",
    "            id_column=id_column, \n",
    "            text_column=text_column, \n",
    "            timestamp_column=timestamp_column,\n",
    "            longname=longname)\n",
    "translate_data(root_dir, data_source, language_column)\n",
    "analyse_data(root_dir, data_source, client, chat_model_name)\n",
    "embed_data(root_dir, data_source, client, embed_key)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c68ec9d9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Google play review\n",
    "import datetime\n",
    "import google_play_scraper as gps\n",
    "\n",
    "data_source = 'Google Play'\n",
    "id_column = 'reviewId'\n",
    "text_column = 'content'\n",
    "timestamp_column = 'at'\n",
    "language_column = 'language'\n",
    "embed_key = \"sentence\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "\n",
    "def google_play_query_function():\n",
    "    app_id = \"com.pikpok.hrc.play\"\n",
    "    longname = '.'.join(app_id.split('.')[:3])\n",
    "    result, continuation_token = gps.reviews(\n",
    "        app_id,\n",
    "#         lang=\"en\",  # Language (English)\n",
    "#         country=\"us\",  # Country (United States)\n",
    "        count=2000,  # Number of reviews to fetch\n",
    "        sort=gps.Sort.NEWEST\n",
    "    )\n",
    "    for e in result:\n",
    "        e['longname'] = longname\n",
    "        for k in e.keys():\n",
    "            if isinstance(e[k], datetime.datetime):\n",
    "                e[k] = int(e[k].timestamp())\n",
    "    return json.dumps(result)\n",
    "\n",
    "\n",
    "path_dir = os.path.join(root_dir, data_source)\n",
    "if not os.path.exists(path_dir):\n",
    "    os.mkdir(path_dir)\n",
    "    \n",
    "gather_data(root_dir, data_source, \n",
    "            query_function=google_play_query_function, \n",
    "            id_column=id_column, \n",
    "            text_column=text_column, \n",
    "            timestamp_column=timestamp_column)\n",
    "translate_data(root_dir, data_source, language_column)\n",
    "analyse_data(root_dir, data_source, client, chat_model_name)\n",
    "embed_data(root_dir, data_source, client, embed_key)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d669826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:45:50,314 - INFO - Data successfully saved to C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Zendesk\\db_prepared.json\n",
      "2025-02-18 15:45:50,326 - INFO - Starting language detection for column: 'pp_review'\n",
      "2025-02-18 15:45:50,328 - INFO - Language detection completed. Added column 'language'.\n",
      "2025-02-18 15:45:50,328 - INFO - No existing file found. Starting fresh.\n",
      "2025-02-18 15:45:50,331 - INFO - Found 5 new reviews to process.\n",
      "2025-02-18 15:45:50,336 - INFO - Updated file saved to: C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Zendesk\\db_translated.json\n",
      "2025-02-18 15:45:50,336 - INFO - Translation completed. Total reviews translated: 0\n",
      "2025-02-18 15:45:50,353 - INFO - Tokens used so far: Prompt Tokens: 4432, Completion Tokens: 384\n",
      "2025-02-18 15:45:50,354 - INFO - Extracting topics for entry ID 101047\n",
      "2025-02-18 15:45:51,303 - INFO - Analyzing sentiment for topic 'Store Accessibility' (Entry ID 101047)\n",
      "2025-02-18 15:45:51,983 - INFO - Tokens used so far: Prompt Tokens: 5185, Completion Tokens: 427\n",
      "2025-02-18 15:45:51,986 - INFO - Extracting topics for entry ID 101046\n",
      "2025-02-18 15:45:52,918 - INFO - Analyzing sentiment for topic 'Wrong Item' (Entry ID 101046)\n",
      "2025-02-18 15:45:53,274 - INFO - Tokens used so far: Prompt Tokens: 5936, Completion Tokens: 469\n",
      "2025-02-18 15:45:53,276 - INFO - Extracting topics for entry ID 101045\n",
      "2025-02-18 15:45:54,136 - INFO - Analyzing sentiment for topic 'Item Accuracy' (Entry ID 101045)\n",
      "2025-02-18 15:45:54,793 - INFO - Tokens used so far: Prompt Tokens: 6687, Completion Tokens: 511\n",
      "2025-02-18 15:45:54,795 - INFO - Extracting topics for entry ID 101042\n",
      "2025-02-18 15:45:55,659 - INFO - Analyzing sentiment for topic 'Software Stability' (Entry ID 101042)\n",
      "2025-02-18 15:45:56,230 - INFO - Tokens used so far: Prompt Tokens: 7440, Completion Tokens: 554\n",
      "2025-02-18 15:45:56,232 - INFO - Extracting topics for entry ID 101040\n",
      "2025-02-18 15:45:57,221 - INFO - Analyzing sentiment for topic 'Service Feedback' (Entry ID 101040)\n",
      "2025-02-18 15:45:57,790 - INFO - Data successfully saved to C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Zendesk\\db_analysed.json\n",
      "2025-02-18 15:45:57,792 - INFO - All entries processed and final results saved.\n",
      "2025-02-18 15:45:57,811 - INFO - Processing entry 0\n",
      "2025-02-18 15:46:00,760 - INFO - Data successfully saved to C:\\Users\\mshen\\Documents\\Msheng_Domestic\\sentiment_analysis\\Zendesk\\db_embedded.json\n"
     ]
    }
   ],
   "source": [
    "# Zendesk CS tickets (temporarily using subcategory as the context)\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "data_source = 'Zendesk'\n",
    "id_column = 'ticket_id'\n",
    "text_column = 'content'\n",
    "timestamp_column = 'created_at'\n",
    "language_column = 'language'\n",
    "embed_key = \"sentence\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "\n",
    "def zendesk_query_function():\n",
    "    conn = psycopg2.connect(host=os.getenv(\"POSTGRESQL_HOST\"), \n",
    "                            database=os.getenv(\"POSTGRESQL_DATABASE\"),\n",
    "                            user=os.getenv(\"POSTGRESQL_USER\"), \n",
    "                            password=os.getenv(\"POSTGRESQL_PASSWORD\"), \n",
    "                            port=os.getenv(\"POSTGRESQL_PORT\"))\n",
    "    curr = conn.cursor()  # TODO: please fill in the query and replace the date filter\n",
    "    s_query = \"\"\"\n",
    "    select coalesce(intent_subcategory2, coalesce(intent_subcategory1, intent_primary)) as content,\n",
    "           * \n",
    "    from zendesk \n",
    "    where product = 'HRC' \n",
    "    and coalesce(intent_subcategory2, coalesce(intent_subcategory1, intent_primary)) is not null\n",
    "    order by created_at desc\n",
    "    limit 5\n",
    "    \"\"\"\n",
    "    s_query = s_query.replace('>>', '>').replace('<<', '<')\n",
    "    curr.execute(s_query)\n",
    "    df = pd.DataFrame(curr.fetchall(), columns=[i[0] for i in curr.description])\n",
    "    df['created_at'] = df['created_at'].apply(lambda x: int(x.timestamp()))\n",
    "    df['longname'] = ['com.pikpok.' + str(x).lower() if x is not None else None for x in df['product']]\n",
    "    return df.to_json(orient='records')\n",
    "\n",
    "\n",
    "path_dir = os.path.join(root_dir, data_source)\n",
    "if not os.path.exists(path_dir):\n",
    "    os.mkdir(path_dir)\n",
    "    \n",
    "gather_data(root_dir, data_source, \n",
    "            query_function=zendesk_query_function, \n",
    "            id_column=id_column, \n",
    "            text_column=text_column, \n",
    "            timestamp_column=timestamp_column)\n",
    "translate_data(root_dir, data_source, language_column)\n",
    "analyse_data(root_dir, data_source, client, chat_model_name)\n",
    "embed_data(root_dir, data_source, client, embed_key)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:37:35.003865Z",
     "start_time": "2025-03-06T01:37:34.727106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Surveys\n",
    "# concat different cols to one\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'input.json' with the path to your JSON file\n",
    "file = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\DRS_NextFest_Data_Fixed.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# Specify the columns you want to combine\n",
    "cols_to_concat = ['Add,', 'Change,', 'Remove,']\n",
    "\n",
    "# Create a new column \"combined_text\" that concatenates the values of the given columns\n",
    "# If a column is empty or NaN, it will be skipped\n",
    "df['combined_text'] = df[cols_to_concat].apply(\n",
    "    lambda row: ' '.join(str(x) for x in row if pd.notnull(x) and str(x).strip() != ''),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Mutate the col \"session_length\" to minutes by dividing the entries by 60\n",
    "df['session_length'] = df['session_length'] / 60\n",
    "\n",
    "# rename session_length to playtime_at_review_minutes\n",
    "df.rename(columns={'session_length':'playtime_at_review_minutes'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('output.xlsx', index=False)\n"
   ],
   "id": "3912a19a9c2cd822",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "534f9e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:14:48.176186Z",
     "start_time": "2025-03-06T01:14:41.238878Z"
    }
   },
   "source": [
    "# Surveys\n",
    "data_source = 'Survey'\n",
    "file = r'S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\DRS\\DRS new\\output.xlsx'\n",
    "longname = 'com.pikpok.drs'\n",
    "id_column = 'main_id'\n",
    "text_column = \"combined_text\"   #[\"If you had a magic wand and you could add, change or remove anything from the game, what would it be and why?\", \"Unnamed: 19\", \"Unnamed: 20\"]\n",
    "timestamp_column = 'End Date'\n",
    "language_column = 'language'\n",
    "embed_key = \"sentence\"  # \"topic\" or \"sentence\"\n",
    "\n",
    "\n",
    "def survey_query_function(file):\n",
    "    file_name, file_extension = os.path.splitext(file)\n",
    "    if '.csv' == file_extension:\n",
    "        df = pd.read_csv(file)\n",
    "        js = json.loads(df.to_json(orient='records'))\n",
    "    elif '.xls' == file_extension or '.xlsx' == file_extension:\n",
    "        df = pd.read_excel(file)\n",
    "        js = json.loads(df.to_json(orient='records'))\n",
    "    elif '.txt' == file_extension:\n",
    "        with open(file, 'r') as f:\n",
    "            js = json.load(f)\n",
    "    for e in js:\n",
    "        if 'Respondent ID' in e.keys() and 'pcubed_id' in e.keys() and 'review_id' not in e.keys():\n",
    "            e['review_id'] = str(e['Respondent ID']) + ':' + str(e['pcubed_id'])\n",
    "    return json.dumps(js)\n",
    "\n",
    "\n",
    "path_dir = os.path.join(root_dir, data_source)\n",
    "if not os.path.exists(path_dir):\n",
    "    os.mkdir(path_dir)\n",
    "    \n",
    "gather_data(root_dir, data_source, \n",
    "            query_function=survey_query_function,\n",
    "            query_function_args=[file,],\n",
    "            id_column=id_column, \n",
    "            text_column=text_column, \n",
    "            timestamp_column=timestamp_column)\n",
    "translate_data(root_dir, data_source, language_column)\n",
    "analyse_data(root_dir, data_source, client, chat_model_name)\n",
    "embed_data(root_dir, data_source, client, embed_key)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 14:14:41,771 - INFO - Data successfully saved to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\Backup\\Survey\\db_prepared.json\n",
      "2025-03-06 14:14:41,801 - INFO - Starting language detection for column: 'pp_review'\n",
      "2025-03-06 14:14:42,238 - INFO - Language detection completed. Added column 'language'.\n",
      "2025-03-06 14:14:42,239 - INFO - Loading existing reviews from: S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\Backup\\Survey\\db_translated.json\n",
      "2025-03-06 14:14:42,273 - INFO - Found 68 new reviews to process.\n",
      "2025-03-06 14:14:42,280 - INFO - No new reviews to add. All IDs already exist.\n",
      "2025-03-06 14:14:42,281 - INFO - Translation completed. Total reviews translated: 0\n",
      "2025-03-06 14:14:42,321 - INFO - Skipping entry 0 (ID: 267) - already processed.\n",
      "2025-03-06 14:14:42,322 - INFO - Skipping entry 1 (ID: 306) - already processed.\n",
      "2025-03-06 14:14:42,323 - INFO - Skipping entry 2 (ID: 193) - already processed.\n",
      "2025-03-06 14:14:42,323 - INFO - Skipping entry 3 (ID: 377) - already processed.\n",
      "2025-03-06 14:14:42,323 - INFO - Skipping entry 4 (ID: 252) - already processed.\n",
      "2025-03-06 14:14:42,324 - INFO - Skipping entry 5 (ID: 328) - already processed.\n",
      "2025-03-06 14:14:42,324 - INFO - Skipping entry 6 (ID: 330) - already processed.\n",
      "2025-03-06 14:14:42,325 - INFO - Skipping entry 7 (ID: 375) - already processed.\n",
      "2025-03-06 14:14:42,325 - INFO - Skipping entry 8 (ID: 277) - already processed.\n",
      "2025-03-06 14:14:42,325 - INFO - Skipping entry 9 (ID: 354) - already processed.\n",
      "2025-03-06 14:14:42,326 - INFO - Skipping entry 10 (ID: 163) - already processed.\n",
      "2025-03-06 14:14:42,326 - INFO - Skipping entry 11 (ID: 327) - already processed.\n",
      "2025-03-06 14:14:42,326 - INFO - Skipping entry 12 (ID: 323) - already processed.\n",
      "2025-03-06 14:14:42,327 - INFO - Skipping entry 13 (ID: 145) - already processed.\n",
      "2025-03-06 14:14:42,327 - INFO - Skipping entry 14 (ID: 366) - already processed.\n",
      "2025-03-06 14:14:42,328 - INFO - Skipping entry 15 (ID: 72) - already processed.\n",
      "2025-03-06 14:14:42,328 - INFO - Skipping entry 16 (ID: 146) - already processed.\n",
      "2025-03-06 14:14:42,328 - INFO - Skipping entry 17 (ID: 43) - already processed.\n",
      "2025-03-06 14:14:42,329 - INFO - Skipping entry 18 (ID: 314) - already processed.\n",
      "2025-03-06 14:14:42,329 - INFO - Skipping entry 19 (ID: 312) - already processed.\n",
      "2025-03-06 14:14:42,330 - INFO - Skipping entry 20 (ID: 18) - already processed.\n",
      "2025-03-06 14:14:42,330 - INFO - Skipping entry 21 (ID: 246) - already processed.\n",
      "2025-03-06 14:14:42,330 - INFO - Skipping entry 22 (ID: 350) - already processed.\n",
      "2025-03-06 14:14:42,331 - INFO - Skipping entry 23 (ID: 92) - already processed.\n",
      "2025-03-06 14:14:42,331 - INFO - Skipping entry 24 (ID: 365) - already processed.\n",
      "2025-03-06 14:14:42,331 - INFO - Skipping entry 25 (ID: 320) - already processed.\n",
      "2025-03-06 14:14:42,332 - INFO - Skipping entry 26 (ID: 372) - already processed.\n",
      "2025-03-06 14:14:42,332 - INFO - Skipping entry 27 (ID: 83) - already processed.\n",
      "2025-03-06 14:14:42,332 - INFO - Skipping entry 28 (ID: 186) - already processed.\n",
      "2025-03-06 14:14:42,333 - INFO - Skipping entry 29 (ID: 336) - already processed.\n",
      "2025-03-06 14:14:42,333 - INFO - Skipping entry 30 (ID: 297) - already processed.\n",
      "2025-03-06 14:14:42,333 - INFO - Skipping entry 31 (ID: 143) - already processed.\n",
      "2025-03-06 14:14:42,334 - INFO - Skipping entry 32 (ID: 318) - already processed.\n",
      "2025-03-06 14:14:42,334 - INFO - Skipping entry 33 (ID: 107) - already processed.\n",
      "2025-03-06 14:14:42,334 - INFO - Skipping entry 34 (ID: 161) - already processed.\n",
      "2025-03-06 14:14:42,334 - INFO - Skipping entry 35 (ID: 374) - already processed.\n",
      "2025-03-06 14:14:42,335 - INFO - Skipping entry 36 (ID: 285) - already processed.\n",
      "2025-03-06 14:14:42,336 - INFO - Skipping entry 37 (ID: 23) - already processed.\n",
      "2025-03-06 14:14:42,336 - INFO - Skipping entry 38 (ID: 302) - already processed.\n",
      "2025-03-06 14:14:42,337 - INFO - Skipping entry 39 (ID: 227) - already processed.\n",
      "2025-03-06 14:14:42,337 - INFO - Skipping entry 40 (ID: 215) - already processed.\n",
      "2025-03-06 14:14:42,337 - INFO - Skipping entry 41 (ID: 82) - already processed.\n",
      "2025-03-06 14:14:42,338 - INFO - Skipping entry 42 (ID: 147) - already processed.\n",
      "2025-03-06 14:14:42,338 - INFO - Skipping entry 43 (ID: 121) - already processed.\n",
      "2025-03-06 14:14:42,338 - INFO - Skipping entry 44 (ID: 274) - already processed.\n",
      "2025-03-06 14:14:42,338 - INFO - Skipping entry 45 (ID: 169) - already processed.\n",
      "2025-03-06 14:14:42,339 - INFO - Skipping entry 46 (ID: 271) - already processed.\n",
      "2025-03-06 14:14:42,339 - INFO - Skipping entry 47 (ID: 85) - already processed.\n",
      "2025-03-06 14:14:42,339 - INFO - Skipping entry 48 (ID: 183) - already processed.\n",
      "2025-03-06 14:14:42,339 - INFO - Skipping entry 49 (ID: 153) - already processed.\n",
      "2025-03-06 14:14:42,340 - INFO - Skipping entry 50 (ID: 235) - already processed.\n",
      "2025-03-06 14:14:42,340 - INFO - Skipping entry 51 (ID: 269) - already processed.\n",
      "2025-03-06 14:14:42,340 - INFO - Skipping entry 52 (ID: 171) - already processed.\n",
      "2025-03-06 14:14:42,340 - INFO - Skipping entry 53 (ID: 189) - already processed.\n",
      "2025-03-06 14:14:42,340 - INFO - Skipping entry 54 (ID: 87) - already processed.\n",
      "2025-03-06 14:14:42,341 - INFO - Skipping entry 55 (ID: 381) - already processed.\n",
      "2025-03-06 14:14:42,341 - INFO - Skipping entry 56 (ID: 47) - already processed.\n",
      "2025-03-06 14:14:42,341 - INFO - Skipping entry 57 (ID: 180) - already processed.\n",
      "2025-03-06 14:14:42,341 - INFO - Skipping entry 58 (ID: 200) - already processed.\n",
      "2025-03-06 14:14:42,342 - INFO - Skipping entry 59 (ID: 368) - already processed.\n",
      "2025-03-06 14:14:42,343 - INFO - Skipping entry 60 (ID: 19) - already processed.\n",
      "2025-03-06 14:14:42,343 - INFO - Skipping entry 61 (ID: 311) - already processed.\n",
      "2025-03-06 14:14:42,343 - INFO - Skipping entry 62 (ID: 325) - already processed.\n",
      "2025-03-06 14:14:42,344 - INFO - Skipping entry 63 (ID: 380) - already processed.\n",
      "2025-03-06 14:14:42,344 - INFO - Skipping entry 64 (ID: 24) - already processed.\n",
      "2025-03-06 14:14:42,344 - INFO - Skipping entry 65 (ID: 114) - already processed.\n",
      "2025-03-06 14:14:42,344 - INFO - Skipping entry 66 (ID: 124) - already processed.\n",
      "2025-03-06 14:14:42,345 - INFO - Skipping entry 67 (ID: 52) - already processed.\n",
      "2025-03-06 14:14:42,345 - INFO - Skipping entry 68 (ID: 362) - already processed.\n",
      "2025-03-06 14:14:42,346 - INFO - Skipping entry 69 (ID: 218) - already processed.\n",
      "2025-03-06 14:14:42,346 - INFO - Skipping entry 70 (ID: 182) - already processed.\n",
      "2025-03-06 14:14:42,346 - INFO - Skipping entry 71 (ID: 309) - already processed.\n",
      "2025-03-06 14:14:42,347 - INFO - Skipping entry 72 (ID: 71) - already processed.\n",
      "2025-03-06 14:14:42,347 - INFO - Skipping entry 73 (ID: 351) - already processed.\n",
      "2025-03-06 14:14:42,347 - INFO - Skipping entry 74 (ID: 209) - already processed.\n",
      "2025-03-06 14:14:42,348 - INFO - Skipping entry 75 (ID: 172) - already processed.\n",
      "2025-03-06 14:14:42,348 - INFO - Skipping entry 76 (ID: 25) - already processed.\n",
      "2025-03-06 14:14:42,348 - INFO - Skipping entry 77 (ID: 260) - already processed.\n",
      "2025-03-06 14:14:42,349 - INFO - Skipping entry 78 (ID: 284) - already processed.\n",
      "2025-03-06 14:14:42,349 - INFO - Skipping entry 79 (ID: 104) - already processed.\n",
      "2025-03-06 14:14:42,350 - INFO - Skipping entry 80 (ID: 20) - already processed.\n",
      "2025-03-06 14:14:42,350 - INFO - Skipping entry 81 (ID: 358) - already processed.\n",
      "2025-03-06 14:14:42,350 - INFO - Skipping entry 82 (ID: 319) - already processed.\n",
      "2025-03-06 14:14:42,351 - INFO - Skipping entry 83 (ID: 262) - already processed.\n",
      "2025-03-06 14:14:42,351 - INFO - Skipping entry 84 (ID: 56) - already processed.\n",
      "2025-03-06 14:14:42,351 - INFO - Skipping entry 85 (ID: 225) - already processed.\n",
      "2025-03-06 14:14:42,351 - INFO - Skipping entry 86 (ID: 181) - already processed.\n",
      "2025-03-06 14:14:42,352 - INFO - Skipping entry 87 (ID: 220) - already processed.\n",
      "2025-03-06 14:14:42,352 - INFO - Skipping entry 88 (ID: 141) - already processed.\n",
      "2025-03-06 14:14:42,352 - INFO - Skipping entry 89 (ID: 15) - already processed.\n",
      "2025-03-06 14:14:42,352 - INFO - Skipping entry 90 (ID: 8) - already processed.\n",
      "2025-03-06 14:14:42,353 - INFO - Skipping entry 91 (ID: 81) - already processed.\n",
      "2025-03-06 14:14:42,354 - INFO - Skipping entry 92 (ID: 142) - already processed.\n",
      "2025-03-06 14:14:42,354 - INFO - Skipping entry 93 (ID: 50) - already processed.\n",
      "2025-03-06 14:14:42,354 - INFO - Skipping entry 94 (ID: 51) - already processed.\n",
      "2025-03-06 14:14:42,355 - INFO - Skipping entry 95 (ID: 130) - already processed.\n",
      "2025-03-06 14:14:42,355 - INFO - Skipping entry 96 (ID: 222) - already processed.\n",
      "2025-03-06 14:14:42,355 - INFO - Skipping entry 97 (ID: 93) - already processed.\n",
      "2025-03-06 14:14:42,355 - INFO - Skipping entry 98 (ID: 348) - already processed.\n",
      "2025-03-06 14:14:42,356 - INFO - Skipping entry 99 (ID: 286) - already processed.\n",
      "2025-03-06 14:14:42,356 - INFO - Skipping entry 100 (ID: 197) - already processed.\n",
      "2025-03-06 14:14:42,356 - INFO - Skipping entry 101 (ID: 270) - already processed.\n",
      "2025-03-06 14:14:42,357 - INFO - Skipping entry 102 (ID: 339) - already processed.\n",
      "2025-03-06 14:14:42,357 - INFO - Skipping entry 103 (ID: 79) - already processed.\n",
      "2025-03-06 14:14:42,357 - INFO - Skipping entry 104 (ID: 206) - already processed.\n",
      "2025-03-06 14:14:42,358 - INFO - Skipping entry 105 (ID: 26) - already processed.\n",
      "2025-03-06 14:14:42,358 - INFO - Skipping entry 106 (ID: 240) - already processed.\n",
      "2025-03-06 14:14:42,358 - INFO - Skipping entry 107 (ID: 94) - already processed.\n",
      "2025-03-06 14:14:42,359 - INFO - Skipping entry 108 (ID: 117) - already processed.\n",
      "2025-03-06 14:14:42,359 - INFO - Skipping entry 109 (ID: 241) - already processed.\n",
      "2025-03-06 14:14:42,359 - INFO - Skipping entry 110 (ID: 231) - already processed.\n",
      "2025-03-06 14:14:42,359 - INFO - Skipping entry 111 (ID: 122) - already processed.\n",
      "2025-03-06 14:14:42,360 - INFO - Skipping entry 112 (ID: 230) - already processed.\n",
      "2025-03-06 14:14:42,360 - INFO - Skipping entry 113 (ID: 125) - already processed.\n",
      "2025-03-06 14:14:42,360 - INFO - Skipping entry 114 (ID: 308) - already processed.\n",
      "2025-03-06 14:14:42,361 - INFO - Skipping entry 115 (ID: 14) - already processed.\n",
      "2025-03-06 14:14:42,362 - INFO - Skipping entry 116 (ID: 27) - already processed.\n",
      "2025-03-06 14:14:42,362 - INFO - Skipping entry 117 (ID: 383) - already processed.\n",
      "2025-03-06 14:14:42,362 - INFO - Skipping entry 118 (ID: 276) - already processed.\n",
      "2025-03-06 14:14:42,363 - INFO - Skipping entry 119 (ID: 151) - already processed.\n",
      "2025-03-06 14:14:42,363 - INFO - Skipping entry 120 (ID: 356) - already processed.\n",
      "2025-03-06 14:14:42,363 - INFO - Skipping entry 121 (ID: 160) - already processed.\n",
      "2025-03-06 14:14:42,364 - INFO - Skipping entry 122 (ID: 313) - already processed.\n",
      "2025-03-06 14:14:42,364 - INFO - Skipping entry 123 (ID: 179) - already processed.\n",
      "2025-03-06 14:14:42,364 - INFO - Skipping entry 124 (ID: 296) - already processed.\n",
      "2025-03-06 14:14:42,364 - INFO - Skipping entry 125 (ID: 282) - already processed.\n",
      "2025-03-06 14:14:42,365 - INFO - Skipping entry 126 (ID: 210) - already processed.\n",
      "2025-03-06 14:14:42,365 - INFO - Skipping entry 127 (ID: 249) - already processed.\n",
      "2025-03-06 14:14:42,365 - INFO - Skipping entry 128 (ID: 127) - already processed.\n",
      "2025-03-06 14:14:42,365 - INFO - Skipping entry 129 (ID: 55) - already processed.\n",
      "2025-03-06 14:14:42,366 - INFO - Skipping entry 130 (ID: 78) - already processed.\n",
      "2025-03-06 14:14:42,367 - INFO - Skipping entry 131 (ID: 64) - already processed.\n",
      "2025-03-06 14:14:42,367 - INFO - Skipping entry 132 (ID: 12) - already processed.\n",
      "2025-03-06 14:14:42,367 - INFO - Skipping entry 133 (ID: 140) - already processed.\n",
      "2025-03-06 14:14:42,368 - INFO - Skipping entry 134 (ID: 57) - already processed.\n",
      "2025-03-06 14:14:42,368 - INFO - Skipping entry 135 (ID: 126) - already processed.\n",
      "2025-03-06 14:14:42,368 - INFO - Skipping entry 136 (ID: 164) - already processed.\n",
      "2025-03-06 14:14:42,369 - INFO - Skipping entry 137 (ID: 10) - already processed.\n",
      "2025-03-06 14:14:42,369 - INFO - Skipping entry 138 (ID: 239) - already processed.\n",
      "2025-03-06 14:14:42,369 - INFO - Skipping entry 139 (ID: 131) - already processed.\n",
      "2025-03-06 14:14:42,370 - INFO - Skipping entry 140 (ID: 194) - already processed.\n",
      "2025-03-06 14:14:42,370 - INFO - Skipping entry 141 (ID: 205) - already processed.\n",
      "2025-03-06 14:14:42,370 - INFO - Skipping entry 142 (ID: 355) - already processed.\n",
      "2025-03-06 14:14:42,371 - INFO - Skipping entry 143 (ID: 326) - already processed.\n",
      "2025-03-06 14:14:42,371 - INFO - Skipping entry 144 (ID: 77) - already processed.\n",
      "2025-03-06 14:14:42,371 - INFO - Skipping entry 145 (ID: 307) - already processed.\n",
      "2025-03-06 14:14:42,371 - INFO - Skipping entry 146 (ID: 159) - already processed.\n",
      "2025-03-06 14:14:42,372 - INFO - Skipping entry 147 (ID: 322) - already processed.\n",
      "2025-03-06 14:14:42,372 - INFO - Skipping entry 148 (ID: 335) - already processed.\n",
      "2025-03-06 14:14:42,372 - INFO - Skipping entry 149 (ID: 149) - already processed.\n",
      "2025-03-06 14:14:42,372 - INFO - Skipping entry 150 (ID: 243) - already processed.\n",
      "2025-03-06 14:14:42,373 - INFO - Skipping entry 151 (ID: 184) - already processed.\n",
      "2025-03-06 14:14:42,373 - INFO - Skipping entry 152 (ID: 360) - already processed.\n",
      "2025-03-06 14:14:42,373 - INFO - Skipping entry 153 (ID: 279) - already processed.\n",
      "2025-03-06 14:14:42,373 - INFO - Skipping entry 154 (ID: 89) - already processed.\n",
      "2025-03-06 14:14:42,374 - INFO - Skipping entry 155 (ID: 36) - already processed.\n",
      "2025-03-06 14:14:42,374 - INFO - Skipping entry 156 (ID: 250) - already processed.\n",
      "2025-03-06 14:14:42,374 - INFO - Skipping entry 157 (ID: 261) - already processed.\n",
      "2025-03-06 14:14:42,375 - INFO - Skipping entry 158 (ID: 13) - already processed.\n",
      "2025-03-06 14:14:42,375 - INFO - Skipping entry 159 (ID: 221) - already processed.\n",
      "2025-03-06 14:14:42,375 - INFO - Skipping entry 160 (ID: 139) - already processed.\n",
      "2025-03-06 14:14:42,375 - INFO - Skipping entry 161 (ID: 304) - already processed.\n",
      "2025-03-06 14:14:42,376 - INFO - Skipping entry 162 (ID: 224) - already processed.\n",
      "2025-03-06 14:14:42,376 - INFO - Skipping entry 163 (ID: 45) - already processed.\n",
      "2025-03-06 14:14:42,376 - INFO - Skipping entry 164 (ID: 264) - already processed.\n",
      "2025-03-06 14:14:42,376 - INFO - Skipping entry 165 (ID: 75) - already processed.\n",
      "2025-03-06 14:14:42,376 - INFO - Skipping entry 166 (ID: 110) - already processed.\n",
      "2025-03-06 14:14:42,377 - INFO - Skipping entry 167 (ID: 378) - already processed.\n",
      "2025-03-06 14:14:42,377 - INFO - Skipping entry 168 (ID: 190) - already processed.\n",
      "2025-03-06 14:14:42,377 - INFO - Skipping entry 169 (ID: 212) - already processed.\n",
      "2025-03-06 14:14:42,377 - INFO - Skipping entry 170 (ID: 173) - already processed.\n",
      "2025-03-06 14:14:42,378 - INFO - Skipping entry 171 (ID: 331) - already processed.\n",
      "2025-03-06 14:14:42,378 - INFO - Skipping entry 172 (ID: 364) - already processed.\n",
      "2025-03-06 14:14:42,378 - INFO - Skipping entry 173 (ID: 219) - already processed.\n",
      "2025-03-06 14:14:42,379 - INFO - Skipping entry 174 (ID: 166) - already processed.\n",
      "2025-03-06 14:14:42,379 - INFO - Skipping entry 175 (ID: 287) - already processed.\n",
      "2025-03-06 14:14:42,379 - INFO - Skipping entry 176 (ID: 324) - already processed.\n",
      "2025-03-06 14:14:42,379 - INFO - Skipping entry 177 (ID: 382) - already processed.\n",
      "2025-03-06 14:14:42,380 - INFO - Skipping entry 178 (ID: 162) - already processed.\n",
      "2025-03-06 14:14:42,380 - INFO - Skipping entry 179 (ID: 255) - already processed.\n",
      "2025-03-06 14:14:42,382 - INFO - Skipping entry 180 (ID: 157) - already processed.\n",
      "2025-03-06 14:14:42,383 - INFO - Skipping entry 181 (ID: 120) - already processed.\n",
      "2025-03-06 14:14:42,383 - INFO - Skipping entry 182 (ID: 165) - already processed.\n",
      "2025-03-06 14:14:42,383 - INFO - Skipping entry 183 (ID: 343) - already processed.\n",
      "2025-03-06 14:14:42,384 - INFO - Skipping entry 184 (ID: 68) - already processed.\n",
      "2025-03-06 14:14:42,384 - INFO - Skipping entry 185 (ID: 272) - already processed.\n",
      "2025-03-06 14:14:42,384 - INFO - Skipping entry 186 (ID: 28) - already processed.\n",
      "2025-03-06 14:14:42,385 - INFO - Skipping entry 187 (ID: 199) - already processed.\n",
      "2025-03-06 14:14:42,385 - INFO - Skipping entry 188 (ID: 233) - already processed.\n",
      "2025-03-06 14:14:42,385 - INFO - Skipping entry 189 (ID: 217) - already processed.\n",
      "2025-03-06 14:14:42,385 - INFO - Skipping entry 190 (ID: 33) - already processed.\n",
      "2025-03-06 14:14:42,386 - INFO - Skipping entry 191 (ID: 61) - already processed.\n",
      "2025-03-06 14:14:42,386 - INFO - Skipping entry 192 (ID: 150) - already processed.\n",
      "2025-03-06 14:14:42,386 - INFO - Skipping entry 193 (ID: 201) - already processed.\n",
      "2025-03-06 14:14:42,387 - INFO - Skipping entry 194 (ID: 192) - already processed.\n",
      "2025-03-06 14:14:42,387 - INFO - Skipping entry 195 (ID: 132) - already processed.\n",
      "2025-03-06 14:14:42,387 - INFO - Skipping entry 196 (ID: 340) - already processed.\n",
      "2025-03-06 14:14:42,387 - INFO - Skipping entry 197 (ID: 48) - already processed.\n",
      "2025-03-06 14:14:42,388 - INFO - Skipping entry 198 (ID: 242) - already processed.\n",
      "2025-03-06 14:14:42,388 - INFO - Skipping entry 199 (ID: 363) - already processed.\n",
      "2025-03-06 14:14:42,388 - INFO - Skipping entry 200 (ID: 185) - already processed.\n",
      "2025-03-06 14:14:42,388 - INFO - Skipping entry 201 (ID: 238) - already processed.\n",
      "2025-03-06 14:14:42,389 - INFO - Skipping entry 202 (ID: 298) - already processed.\n",
      "2025-03-06 14:14:42,389 - INFO - Skipping entry 203 (ID: 289) - already processed.\n",
      "2025-03-06 14:14:42,390 - INFO - Skipping entry 204 (ID: 111) - already processed.\n",
      "2025-03-06 14:14:42,390 - INFO - Skipping entry 205 (ID: 305) - already processed.\n",
      "2025-03-06 14:14:42,390 - INFO - Skipping entry 206 (ID: 299) - already processed.\n",
      "2025-03-06 14:14:42,390 - INFO - Skipping entry 207 (ID: 317) - already processed.\n",
      "2025-03-06 14:14:42,391 - INFO - Skipping entry 208 (ID: 273) - already processed.\n",
      "2025-03-06 14:14:42,391 - INFO - Skipping entry 209 (ID: 367) - already processed.\n",
      "2025-03-06 14:14:42,392 - INFO - Skipping entry 210 (ID: 108) - already processed.\n",
      "2025-03-06 14:14:42,392 - INFO - Skipping entry 211 (ID: 281) - already processed.\n",
      "2025-03-06 14:14:42,393 - INFO - Skipping entry 212 (ID: 137) - already processed.\n",
      "2025-03-06 14:14:42,393 - INFO - Skipping entry 213 (ID: 370) - already processed.\n",
      "2025-03-06 14:14:42,393 - INFO - Skipping entry 214 (ID: 116) - already processed.\n",
      "2025-03-06 14:14:42,393 - INFO - Skipping entry 215 (ID: 234) - already processed.\n",
      "2025-03-06 14:14:42,394 - INFO - Skipping entry 216 (ID: 34) - already processed.\n",
      "2025-03-06 14:14:42,394 - INFO - Skipping entry 217 (ID: 291) - already processed.\n",
      "2025-03-06 14:14:42,394 - INFO - Skipping entry 218 (ID: 101) - already processed.\n",
      "2025-03-06 14:14:42,394 - INFO - Skipping entry 219 (ID: 195) - already processed.\n",
      "2025-03-06 14:14:42,395 - INFO - Skipping entry 220 (ID: 97) - already processed.\n",
      "2025-03-06 14:14:42,395 - INFO - Skipping entry 221 (ID: 40) - already processed.\n",
      "2025-03-06 14:14:42,395 - INFO - Skipping entry 222 (ID: 211) - already processed.\n",
      "2025-03-06 14:14:42,395 - INFO - Skipping entry 223 (ID: 254) - already processed.\n",
      "2025-03-06 14:14:42,396 - INFO - Skipping entry 224 (ID: 237) - already processed.\n",
      "2025-03-06 14:14:42,396 - INFO - Skipping entry 225 (ID: 35) - already processed.\n",
      "2025-03-06 14:14:42,396 - INFO - Skipping entry 226 (ID: 268) - already processed.\n",
      "2025-03-06 14:14:42,396 - INFO - Skipping entry 227 (ID: 303) - already processed.\n",
      "2025-03-06 14:14:42,397 - INFO - Skipping entry 228 (ID: 265) - already processed.\n",
      "2025-03-06 14:14:42,397 - INFO - Skipping entry 229 (ID: 175) - already processed.\n",
      "2025-03-06 14:14:42,397 - INFO - Skipping entry 230 (ID: 95) - already processed.\n",
      "2025-03-06 14:14:42,400 - INFO - Skipping entry 231 (ID: 17) - already processed.\n",
      "2025-03-06 14:14:42,401 - INFO - Skipping entry 232 (ID: 103) - already processed.\n",
      "2025-03-06 14:14:42,401 - INFO - Skipping entry 233 (ID: 80) - already processed.\n",
      "2025-03-06 14:14:42,401 - INFO - Skipping entry 234 (ID: 294) - already processed.\n",
      "2025-03-06 14:14:42,401 - INFO - Skipping entry 235 (ID: 9) - already processed.\n",
      "2025-03-06 14:14:42,402 - INFO - Skipping entry 236 (ID: 113) - already processed.\n",
      "2025-03-06 14:14:42,402 - INFO - Skipping entry 237 (ID: 263) - already processed.\n",
      "2025-03-06 14:14:42,402 - INFO - Skipping entry 238 (ID: 359) - already processed.\n",
      "2025-03-06 14:14:42,402 - INFO - Skipping entry 239 (ID: 208) - already processed.\n",
      "2025-03-06 14:14:42,403 - INFO - Skipping entry 240 (ID: 2) - already processed.\n",
      "2025-03-06 14:14:42,403 - INFO - Skipping entry 241 (ID: 135) - already processed.\n",
      "2025-03-06 14:14:42,403 - INFO - Skipping entry 242 (ID: 167) - already processed.\n",
      "2025-03-06 14:14:42,403 - INFO - Skipping entry 243 (ID: 7) - already processed.\n",
      "2025-03-06 14:14:42,404 - INFO - Skipping entry 244 (ID: 292) - already processed.\n",
      "2025-03-06 14:14:42,404 - INFO - Skipping entry 245 (ID: 74) - already processed.\n",
      "2025-03-06 14:14:42,404 - INFO - Skipping entry 246 (ID: 154) - already processed.\n",
      "2025-03-06 14:14:42,405 - INFO - Skipping entry 247 (ID: 70) - already processed.\n",
      "2025-03-06 14:14:42,406 - INFO - Skipping entry 248 (ID: 69) - already processed.\n",
      "2025-03-06 14:14:42,406 - INFO - Skipping entry 249 (ID: 123) - already processed.\n",
      "2025-03-06 14:14:42,406 - INFO - Skipping entry 250 (ID: 338) - already processed.\n",
      "2025-03-06 14:14:42,406 - INFO - Skipping entry 251 (ID: 247) - already processed.\n",
      "2025-03-06 14:14:42,407 - INFO - Skipping entry 252 (ID: 275) - already processed.\n",
      "2025-03-06 14:14:42,407 - INFO - Skipping entry 253 (ID: 30) - already processed.\n",
      "2025-03-06 14:14:42,407 - INFO - Skipping entry 254 (ID: 373) - already processed.\n",
      "2025-03-06 14:14:42,407 - INFO - Skipping entry 255 (ID: 105) - already processed.\n",
      "2025-03-06 14:14:42,408 - INFO - Skipping entry 256 (ID: 349) - already processed.\n",
      "2025-03-06 14:14:42,408 - INFO - Skipping entry 257 (ID: 133) - already processed.\n",
      "2025-03-06 14:14:42,408 - INFO - Skipping entry 258 (ID: 283) - already processed.\n",
      "2025-03-06 14:14:42,408 - INFO - Skipping entry 259 (ID: 232) - already processed.\n",
      "2025-03-06 14:14:42,409 - INFO - Skipping entry 260 (ID: 144) - already processed.\n",
      "2025-03-06 14:14:42,409 - INFO - Skipping entry 261 (ID: 379) - already processed.\n",
      "2025-03-06 14:14:42,410 - INFO - Skipping entry 262 (ID: 102) - already processed.\n",
      "2025-03-06 14:14:42,410 - INFO - Skipping entry 263 (ID: 300) - already processed.\n",
      "2025-03-06 14:14:42,410 - INFO - Skipping entry 264 (ID: 266) - already processed.\n",
      "2025-03-06 14:14:42,410 - INFO - Skipping entry 265 (ID: 177) - already processed.\n",
      "2025-03-06 14:14:42,411 - INFO - Skipping entry 266 (ID: 353) - already processed.\n",
      "2025-03-06 14:14:42,411 - INFO - Skipping entry 267 (ID: 158) - already processed.\n",
      "2025-03-06 14:14:42,411 - INFO - Skipping entry 268 (ID: 333) - already processed.\n",
      "2025-03-06 14:14:42,412 - INFO - Skipping entry 269 (ID: 257) - already processed.\n",
      "2025-03-06 14:14:42,412 - INFO - Skipping entry 270 (ID: 96) - already processed.\n",
      "2025-03-06 14:14:42,412 - INFO - Skipping entry 271 (ID: 59) - already processed.\n",
      "2025-03-06 14:14:42,413 - INFO - Skipping entry 272 (ID: 342) - already processed.\n",
      "2025-03-06 14:14:42,413 - INFO - Skipping entry 273 (ID: 301) - already processed.\n",
      "2025-03-06 14:14:42,413 - INFO - Skipping entry 274 (ID: 178) - already processed.\n",
      "2025-03-06 14:14:42,413 - INFO - Skipping entry 275 (ID: 44) - already processed.\n",
      "2025-03-06 14:14:42,414 - INFO - Skipping entry 276 (ID: 290) - already processed.\n",
      "2025-03-06 14:14:42,414 - INFO - Skipping entry 277 (ID: 376) - already processed.\n",
      "2025-03-06 14:14:42,414 - INFO - Skipping entry 278 (ID: 251) - already processed.\n",
      "2025-03-06 14:14:42,414 - INFO - Skipping entry 279 (ID: 213) - already processed.\n",
      "2025-03-06 14:14:42,415 - INFO - Skipping entry 280 (ID: 223) - already processed.\n",
      "2025-03-06 14:14:42,415 - INFO - Skipping entry 281 (ID: 29) - already processed.\n",
      "2025-03-06 14:14:42,415 - INFO - Skipping entry 282 (ID: 31) - already processed.\n",
      "2025-03-06 14:14:42,415 - INFO - Skipping entry 283 (ID: 99) - already processed.\n",
      "2025-03-06 14:14:42,416 - INFO - Skipping entry 284 (ID: 188) - already processed.\n",
      "2025-03-06 14:14:42,416 - INFO - Skipping entry 285 (ID: 54) - already processed.\n",
      "2025-03-06 14:14:42,417 - INFO - Skipping entry 286 (ID: 100) - already processed.\n",
      "2025-03-06 14:14:42,417 - INFO - Skipping entry 287 (ID: 88) - already processed.\n",
      "2025-03-06 14:14:42,417 - INFO - Skipping entry 288 (ID: 53) - already processed.\n",
      "2025-03-06 14:14:42,418 - INFO - Skipping entry 289 (ID: 109) - already processed.\n",
      "2025-03-06 14:14:42,418 - INFO - Skipping entry 290 (ID: 334) - already processed.\n",
      "2025-03-06 14:14:42,418 - INFO - Skipping entry 291 (ID: 280) - already processed.\n",
      "2025-03-06 14:14:42,418 - INFO - Skipping entry 292 (ID: 84) - already processed.\n",
      "2025-03-06 14:14:42,419 - INFO - Skipping entry 293 (ID: 91) - already processed.\n",
      "2025-03-06 14:14:42,419 - INFO - Skipping entry 294 (ID: 191) - already processed.\n",
      "2025-03-06 14:14:42,419 - INFO - Skipping entry 295 (ID: 129) - already processed.\n",
      "2025-03-06 14:14:42,420 - INFO - Skipping entry 296 (ID: 156) - already processed.\n",
      "2025-03-06 14:14:42,420 - INFO - Skipping entry 297 (ID: 202) - already processed.\n",
      "2025-03-06 14:14:42,420 - INFO - Skipping entry 298 (ID: 204) - already processed.\n",
      "2025-03-06 14:14:42,420 - INFO - Skipping entry 299 (ID: 1) - already processed.\n",
      "2025-03-06 14:14:42,421 - INFO - Skipping entry 300 (ID: 361) - already processed.\n",
      "2025-03-06 14:14:42,421 - INFO - Skipping entry 301 (ID: 168) - already processed.\n",
      "2025-03-06 14:14:42,421 - INFO - Skipping entry 302 (ID: 3) - already processed.\n",
      "2025-03-06 14:14:42,422 - INFO - Skipping entry 303 (ID: 63) - already processed.\n",
      "2025-03-06 14:14:42,422 - INFO - Skipping entry 304 (ID: 288) - already processed.\n",
      "2025-03-06 14:14:42,422 - INFO - Skipping entry 305 (ID: 228) - already processed.\n",
      "2025-03-06 14:14:42,422 - INFO - Skipping entry 306 (ID: 259) - already processed.\n",
      "2025-03-06 14:14:42,423 - INFO - Skipping entry 307 (ID: 214) - already processed.\n",
      "2025-03-06 14:14:42,423 - INFO - Skipping entry 308 (ID: 32) - already processed.\n",
      "2025-03-06 14:14:42,424 - INFO - Skipping entry 309 (ID: 65) - already processed.\n",
      "2025-03-06 14:14:42,424 - INFO - Skipping entry 310 (ID: 73) - already processed.\n",
      "2025-03-06 14:14:42,424 - INFO - Skipping entry 311 (ID: 90) - already processed.\n",
      "2025-03-06 14:14:42,424 - INFO - Skipping entry 312 (ID: 229) - already processed.\n",
      "2025-03-06 14:14:42,425 - INFO - Skipping entry 313 (ID: 337) - already processed.\n",
      "2025-03-06 14:14:42,425 - INFO - Skipping entry 314 (ID: 369) - already processed.\n",
      "2025-03-06 14:14:42,528 - INFO - Data successfully saved to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\Backup\\Survey\\db_analysed.json\n",
      "2025-03-06 14:14:42,529 - INFO - All entries processed and final results saved.\n",
      "S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\helper\\data_pipeline.py:124: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 41\u001B[0m\n\u001B[0;32m     39\u001B[0m translate_data(root_dir, data_source, language_column)\n\u001B[0;32m     40\u001B[0m analyse_data(root_dir, data_source, client, chat_model_name)\n\u001B[1;32m---> 41\u001B[0m \u001B[43membed_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_source\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed_key\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mS:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\helper\\data_pipeline.py:124\u001B[0m, in \u001B[0;36membed_data\u001B[1;34m(root_dir, data_source, client, embed_key)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LangchainEmbedding\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HuggingFaceEmbeddings\n\u001B[1;32m--> 124\u001B[0m embed_model \u001B[38;5;241m=\u001B[39m LangchainEmbedding(\u001B[43mHuggingFaceEmbeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mall-MiniLM-L6-v2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_offline_embedding\u001B[39m(text):\n\u001B[0;32m    126\u001B[0m     text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mencode(encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mASCII\u001B[39m\u001B[38;5;124m\"\u001B[39m, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode()\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    215\u001B[0m     emit_warning()\n\u001B[1;32m--> 216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:84\u001B[0m, in \u001B[0;36mHuggingFaceEmbeddings.__init__\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     warn_deprecated(\n\u001B[0;32m     75\u001B[0m         since\u001B[38;5;241m=\u001B[39msince,\n\u001B[0;32m     76\u001B[0m         removal\u001B[38;5;241m=\u001B[39mremoval,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m constructor instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     81\u001B[0m     )\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     87\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     88\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not import sentence_transformers python package. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     89\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease install it with `pip install sentence-transformers`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     90\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sentence_transformers\\__init__.py:9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     10\u001B[0m     export_dynamic_quantized_onnx_model,\n\u001B[0;32m     11\u001B[0m     export_optimized_onnx_model,\n\u001B[0;32m     12\u001B[0m     export_static_quantized_openvino_model,\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sentence_transformers\\backend.py:11\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Callable, Literal\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m disable_datasets_caching, is_datasets_available\n\u001B[0;32m     13\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sentence_transformers\\util.py:17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m hf_hub_download, snapshot_download\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Tensor, device\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\__init__.py:2486\u001B[0m\n\u001B[0;32m   2482\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m vmap \u001B[38;5;28;01mas\u001B[39;00m vmap\n\u001B[0;32m   2485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m-> 2486\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations\n\u001B[0;32m   2488\u001B[0m \u001B[38;5;66;03m# Enable CUDA Sanitizer\u001B[39;00m\n\u001B[0;32m   2489\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTORCH_CUDA_SANITIZER\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_meta_registrations.py:10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_prims_common\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SymBool, SymFloat, Tensor\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_decomp\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     11\u001B[0m     _add_op_to_registry,\n\u001B[0;32m     12\u001B[0m     _convert_out_params,\n\u001B[0;32m     13\u001B[0m     global_decomposition_table,\n\u001B[0;32m     14\u001B[0m     meta_table,\n\u001B[0;32m     15\u001B[0m )\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ops\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OpOverload\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_prims\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_decomp\\__init__.py:249\u001B[0m\n\u001B[0;32m    245\u001B[0m             decompositions\u001B[38;5;241m.\u001B[39mpop(op, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# populate the table\u001B[39;00m\n\u001B[1;32m--> 249\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_decomp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecompositions\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_refs\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# See NOTE [Core ATen Ops]\u001B[39;00m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;66;03m# list was copied from torch/_inductor/decomposition.py\u001B[39;00m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;66;03m# excluding decompositions that results in prim ops\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_decomp\\decompositions.py:15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_meta_registrations\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_prims\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mprims\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_prims_common\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mutils\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mF\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:749\u001B[0m\n\u001B[0;32m    735\u001B[0m erfc \u001B[38;5;241m=\u001B[39m _make_elementwise_unary_prim(\n\u001B[0;32m    736\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merfc\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    737\u001B[0m     impl_aten\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mspecial\u001B[38;5;241m.\u001B[39merfc,\n\u001B[0;32m    738\u001B[0m     doc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    739\u001B[0m     type_promotion\u001B[38;5;241m=\u001B[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001B[38;5;241m.\u001B[39mDEFAULT,\n\u001B[0;32m    740\u001B[0m )\n\u001B[0;32m    742\u001B[0m erfcx \u001B[38;5;241m=\u001B[39m _make_elementwise_unary_prim(\n\u001B[0;32m    743\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merfcx\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    744\u001B[0m     impl_aten\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mspecial\u001B[38;5;241m.\u001B[39merfcx,\n\u001B[0;32m    745\u001B[0m     doc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    746\u001B[0m     type_promotion\u001B[38;5;241m=\u001B[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001B[38;5;241m.\u001B[39mDEFAULT,\n\u001B[0;32m    747\u001B[0m )\n\u001B[1;32m--> 749\u001B[0m exp \u001B[38;5;241m=\u001B[39m \u001B[43m_make_elementwise_unary_prim\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    750\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    751\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimpl_aten\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    752\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    753\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtype_promotion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDEFAULT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    754\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    756\u001B[0m expm1 \u001B[38;5;241m=\u001B[39m _make_elementwise_unary_prim(\n\u001B[0;32m    757\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpm1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    758\u001B[0m     impl_aten\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mspecial\u001B[38;5;241m.\u001B[39mexpm1,\n\u001B[0;32m    759\u001B[0m     doc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    760\u001B[0m     type_promotion\u001B[38;5;241m=\u001B[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001B[38;5;241m.\u001B[39mDEFAULT,\n\u001B[0;32m    761\u001B[0m )\n\u001B[0;32m    763\u001B[0m exp2 \u001B[38;5;241m=\u001B[39m _make_elementwise_unary_prim(\n\u001B[0;32m    764\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexp2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    765\u001B[0m     impl_aten\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mspecial\u001B[38;5;241m.\u001B[39mexp2,\n\u001B[0;32m    766\u001B[0m     doc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    767\u001B[0m     type_promotion\u001B[38;5;241m=\u001B[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001B[38;5;241m.\u001B[39mDEFAULT,\n\u001B[0;32m    768\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:491\u001B[0m, in \u001B[0;36m_make_elementwise_unary_prim\u001B[1;34m(name, type_promotion, **kwargs)\u001B[0m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_make_elementwise_unary_prim\u001B[39m(\n\u001B[0;32m    485\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    486\u001B[0m ):\n\u001B[0;32m    487\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;124;03m    Creates an elementwise unary prim.\u001B[39;00m\n\u001B[0;32m    489\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 491\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_make_prim\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m(Tensor self) -> Tensor\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_prim_elementwise_meta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtype_promotion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtype_promotion\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRETURN_TYPE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNEW\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:319\u001B[0m, in \u001B[0;36m_make_prim\u001B[1;34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001B[0m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m arg\u001B[38;5;241m.\u001B[39malias_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m arg\u001B[38;5;241m.\u001B[39malias_info\u001B[38;5;241m.\u001B[39mis_write:\n\u001B[0;32m    318\u001B[0m         mutates_args\u001B[38;5;241m.\u001B[39mappend(arg\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m--> 319\u001B[0m prim_def \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlibrary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcustom_op\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprims::\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_prim_impl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmutates_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmutates_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m    \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m prim_def\u001B[38;5;241m.\u001B[39mregister_fake(meta)\n\u001B[0;32m    327\u001B[0m \u001B[38;5;66;03m# all view ops get conj/neg fallthroughs\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:157\u001B[0m, in \u001B[0;36mcustom_op\u001B[1;34m(name, fn, mutates_args, device_types, schema)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner\n\u001B[1;32m--> 157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:138\u001B[0m, in \u001B[0;36mcustom_op.<locals>.inner\u001B[1;34m(fn)\u001B[0m\n\u001B[0;32m    135\u001B[0m     schema_str \u001B[38;5;241m=\u001B[39m schema\n\u001B[0;32m    137\u001B[0m namespace, opname \u001B[38;5;241m=\u001B[39m name\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m::\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 138\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mCustomOpDef\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m     expected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:185\u001B[0m, in \u001B[0;36mCustomOpDef.__init__\u001B[1;34m(self, namespace, name, schema, fn)\u001B[0m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_torch_dispatch_fns: Dict[\u001B[38;5;28mtype\u001B[39m, Callable] \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_vmap_fn: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lib \u001B[38;5;241m=\u001B[39m \u001B[43mget_library_allowing_overwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_namespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_to_dispatcher()\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disabled_kernel: Set \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:805\u001B[0m, in \u001B[0;36mget_library_allowing_overwrite\u001B[1;34m(namespace, name)\u001B[0m\n\u001B[0;32m    802\u001B[0m     OPDEF_TO_LIB[qualname]\u001B[38;5;241m.\u001B[39m_destroy()\n\u001B[0;32m    803\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m OPDEF_TO_LIB[qualname]\n\u001B[1;32m--> 805\u001B[0m lib \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlibrary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLibrary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFRAGMENT\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# noqa: TOR901\u001B[39;00m\n\u001B[0;32m    806\u001B[0m OPDEF_TO_LIB[qualname] \u001B[38;5;241m=\u001B[39m lib\n\u001B[0;32m    807\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m lib\n",
      "File \u001B[1;32m~\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\torch\\library.py:85\u001B[0m, in \u001B[0;36mLibrary.__init__\u001B[1;34m(self, ns, kind, dispatch_key)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ns \u001B[38;5;129;01min\u001B[39;00m _reserved_namespaces \u001B[38;5;129;01mand\u001B[39;00m (kind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEF\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m kind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFRAGMENT\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     81\u001B[0m         ns,\n\u001B[0;32m     82\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is a reserved namespace. Please try creating a library with another name.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     83\u001B[0m     )\n\u001B[1;32m---> 85\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[43mtraceback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     86\u001B[0m filename, lineno \u001B[38;5;241m=\u001B[39m frame\u001B[38;5;241m.\u001B[39mfilename, frame\u001B[38;5;241m.\u001B[39mlineno\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm: Optional[Any] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_dispatch_library(\n\u001B[0;32m     88\u001B[0m     kind, ns, dispatch_key, filename, lineno\n\u001B[0;32m     89\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:228\u001B[0m, in \u001B[0;36mextract_stack\u001B[1;34m(f, limit)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m f \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    227\u001B[0m     f \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39m_getframe()\u001B[38;5;241m.\u001B[39mf_back\n\u001B[1;32m--> 228\u001B[0m stack \u001B[38;5;241m=\u001B[39m \u001B[43mStackSummary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwalk_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m stack\u001B[38;5;241m.\u001B[39mreverse()\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stack\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:390\u001B[0m, in \u001B[0;36mStackSummary.extract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f, lineno \u001B[38;5;129;01min\u001B[39;00m frame_gen:\n\u001B[0;32m    388\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m f, (lineno, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mklass\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extract_from_extended_frame_gen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextended_frame_gen\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlookup_lines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlookup_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapture_locals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapture_locals\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:429\u001B[0m, in \u001B[0;36mStackSummary._extract_from_extended_frame_gen\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    425\u001B[0m     result\u001B[38;5;241m.\u001B[39mappend(FrameSummary(\n\u001B[0;32m    426\u001B[0m         filename, lineno, name, lookup_line\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28mlocals\u001B[39m\u001B[38;5;241m=\u001B[39mf_locals,\n\u001B[0;32m    427\u001B[0m         end_lineno\u001B[38;5;241m=\u001B[39mend_lineno, colno\u001B[38;5;241m=\u001B[39mcolno, end_colno\u001B[38;5;241m=\u001B[39mend_colno))\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m fnames:\n\u001B[1;32m--> 429\u001B[0m     \u001B[43mlinecache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheckcache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001B[39;00m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lookup_lines:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\linecache.py:72\u001B[0m, in \u001B[0;36mcheckcache\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m   \u001B[38;5;66;03m# no-op for files loaded via a __loader__\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 72\u001B[0m     stat \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mstat(fullname)\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m     74\u001B[0m     cache\u001B[38;5;241m.\u001B[39mpop(filename, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Finalise data",
   "id": "8afe185eaaf52025"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:14:57.892813Z",
     "start_time": "2025-03-05T04:14:56.749746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reduce dimensions a priori\n",
    "# The clustering does not perform to good. Some datapoints that clearly should be in a cluster based on eyeballing and their topic name but they are not. Rather often some points in a dense cluster are categorized as noise.\n",
    "# I will try to improve this by first perform a dimension reduction and then perform clustering. Reason being, that in high dimensions the data might be too sparse for the clustering algorithm to work properly.\n",
    "\n",
    "\n",
    "import umap\n",
    "\n",
    "path_db_embedded = os.path.join(root_dir, data_source, \"db_embedded.json\")\n",
    "\n",
    "with open(path_db_embedded, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n"
   ],
   "id": "f85b99377f2c0377",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:14:58.817990Z",
     "start_time": "2025-03-05T04:14:58.812716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counter = 0\n",
    "for entry in data:\n",
    "    print(counter)\n",
    "    counter += 1"
   ],
   "id": "a12830ab00febce4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:15:05.774323Z",
     "start_time": "2025-03-05T04:15:05.129063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gather all embeddings\n",
    "embeddings = [entry['embedding'] for entry in data]\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# Perform UMAP\n",
    "X_embedded = umap.UMAP(n_components=10).fit_transform(X) # 40 dimensions\n",
    "\n",
    "# Store the updated embeddings in the data\n",
    "for i, entry in enumerate(data):\n",
    "    entry['embedding'] = X_embedded[i].tolist()"
   ],
   "id": "579875192e0cbe10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:15:21.726433Z",
     "start_time": "2025-03-05T04:15:14.514531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helper.cluster_analysis import *\n",
    "from helper.utils import *\n",
    "\n",
    "# Adjustable parameters\n",
    "dimensionality_methods = ['UMAP', 'PCA', 'tSNE']\n",
    "hdbscan_params = {\"min_cluster_size\": 10, \"min_samples\": 1, \"cluster_selection_epsilon\": 0.15, \"cluster_selection_method\": \"leaf\"}           #, \"min_samples\": 2, \"cluster_selection_epsilon\": 0.15\n",
    "\n",
    "data = read_json(path_db_embedded)\n",
    "df = pd.DataFrame(data)\n",
    "df = df[df['embedding'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "print(f\"Loaded {len(df)} valid entries with embeddings.\")\n",
    "\n",
    "# Extract embeddings\n",
    "mat = np.array(df['embedding'].tolist())\n",
    "\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(**hdbscan_params)\n",
    "cluster_labels = hdbscan_clusterer.fit_predict(mat)\n",
    "\n",
    "reduction_results = {}\n",
    "\n",
    "for method in dimensionality_methods:\n",
    "    coords_2d = dimensionality_reduction(mat, method, n_components=2)\n",
    "    reduction_results[f'hdbscan_{method}_2D'] = {\n",
    "        'x': coords_2d[:, 0],\n",
    "        'y': coords_2d[:, 1]\n",
    "    }\n",
    "\n",
    "# 3D Reduction\n",
    "    coords_3d = dimensionality_reduction(mat, method, n_components=3)\n",
    "    reduction_results[f'hdbscan_{method}_3D'] = {\n",
    "        'x': coords_3d[:, 0],\n",
    "        'y': coords_3d[:, 1],\n",
    "        'z': coords_3d[:, 2]\n",
    "    }\n",
    "\n",
    "# Add dimensional coordinates to DataFrame\n",
    "for method_dim, coords in reduction_results.items():\n",
    "    for axis, values in coords.items():\n",
    "        df[f'{method_dim}_{axis}'] = values\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "df['hdbscan_id'] = cluster_labels\n"
   ],
   "id": "83a81ed4fc2b6475",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 998 valid entries with embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-03-05 17:15:16,852 - INFO - Applying UMAP with 2 components.\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-03-05 17:15:17,385 - INFO - Applying UMAP with 3 components.\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "2025-03-05 17:15:17,972 - INFO - Applying PCA with 2 components.\n",
      "2025-03-05 17:15:18,003 - INFO - Applying PCA with 3 components.\n",
      "2025-03-05 17:15:18,035 - INFO - Applying tSNE with 2 components.\n",
      "2025-03-05 17:15:18,036 - INFO - Perplexity not provided, setting to 30 based on sample size.\n",
      "2025-03-05 17:15:19,207 - INFO - Applying tSNE with 3 components.\n",
      "2025-03-05 17:15:19,208 - INFO - Perplexity not provided, setting to 30 based on sample size.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:15:21.739790Z",
     "start_time": "2025-03-05T04:15:21.734437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print unique cluster labels\n",
    "\n",
    "df['hdbscan_id'].unique()"
   ],
   "id": "8b9b4e010b427677",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  2,  8,  9, 11,  5, 13, 10,  3,  1, 12, 15, 14, 16,  4,  6,  7,\n",
       "        0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:15:31.572387Z",
     "start_time": "2025-03-06T01:15:31.569251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_db_clustered = os.path.join(root_dir, data_source, \"db_clustered.json\")\n",
    "\n",
    "save_df_as_json(df, path_db_clustered)"
   ],
   "id": "758eebd176f11c2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:15:46.660477Z",
     "start_time": "2025-03-06T01:15:35.178884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from helper.cluster_naming import *\n",
    "\n",
    "api_settings = {\"client\": client, \"model\": chat_model_name}\n",
    "\n",
    "def name_clusters(\n",
    "    df,\n",
    "    cluster_columns,\n",
    "    embedding_col=\"embedding\",\n",
    "    text_col=\"sentence\",\n",
    "    top_k=25,\n",
    "    skip_noise_label=-1\n",
    "):\n",
    "    for col in cluster_columns:\n",
    "        # Prepare a dict to store {cluster_id -> cluster_name}\n",
    "        cluster_id_to_name = {}\n",
    "        logger.info((f'Preparing to name clusters in column \"{col}\"'))\n",
    "\n",
    "        # Get unique cluster IDs from this column\n",
    "        cluster_ids = df[col].unique()\n",
    "\n",
    "        for cluster_id in cluster_ids:\n",
    "\n",
    "            if skip_noise_label is not None and cluster_id == skip_noise_label:\n",
    "                continue\n",
    "\n",
    "            # Select rows belonging to this cluster\n",
    "            cluster_data = df[df[col] == cluster_id]\n",
    "            if cluster_data.empty:\n",
    "                continue\n",
    "\n",
    "            # Compute centroid of embeddings\n",
    "            embeddings = np.array(cluster_data[embedding_col].tolist())\n",
    "            centroid = embeddings.mean(axis=0, dtype=np.float32, keepdims=True)\n",
    "\n",
    "            # Find top_k closest points to centroid\n",
    "            distances = cosine_distances(centroid, embeddings).flatten()\n",
    "            closest_indices = np.argsort(distances)[:top_k]\n",
    "            representative_texts = cluster_data.iloc[closest_indices][text_col].tolist()\n",
    "\n",
    "            # Call your naming function\n",
    "            cluster_name = generate_cluster_name(representative_texts, api_settings)\n",
    "            cluster_id_to_name[cluster_id] = cluster_name\n",
    "\n",
    "        # Create a new column with the cluster name for each row\n",
    "        name_col = f\"{col}_name\"\n",
    "        df[name_col] = df[col].apply(lambda cid: cluster_id_to_name.get(cid, \"Noise\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "data = read_json(path_db_clustered)  # data is probably a list of dicts\n",
    "df = pd.DataFrame(data)              # Convert to DataFrame\n",
    "\n",
    "cluster_columns = ['hdbscan_id'] #, 'kmeans_7_id', 'kmeans_10_id', 'kmeans_12_id', 'kmeans_15_id'\n",
    "\n",
    "df_named = name_clusters(\n",
    "    df,\n",
    "    cluster_columns,\n",
    "    embedding_col=\"embedding\",\n",
    "    text_col=\"sentence\",\n",
    "    top_k=10,\n",
    "    skip_noise_label=-1  # for HDBSCAN noise\n",
    ")\n"
   ],
   "id": "ccc9f5cbedabf0a4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 14:15:36,367 - INFO - Preparing to name clusters in column \"hdbscan_id\"\n",
      "2025-03-06 14:15:37,025 - INFO - Generated cluster name: Enhanced Tutorial for Better Gameplay\n",
      "2025-03-06 14:15:37,026 - INFO - Tokens used so far: Prompt Tokens: 249, Completion Tokens: 6\n",
      "2025-03-06 14:15:37,581 - INFO - Generated cluster name: Enhanced Survivor Action Options\n",
      "2025-03-06 14:15:37,584 - INFO - Tokens used so far: Prompt Tokens: 499, Completion Tokens: 11\n",
      "2025-03-06 14:15:38,019 - INFO - Generated cluster name: Character Interaction and Depth\n",
      "2025-03-06 14:15:38,019 - INFO - Tokens used so far: Prompt Tokens: 742, Completion Tokens: 16\n",
      "2025-03-06 14:15:38,456 - INFO - Generated cluster name: Automatic Weapon Management\n",
      "2025-03-06 14:15:38,457 - INFO - Tokens used so far: Prompt Tokens: 985, Completion Tokens: 20\n",
      "2025-03-06 14:15:38,983 - INFO - Generated cluster name: Game Difficulty Adjustment Suggestions\n",
      "2025-03-06 14:15:38,985 - INFO - Tokens used so far: Prompt Tokens: 1135, Completion Tokens: 25\n",
      "2025-03-06 14:15:39,679 - INFO - Generated cluster name: Weapon Durability Enhancement Needs\n",
      "2025-03-06 14:15:39,680 - INFO - Tokens used so far: Prompt Tokens: 1299, Completion Tokens: 31\n",
      "2025-03-06 14:15:40,120 - INFO - Generated cluster name: Character Customization and Diversity\n",
      "2025-03-06 14:15:40,121 - INFO - Tokens used so far: Prompt Tokens: 1425, Completion Tokens: 37\n",
      "2025-03-06 14:15:40,640 - INFO - Generated cluster name: No Removals Needed\n",
      "2025-03-06 14:15:40,642 - INFO - Tokens used so far: Prompt Tokens: 1583, Completion Tokens: 43\n",
      "2025-03-06 14:15:41,224 - INFO - Generated cluster name: Frustrations of Permanent Death Mechanics\n",
      "2025-03-06 14:15:41,227 - INFO - Tokens used so far: Prompt Tokens: 1825, Completion Tokens: 51\n",
      "2025-03-06 14:15:42,473 - INFO - Generated cluster name: Backpack Storage Expansion Options\n",
      "2025-03-06 14:15:42,474 - INFO - Tokens used so far: Prompt Tokens: 1989, Completion Tokens: 57\n",
      "2025-03-06 14:15:43,276 - INFO - Generated cluster name: Enhanced Stealth Kills in Zombie Combat\n",
      "2025-03-06 14:15:43,277 - INFO - Tokens used so far: Prompt Tokens: 2249, Completion Tokens: 66\n",
      "2025-03-06 14:15:43,797 - INFO - Generated cluster name: Enhanced Zombie Combat Mechanics\n",
      "2025-03-06 14:15:43,798 - INFO - Tokens used so far: Prompt Tokens: 2507, Completion Tokens: 71\n",
      "2025-03-06 14:15:44,291 - INFO - Generated cluster name: Interactive Zombie Mechanics\n",
      "2025-03-06 14:15:44,292 - INFO - Tokens used so far: Prompt Tokens: 2725, Completion Tokens: 75\n",
      "2025-03-06 14:15:44,940 - INFO - Generated cluster name: Enhanced Game Saving Options\n",
      "2025-03-06 14:15:44,941 - INFO - Tokens used so far: Prompt Tokens: 2860, Completion Tokens: 80\n",
      "2025-03-06 14:15:45,402 - INFO - Generated cluster name: This War of Mine-inspired Shelter Management\n",
      "2025-03-06 14:15:45,403 - INFO - Tokens used so far: Prompt Tokens: 3032, Completion Tokens: 88\n",
      "2025-03-06 14:15:46,105 - INFO - Generated cluster name: Loot Tracking and Exploration Enhancements\n",
      "2025-03-06 14:15:46,106 - INFO - Tokens used so far: Prompt Tokens: 3353, Completion Tokens: 95\n",
      "2025-03-06 14:15:46,656 - INFO - Generated cluster name: DLSS Performance and Ghosting Issues\n",
      "2025-03-06 14:15:46,657 - INFO - Tokens used so far: Prompt Tokens: 3595, Completion Tokens: 103\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:15:57.784266Z",
     "start_time": "2025-03-06T01:15:57.780925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the named clusters\n",
    "from helper.utils import save_data_for_streamlit\n",
    "\n",
    "path_db_final = os.path.join(root_dir, data_source, \"db_final.json\")\n",
    "\n",
    "#save_data_for_streamlit(df_named, path_db_final)"
   ],
   "id": "2c683ce4de503975",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:15:50.345893Z",
     "start_time": "2025-03-05T04:15:50.342716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# rename timestamp col\n",
    "df_named.rename(columns={'pp_timestamp':'timestamp_updated'}, inplace=True)\n",
    "#save_data_for_streamlit(df_named, path_db_final)"
   ],
   "id": "d6a31d942f1a502b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:21:13.921249Z",
     "start_time": "2025-03-05T04:21:13.917248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_timestamp_string_to_unix_ms(date_str):\n",
    "    \"\"\"\n",
    "    Parse a date string of the form 'MM/DD/YYYY HH:MM:SS AM/PM'\n",
    "    and convert it to a Unix timestamp in milliseconds.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_str, \"%m/%d/%Y %I:%M:%S %p\")\n",
    "    return 1741101588\n",
    "\n",
    "# We assume 'timestamp_updated' is the column to convert\n",
    "def parse_or_preserve(val):\n",
    "    \"\"\"\n",
    "    If the value is a string, try converting it.\n",
    "    If it's already numeric (maybe you have mixed data?), leave it as is.\n",
    "    \"\"\"\n",
    "    return 1741101588\n",
    "\n",
    "# Apply the function to each row in the 'timestamp_updated' column\n",
    "df_named[\"timestamp_updated\"] = df_named[\"timestamp_updated\"].apply(parse_or_preserve)\n",
    "\n",
    "# Save back to JSON\n",
    "# orient=\"records\" creates a list of JSON objects (arrays of dictionaries\n"
   ],
   "id": "c7eb5e9fe16b79ec",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:21:17.759178Z",
     "start_time": "2025-03-05T04:21:17.101536Z"
    }
   },
   "cell_type": "code",
   "source": "save_data_for_streamlit(df_named, path_db_final)",
   "id": "e94f2763d7140de1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:21:17,103 - INFO - Saving updated data to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\Backup\\Survey\\db_final.json\n",
      "2025-03-05 17:21:17,756 - INFO - Data saved successfully.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:40:40.660439Z",
     "start_time": "2025-03-06T01:40:40.449439Z"
    }
   },
   "cell_type": "code",
   "source": "data = read_json(\"S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\Data\\Backup\\Survey\\db_final.json\")",
   "id": "344deccd35ee96ce",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:42:10.083688Z",
     "start_time": "2025-03-06T01:42:10.079390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "for entry in data:\n",
    "    if \"session_length\" in entry:\n",
    "        # Only convert if not None\n",
    "        if entry[\"session_length\"] is not None:\n",
    "            entry[\"session_length\"] = entry[\"session_length\"] / 60.0  # seconds -> minutes\n",
    "\n",
    "        # Now rename the key\n",
    "        entry[\"playtime_at_review_minutes\"] = entry.pop(\"session_length\")\n",
    "\n",
    "\n"
   ],
   "id": "404cdbafefc9d19d",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:42:13.317980Z",
     "start_time": "2025-03-06T01:42:13.313920Z"
    }
   },
   "cell_type": "code",
   "source": "data[0]",
   "id": "40baedb2c05ec46c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Respondent ID': 118808000000,\n",
       " 'Collector ID': 459707592,\n",
       " 'Start Date': '02/26/2025 04:20:00 AM',\n",
       " 'IP Address': None,\n",
       " 'Email Address': None,\n",
       " 'First Name': None,\n",
       " 'Last Name': None,\n",
       " 'Custom Data 1': None,\n",
       " 'Please rate your overall experience playing Into the Dead: Our Darkest Days': 9,\n",
       " 'Please rate the overall difficulty playing Into the Dead: Our Darkest Days': 9,\n",
       " 'Weapon Durability,': 3,\n",
       " 'Combat,': 4.0,\n",
       " 'Stealth, ': 1.0,\n",
       " 'Tutorial,': 1.0,\n",
       " 'Managing Survivors in Shelter,': 4.0,\n",
       " 'Game Difficulty,': 4.0,\n",
       " 'Zombie Behaviour,': 4.0,\n",
       " 'Add,': ' the game needs to have replay value to be successful, I believe that adding different starts depending on the chosen duo would be a good way to add this value. Everyone starting from the same base can be monotonous over time.',\n",
       " 'Change,': ' Increase the variety of bases and weapons available depending on the duo, to further differentiate the gameplay between them. I also think there should be three shifts of gameplay, morning, afternoon and dawn (depending on the location, the dawn period reduces visibility and it is only possible to see with a flashlight and artificial light. Finally, I think it would be interesting to find other (initial) pairs throughout the gameplay and be able to recruit them, for example Dianne being optimistic would have a positive impact on being in the same group as Penny (who has trauma problems) or Hector with Sebastian, since the first is an engineer and the second is good at using weapons. All of this would add more gameplay to the game. The tutorial needs to be worked on if you are looking to reach a large number of players who are not hardcore or tryhards',\n",
       " 'Remove,': None,\n",
       " 'Had you heard of Into the Dead before this demo?': 'Never heard of it',\n",
       " 'What is your age group': '25-34',\n",
       " 'What is your gender?': 'Man',\n",
       " 'What country you are currently located in?': 'Brazil',\n",
       " 'Console,': None,\n",
       " 'Mobile,': None,\n",
       " 'PC/Computer,': 'PC/Computer',\n",
       " 'Virtual Reality,': None,\n",
       " 'Portable Console,': None,\n",
       " 'Other Platforms,': None,\n",
       " 'Mastering stealth,': 1.0,\n",
       " 'The apocalypse atmosphere,': 4.0,\n",
       " 'The presence of zombies,': 3.0,\n",
       " 'Experiencing being a survivor,': 5.0,\n",
       " 'Strategic gameplay,': 2.0,\n",
       " 'pcubed_id': 76561198199667904,\n",
       " 'ov_game_starts': 39.0,\n",
       " 'ov_game_ends': 3.0,\n",
       " 'ov_phase_starts': 356.0,\n",
       " 'ov_phase_ends': 318.0,\n",
       " 'ov_level_starts': 258.0,\n",
       " 'ov_level_ends': 231.0,\n",
       " 'More or less than 1 hour': '1hour+',\n",
       " 'session_count': 38.0,\n",
       " 'sebastian_phases': 128.0,\n",
       " 'dianne_phases': 128.0,\n",
       " 'darrel_phases': 88.0,\n",
       " 'leo_phases': 88.0,\n",
       " 'daphne_phases': 42.0,\n",
       " 'penny_phases': 42.0,\n",
       " 'kayla_phases': 32.0,\n",
       " 'hector_phases': 32.0,\n",
       " 'rahul_phases': 102.0,\n",
       " 'joe_phases': 52.0,\n",
       " 'aubrey_phases': 50.0,\n",
       " 'frank_phases': 0.0,\n",
       " 'review_id': '118808000000:76561198199667904',\n",
       " 'pp_data_source': 'Survey',\n",
       " 'pp_review_id': 267,\n",
       " 'pp_review': ' the game needs to have replay value to be successful, I believe that adding different starts depending on the chosen duo would be a good way to add this value. Everyone starting from the same base can be monotonous over time.  Increase the variety of bases and weapons available depending on the duo, to further differentiate the gameplay between them. I also think there should be three shifts of gameplay, morning, afternoon and dawn (depending on the location, the dawn period reduces visibility and it is only possible to see with a flashlight and artificial light. Finally, I think it would be interesting to find other (initial) pairs throughout the gameplay and be able to recruit them, for example Dianne being optimistic would have a positive impact on being in the same group as Penny (who has trauma problems) or Hector with Sebastian, since the first is an engineer and the second is good at using weapons. All of this would add more gameplay to the game. The tutorial needs to be worked on if you are looking to reach a large number of players who are not hardcore or tryhards',\n",
       " 'timestamp_updated': 1741101588,\n",
       " 'language': 'english',\n",
       " 'topic': 'Replay Value',\n",
       " 'sentiment': 'Positive',\n",
       " 'category': 'request',\n",
       " 'sentence': 'The game needs to have replay value to be successful, I believe that adding different starts depending on the chosen duo would be a good way to add this value.',\n",
       " 'hdbscan_UMAP_2D_x': -0.2907000482,\n",
       " 'hdbscan_UMAP_2D_y': 2.0410912037,\n",
       " 'hdbscan_UMAP_3D_x': -0.211830467,\n",
       " 'hdbscan_UMAP_3D_y': 1.2868508101,\n",
       " 'hdbscan_UMAP_3D_z': 6.0352754593,\n",
       " 'hdbscan_PCA_2D_x': 0.0881991645,\n",
       " 'hdbscan_PCA_2D_y': -0.0361589492,\n",
       " 'hdbscan_PCA_3D_x': 0.0882090352,\n",
       " 'hdbscan_PCA_3D_y': -0.036198847,\n",
       " 'hdbscan_PCA_3D_z': 0.0479319646,\n",
       " 'hdbscan_tSNE_2D_x': 9.7021980286,\n",
       " 'hdbscan_tSNE_2D_y': -14.8363103867,\n",
       " 'hdbscan_tSNE_3D_x': 12.0545988083,\n",
       " 'hdbscan_tSNE_3D_y': -11.6923360825,\n",
       " 'hdbscan_tSNE_3D_z': 11.1981973648,\n",
       " 'hdbscan_id': -1,\n",
       " 'hdbscan_id_name': 'Noise',\n",
       " 'playtime_at_review_minutes': 605.6333333333333}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:41:38.905060Z",
     "start_time": "2025-03-06T01:41:38.889043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for entry in data:\n",
    "    entry.pop(\"embedding\")"
   ],
   "id": "1bdec6b26679b17b",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:41:46.522644Z",
     "start_time": "2025-03-06T01:41:46.518136Z"
    }
   },
   "cell_type": "code",
   "source": "data[0]",
   "id": "5eaf892b6c5bb0b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Respondent ID': 118808000000,\n",
       " 'Collector ID': 459707592,\n",
       " 'Start Date': '02/26/2025 04:20:00 AM',\n",
       " 'IP Address': None,\n",
       " 'Email Address': None,\n",
       " 'First Name': None,\n",
       " 'Last Name': None,\n",
       " 'Custom Data 1': None,\n",
       " 'Please rate your overall experience playing Into the Dead: Our Darkest Days': 9,\n",
       " 'Please rate the overall difficulty playing Into the Dead: Our Darkest Days': 9,\n",
       " 'Weapon Durability,': 3,\n",
       " 'Combat,': 4.0,\n",
       " 'Stealth, ': 1.0,\n",
       " 'Tutorial,': 1.0,\n",
       " 'Managing Survivors in Shelter,': 4.0,\n",
       " 'Game Difficulty,': 4.0,\n",
       " 'Zombie Behaviour,': 4.0,\n",
       " 'Add,': ' the game needs to have replay value to be successful, I believe that adding different starts depending on the chosen duo would be a good way to add this value. Everyone starting from the same base can be monotonous over time.',\n",
       " 'Change,': ' Increase the variety of bases and weapons available depending on the duo, to further differentiate the gameplay between them. I also think there should be three shifts of gameplay, morning, afternoon and dawn (depending on the location, the dawn period reduces visibility and it is only possible to see with a flashlight and artificial light. Finally, I think it would be interesting to find other (initial) pairs throughout the gameplay and be able to recruit them, for example Dianne being optimistic would have a positive impact on being in the same group as Penny (who has trauma problems) or Hector with Sebastian, since the first is an engineer and the second is good at using weapons. All of this would add more gameplay to the game. The tutorial needs to be worked on if you are looking to reach a large number of players who are not hardcore or tryhards',\n",
       " 'Remove,': None,\n",
       " 'Had you heard of Into the Dead before this demo?': 'Never heard of it',\n",
       " 'What is your age group': '25-34',\n",
       " 'What is your gender?': 'Man',\n",
       " 'What country you are currently located in?': 'Brazil',\n",
       " 'Console,': None,\n",
       " 'Mobile,': None,\n",
       " 'PC/Computer,': 'PC/Computer',\n",
       " 'Virtual Reality,': None,\n",
       " 'Portable Console,': None,\n",
       " 'Other Platforms,': None,\n",
       " 'Mastering stealth,': 1.0,\n",
       " 'The apocalypse atmosphere,': 4.0,\n",
       " 'The presence of zombies,': 3.0,\n",
       " 'Experiencing being a survivor,': 5.0,\n",
       " 'Strategic gameplay,': 2.0,\n",
       " 'pcubed_id': 76561198199667904,\n",
       " 'ov_game_starts': 39.0,\n",
       " 'ov_game_ends': 3.0,\n",
       " 'ov_phase_starts': 356.0,\n",
       " 'ov_phase_ends': 318.0,\n",
       " 'ov_level_starts': 258.0,\n",
       " 'ov_level_ends': 231.0,\n",
       " 'session_length': 36338.0,\n",
       " 'More or less than 1 hour': '1hour+',\n",
       " 'session_count': 38.0,\n",
       " 'sebastian_phases': 128.0,\n",
       " 'dianne_phases': 128.0,\n",
       " 'darrel_phases': 88.0,\n",
       " 'leo_phases': 88.0,\n",
       " 'daphne_phases': 42.0,\n",
       " 'penny_phases': 42.0,\n",
       " 'kayla_phases': 32.0,\n",
       " 'hector_phases': 32.0,\n",
       " 'rahul_phases': 102.0,\n",
       " 'joe_phases': 52.0,\n",
       " 'aubrey_phases': 50.0,\n",
       " 'frank_phases': 0.0,\n",
       " 'review_id': '118808000000:76561198199667904',\n",
       " 'pp_data_source': 'Survey',\n",
       " 'pp_review_id': 267,\n",
       " 'pp_review': ' the game needs to have replay value to be successful, I believe that adding different starts depending on the chosen duo would be a good way to add this value. Everyone starting from the same base can be monotonous over time.  Increase the variety of bases and weapons available depending on the duo, to further differentiate the gameplay between them. I also think there should be three shifts of gameplay, morning, afternoon and dawn (depending on the location, the dawn period reduces visibility and it is only possible to see with a flashlight and artificial light. Finally, I think it would be interesting to find other (initial) pairs throughout the gameplay and be able to recruit them, for example Dianne being optimistic would have a positive impact on being in the same group as Penny (who has trauma problems) or Hector with Sebastian, since the first is an engineer and the second is good at using weapons. All of this would add more gameplay to the game. The tutorial needs to be worked on if you are looking to reach a large number of players who are not hardcore or tryhards',\n",
       " 'timestamp_updated': 1741101588,\n",
       " 'language': 'english',\n",
       " 'topic': 'Replay Value',\n",
       " 'sentiment': 'Positive',\n",
       " 'category': 'request',\n",
       " 'sentence': 'The game needs to have replay value to be successful, I believe that adding different starts depending on the chosen duo would be a good way to add this value.',\n",
       " 'hdbscan_UMAP_2D_x': -0.2907000482,\n",
       " 'hdbscan_UMAP_2D_y': 2.0410912037,\n",
       " 'hdbscan_UMAP_3D_x': -0.211830467,\n",
       " 'hdbscan_UMAP_3D_y': 1.2868508101,\n",
       " 'hdbscan_UMAP_3D_z': 6.0352754593,\n",
       " 'hdbscan_PCA_2D_x': 0.0881991645,\n",
       " 'hdbscan_PCA_2D_y': -0.0361589492,\n",
       " 'hdbscan_PCA_3D_x': 0.0882090352,\n",
       " 'hdbscan_PCA_3D_y': -0.036198847,\n",
       " 'hdbscan_PCA_3D_z': 0.0479319646,\n",
       " 'hdbscan_tSNE_2D_x': 9.7021980286,\n",
       " 'hdbscan_tSNE_2D_y': -14.8363103867,\n",
       " 'hdbscan_tSNE_3D_x': 12.0545988083,\n",
       " 'hdbscan_tSNE_3D_y': -11.6923360825,\n",
       " 'hdbscan_tSNE_3D_z': 11.1981973648,\n",
       " 'hdbscan_id': -1,\n",
       " 'hdbscan_id_name': 'Noise'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:44:45.427055Z",
     "start_time": "2025-03-06T01:44:45.407054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert json to df\n",
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ],
   "id": "c0ba7a53f3685450",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Respondent ID  Collector ID              Start Date IP Address  \\\n",
       "0   118808000000     459707592  02/26/2025 04:20:00 AM       None   \n",
       "1   118808000000     459707592  02/26/2025 04:20:00 AM       None   \n",
       "2   118808000000     459707592  02/26/2025 04:20:00 AM       None   \n",
       "3   118808000000     459707592  02/26/2025 04:20:00 AM       None   \n",
       "4   118808000000     459707592  02/26/2025 04:20:00 AM       None   \n",
       "5   118808000000     459707592  02/26/2025 04:20:00 AM       None   \n",
       "6   118807000000     459707592  02/24/2025 02:20:29 PM       None   \n",
       "7   118807000000     459707592  02/24/2025 02:20:29 PM       None   \n",
       "8   118807000000     459707592  02/24/2025 02:20:29 PM       None   \n",
       "9   118810000000     459707592  02/28/2025 04:26:23 AM       None   \n",
       "\n",
       "  Email Address First Name Last Name Custom Data 1  \\\n",
       "0          None       None      None          None   \n",
       "1          None       None      None          None   \n",
       "2          None       None      None          None   \n",
       "3          None       None      None          None   \n",
       "4          None       None      None          None   \n",
       "5          None       None      None          None   \n",
       "6          None       None      None          None   \n",
       "7          None       None      None          None   \n",
       "8          None       None      None          None   \n",
       "9          None       None      None          None   \n",
       "\n",
       "   Please rate your overall experience playing Into the Dead: Our Darkest Days  \\\n",
       "0                                                  9                             \n",
       "1                                                  9                             \n",
       "2                                                  9                             \n",
       "3                                                  9                             \n",
       "4                                                  9                             \n",
       "5                                                  9                             \n",
       "6                                                 10                             \n",
       "7                                                 10                             \n",
       "8                                                 10                             \n",
       "9                                                  8                             \n",
       "\n",
       "   Please rate the overall difficulty playing Into the Dead: Our Darkest Days  \\\n",
       "0                                                  9                            \n",
       "1                                                  9                            \n",
       "2                                                  9                            \n",
       "3                                                  9                            \n",
       "4                                                  9                            \n",
       "5                                                  9                            \n",
       "6                                                  7                            \n",
       "7                                                  7                            \n",
       "8                                                  7                            \n",
       "9                                                  8                            \n",
       "\n",
       "   ...  hdbscan_PCA_3D_y  hdbscan_PCA_3D_z  hdbscan_tSNE_2D_x  \\\n",
       "0  ...         -0.036199          0.047932           9.702198   \n",
       "1  ...          0.194630         -0.202012          -4.958783   \n",
       "2  ...         -0.063228          0.068984         -15.168633   \n",
       "3  ...         -0.129826         -0.023552           9.655229   \n",
       "4  ...         -0.172843          0.028235          12.192753   \n",
       "5  ...          0.104338          0.099757          23.097298   \n",
       "6  ...         -0.361507         -0.159220          -3.968874   \n",
       "7  ...         -0.267633         -0.218185         -16.112844   \n",
       "8  ...         -0.133891          0.067835         -34.618080   \n",
       "9  ...          0.266910         -0.060933         -30.268156   \n",
       "\n",
       "   hdbscan_tSNE_2D_y  hdbscan_tSNE_3D_x  hdbscan_tSNE_3D_y  hdbscan_tSNE_3D_z  \\\n",
       "0         -14.836310          12.054599         -11.692336          11.198197   \n",
       "1          28.009903           0.704799          18.252508           4.860467   \n",
       "2         -11.463284         -14.410099         -13.880930         -13.493813   \n",
       "3         -16.443623          30.611814           1.451048         -13.610722   \n",
       "4         -23.781454          43.830521          -7.462946         -19.306606   \n",
       "5           0.367160          10.457623           8.907899          22.650236   \n",
       "6         -22.280281          -0.087085         -12.603081         -36.124470   \n",
       "7         -21.151455          20.211393           4.539205         -18.108576   \n",
       "8          -4.546374         -30.011858           0.678308         -17.694992   \n",
       "9          22.613737         -17.750408          11.560346          26.786020   \n",
       "\n",
       "  hdbscan_id                               hdbscan_id_name  \\\n",
       "0         -1                                         Noise   \n",
       "1         -1                                         Noise   \n",
       "2         -1                                         Noise   \n",
       "3         -1                                         Noise   \n",
       "4         -1                                         Noise   \n",
       "5          2         Enhanced Tutorial Improvements Needed   \n",
       "6          8          Enhanced Survivor Engagement Options   \n",
       "7          9  Character Interaction and Relationship Depth   \n",
       "8         -1                                         Noise   \n",
       "9         -1                                         Noise   \n",
       "\n",
       "  playtime_at_review_minutes  \n",
       "0                 605.633333  \n",
       "1                 605.633333  \n",
       "2                 605.633333  \n",
       "3                 605.633333  \n",
       "4                 605.633333  \n",
       "5                 605.633333  \n",
       "6                 555.083333  \n",
       "7                 555.083333  \n",
       "8                 555.083333  \n",
       "9                 522.916667  \n",
       "\n",
       "[10 rows x 84 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Respondent ID</th>\n",
       "      <th>Collector ID</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>IP Address</th>\n",
       "      <th>Email Address</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Custom Data 1</th>\n",
       "      <th>Please rate your overall experience playing Into the Dead: Our Darkest Days</th>\n",
       "      <th>Please rate the overall difficulty playing Into the Dead: Our Darkest Days</th>\n",
       "      <th>...</th>\n",
       "      <th>hdbscan_PCA_3D_y</th>\n",
       "      <th>hdbscan_PCA_3D_z</th>\n",
       "      <th>hdbscan_tSNE_2D_x</th>\n",
       "      <th>hdbscan_tSNE_2D_y</th>\n",
       "      <th>hdbscan_tSNE_3D_x</th>\n",
       "      <th>hdbscan_tSNE_3D_y</th>\n",
       "      <th>hdbscan_tSNE_3D_z</th>\n",
       "      <th>hdbscan_id</th>\n",
       "      <th>hdbscan_id_name</th>\n",
       "      <th>playtime_at_review_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118808000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/26/2025 04:20:00 AM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036199</td>\n",
       "      <td>0.047932</td>\n",
       "      <td>9.702198</td>\n",
       "      <td>-14.836310</td>\n",
       "      <td>12.054599</td>\n",
       "      <td>-11.692336</td>\n",
       "      <td>11.198197</td>\n",
       "      <td>-1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>605.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118808000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/26/2025 04:20:00 AM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194630</td>\n",
       "      <td>-0.202012</td>\n",
       "      <td>-4.958783</td>\n",
       "      <td>28.009903</td>\n",
       "      <td>0.704799</td>\n",
       "      <td>18.252508</td>\n",
       "      <td>4.860467</td>\n",
       "      <td>-1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>605.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118808000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/26/2025 04:20:00 AM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063228</td>\n",
       "      <td>0.068984</td>\n",
       "      <td>-15.168633</td>\n",
       "      <td>-11.463284</td>\n",
       "      <td>-14.410099</td>\n",
       "      <td>-13.880930</td>\n",
       "      <td>-13.493813</td>\n",
       "      <td>-1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>605.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118808000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/26/2025 04:20:00 AM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129826</td>\n",
       "      <td>-0.023552</td>\n",
       "      <td>9.655229</td>\n",
       "      <td>-16.443623</td>\n",
       "      <td>30.611814</td>\n",
       "      <td>1.451048</td>\n",
       "      <td>-13.610722</td>\n",
       "      <td>-1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>605.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118808000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/26/2025 04:20:00 AM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172843</td>\n",
       "      <td>0.028235</td>\n",
       "      <td>12.192753</td>\n",
       "      <td>-23.781454</td>\n",
       "      <td>43.830521</td>\n",
       "      <td>-7.462946</td>\n",
       "      <td>-19.306606</td>\n",
       "      <td>-1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>605.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118808000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/26/2025 04:20:00 AM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104338</td>\n",
       "      <td>0.099757</td>\n",
       "      <td>23.097298</td>\n",
       "      <td>0.367160</td>\n",
       "      <td>10.457623</td>\n",
       "      <td>8.907899</td>\n",
       "      <td>22.650236</td>\n",
       "      <td>2</td>\n",
       "      <td>Enhanced Tutorial Improvements Needed</td>\n",
       "      <td>605.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>118807000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/24/2025 02:20:29 PM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361507</td>\n",
       "      <td>-0.159220</td>\n",
       "      <td>-3.968874</td>\n",
       "      <td>-22.280281</td>\n",
       "      <td>-0.087085</td>\n",
       "      <td>-12.603081</td>\n",
       "      <td>-36.124470</td>\n",
       "      <td>8</td>\n",
       "      <td>Enhanced Survivor Engagement Options</td>\n",
       "      <td>555.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>118807000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/24/2025 02:20:29 PM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267633</td>\n",
       "      <td>-0.218185</td>\n",
       "      <td>-16.112844</td>\n",
       "      <td>-21.151455</td>\n",
       "      <td>20.211393</td>\n",
       "      <td>4.539205</td>\n",
       "      <td>-18.108576</td>\n",
       "      <td>9</td>\n",
       "      <td>Character Interaction and Relationship Depth</td>\n",
       "      <td>555.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>118807000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/24/2025 02:20:29 PM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133891</td>\n",
       "      <td>0.067835</td>\n",
       "      <td>-34.618080</td>\n",
       "      <td>-4.546374</td>\n",
       "      <td>-30.011858</td>\n",
       "      <td>0.678308</td>\n",
       "      <td>-17.694992</td>\n",
       "      <td>-1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>555.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>118810000000</td>\n",
       "      <td>459707592</td>\n",
       "      <td>02/28/2025 04:26:23 AM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266910</td>\n",
       "      <td>-0.060933</td>\n",
       "      <td>-30.268156</td>\n",
       "      <td>22.613737</td>\n",
       "      <td>-17.750408</td>\n",
       "      <td>11.560346</td>\n",
       "      <td>26.786020</td>\n",
       "      <td>-1</td>\n",
       "      <td>Noise</td>\n",
       "      <td>522.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 84 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:44:56.781186Z",
     "start_time": "2025-03-06T01:44:56.707518Z"
    }
   },
   "cell_type": "code",
   "source": "save_data_for_streamlit(df, \"S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\db_final.json\")",
   "id": "c5d9c28fc7f07da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 14:44:56,708 - INFO - Saving updated data to S:\\SID\\Analytics\\Working Files\\Individual\\Florian\\Projects\\DataScience\\cluster_analysis\\db_final.json\n",
      "2025-03-06 14:44:56,779 - INFO - Data saved successfully.\n"
     ]
    }
   ],
   "execution_count": 48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
