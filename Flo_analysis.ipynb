{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from umap import umap_ as UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n"
   ],
   "id": "57b34f0095aa0031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "client = openai.Client()\n",
    "\n",
    "chat_model_name = 'gpt-4o-mini'\n",
    "embedding_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "s_root = r'C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/'\n",
    "s_db_json = 'Data/survey_results_with_topics.json'\n",
    "\n",
    "s_db_embed_json = 'Data/review_db_embed.json'\n",
    "s_db_table_json = 'Data/review_db_table.json'\n",
    "s_db_table_xlsx = 'Data/review_db_table.xlsx'\n",
    "s_db_table_pca_json = 'Data/review_db_table_pca.json'\n",
    "s_db_table_pca_xlsx = 'Data/review_db_table_pca.xlsx'\n",
    "s_kmeans_centers = 'Data/kmeans_centers.json'\n",
    "b_override = False"
   ],
   "id": "1794c8356c469726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "prompt_template_translation = PromptTemplate.from_template(\n",
    "'''Please translate each section into English if it is not. The sections are separated by labels \"REASON\" and \"WISH\".\n",
    "\n",
    "[h0]==================================================================[\\h0]\n",
    "REASON: \"兄弟们，我把星空退款的钱拿来买这个了，我做的对吗\"\n",
    "WISH: \"加动态模糊和垂直同步选项\"\n",
    "\n",
    "TRANSLATION:\n",
    "\n",
    "REASON: \"Brothers, I used the refund money from the stars to buy this. Did I do the right thing?\"\n",
    "WISH: \"Add dynamic blur and vertical sync options.\"\n",
    "\n",
    "\n",
    "[h0]==================================================================[\\h0]\n",
    "REASON: \"My first D&D experience and I'm enjoying it a lot.\"\n",
    "WISH: \"I would like more guidance in the game.\"\n",
    "\n",
    "TRANSLATION:\n",
    "\n",
    "REASON: \"My first D&D experience and I'm enjoying it a lot.\"\n",
    "WISH: \"I would like more guidance in the game.\"\n",
    "\n",
    "[h0]==================================================================[\\h0]\n",
    "REASON: \"{reason}\"\n",
    "WISH: \"{wish}\"\n",
    "\n",
    "TRANSLATION:\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "prompt_template_topic = PromptTemplate.from_template(\n",
    "'''Please list the most important topics and their respective original context in the review of a game in a json format with \"Topic\", \"Category\", \"Context\" arguments.  No more than 10 topics.\n",
    "Topics should be game features.  A feature in the game should be a noun rather than a verb or an adjective.\n",
    "Each topic should be categorized as a \"fact\" or a \"request\".\n",
    "Respond in JSON format.\n",
    "\n",
    "[h0]==================================================================[\\h0]\n",
    "REVIEW: \n",
    "\n",
    "\"The weapon durability in this game is frustrating; my sword breaks after just a few swings. The combat itself is fun, but I wish the durability lasted longer. Also, the audio effects are very immersive during battles.\"\n",
    "\n",
    "TOPICS:\n",
    "\n",
    "{{\"Topics\":\n",
    "    [\n",
    "        {{\n",
    "            \"Topic\": \"Weapon Durability\",\n",
    "            \"Category\": \"request\",\n",
    "            \"Context\": \"My sword breaks after just a few swings. I wish the durability lasted longer.\"\n",
    "        }},\n",
    "        {{\n",
    "            \"Topic\": \"Combat and Fighting\",\n",
    "            \"Category\": \"fact\",\n",
    "            \"Context\": \"The combat itself is fun.\"\n",
    "        }},\n",
    "        {{\n",
    "            \"Topic\": \"Audio\",\n",
    "            \"Category\": \"fact\",\n",
    "            \"Context\": \"The audio effects are very immersive during battles.\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "[h0]==================================================================[\\h0]\n",
    "REVIEW: \n",
    "\n",
    "\"Playing during the night adds a thrilling layer to the game. The lack of a proper save feature makes it hard to enjoy it though. Also, there are way too many random encounters that make progress difficult.\"\n",
    "\n",
    "TOPICS:\n",
    "\n",
    "{{\"Topics\":\n",
    "    [\n",
    "        {{\n",
    "            \"Topic\": \"Night\",\n",
    "            \"Category\": \"fact\",\n",
    "            \"Context\": \"Playing during the night adds a thrilling layer to the game.\"\n",
    "        }},\n",
    "        {{\n",
    "            \"Topic\": \"Save Feature\",\n",
    "            \"Category\": \"request\",\n",
    "            \"Context\": \"The lack of a proper save feature makes it hard to enjoy fully.\"\n",
    "        }},\n",
    "        {{\n",
    "            \"Topic\": \"Randomness\",\n",
    "            \"Category\": \"request\",\n",
    "            \"Context\": \"There are way too many random encounters that make progress difficult.\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "[h0]==================================================================[\\h0]\n",
    "REVIEW: \n",
    "\n",
    "\"{review}\"\n",
    "\n",
    "TOPICS:\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "prompt_template_topic_view = PromptTemplate.from_template(\n",
    "'''What's the sentiment of the review with regard to the topic?\n",
    "Always answer with 'Positive' or 'Negative' or 'Inconclusive'.\n",
    "\n",
    "REVIEW: My first D&D experience and I'm enjoying it a lot.\n",
    "TOPIC: D&D\n",
    "SENTIMENT: Positive \n",
    "\n",
    "REVIEW: This game lacks a proper ending or epilog\n",
    "TOPIC: epilogue\n",
    "SENTIMENT: Negative\n",
    "\n",
    "REVIEW: Posted: August 8\n",
    "TOPIC: release date\n",
    "SENTIMENT: Inconclusive \n",
    "\n",
    "REVIEW: {review}\n",
    "TOPIC: {topic}\n",
    "SENTIMENT: '''\n",
    ")"
   ],
   "id": "3a22efb6ab06782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read in the JSON file with survey results \n",
    "\n",
    "with open(s_root + 'Data/survey_results_clean.json', 'r') as f:\n",
    "    db = json.load(f)"
   ],
   "id": "b0835628c67edb43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Translate reviews",
   "id": "a116b6f5dc93843d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "# Load JSON data from file\n",
    "input_file_path = 'Data/survey_results_clean.json'  # Adjust the path if needed\n",
    "output_file_path = 'Data/survey_results_trans.json'  # New JSON with language and translations\n",
    "\n",
    "# Initialize the language detector\n",
    "detector = LanguageDetectorBuilder.from_languages(\n",
    "    Language.ENGLISH, Language.SPANISH, Language.CHINESE, Language.GERMAN, Language.FRENCH\n",
    ").build()\n",
    "\n",
    "# Load JSON data\n",
    "with open(input_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Process each entry\n",
    "for entry in data:\n",
    "    # Get the values for reason and wish fields, making sure NaNs are handled properly\n",
    "    reason_text = entry.get(\"Please tell us why you chose the rating above:\")\n",
    "    wish_text = entry.get(\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\")\n",
    "\n",
    "    # Initialize detected language as unknown\n",
    "    detected_language = \"unknown\"\n",
    "    \n",
    "    # Determine language only for fields with actual text\n",
    "    if isinstance(reason_text, str) and reason_text.strip():\n",
    "        detected_language_reason = detector.detect_language_of(reason_text).name.lower()\n",
    "    else:\n",
    "        detected_language_reason = \"none\"\n",
    "\n",
    "    if isinstance(wish_text, str) and wish_text.strip():\n",
    "        detected_language_wish = detector.detect_language_of(wish_text).name.lower()\n",
    "    else:\n",
    "        detected_language_wish = \"none\"\n",
    "    \n",
    "    # Set the overall detected language based on valid fields\n",
    "    if detected_language_reason != \"none\" and detected_language_reason == detected_language_wish:\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_reason != \"none\" and detected_language_reason != detected_language_wish:\n",
    "        detected_language = \"mixed\"\n",
    "    elif detected_language_reason != \"none\":\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_wish != \"none\":\n",
    "        detected_language = detected_language_wish\n",
    "    \n",
    "    # Save the detected language in the JSON entry\n",
    "    entry[\"language\"] = detected_language\n",
    "\n",
    "    # Only proceed with translation if:\n",
    "    # - There is text in either reason_text or wish_text\n",
    "    # - The detected language is not English\n",
    "    if detected_language not in [\"english\", \"none\"]:\n",
    "        # Prepare the prompt with only the fields that have text\n",
    "        reason_text_for_prompt = reason_text if detected_language_reason != \"none\" else \"N/A\"\n",
    "        wish_text_for_prompt = wish_text if detected_language_wish != \"none\" else \"N/A\"\n",
    "        \n",
    "        prompt_translation = prompt_template_translation.format(\n",
    "            reason=reason_text_for_prompt, \n",
    "            wish=wish_text_for_prompt\n",
    "        )\n",
    "\n",
    "        # Make the OpenAI API call to translate the review\n",
    "        response = client.chat.completions.create(\n",
    "            model=chat_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant expertised in game review analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_translation},\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "\n",
    "        # Extract the translation response\n",
    "        translation_response = response.choices[0].message.content\n",
    "\n",
    "        # Update the entry only with translated text if present\n",
    "        if \"REASON:\" in translation_response and detected_language_reason != \"none\":\n",
    "            reason_translation = translation_response.split(\"REASON:\")[1].split(\"WISH:\")[0].strip()\n",
    "            entry[\"Please tell us why you chose the rating above:\"] = reason_translation\n",
    "\n",
    "        if \"WISH:\" in translation_response and detected_language_wish != \"none\":\n",
    "            wish_translation = translation_response.split(\"WISH:\")[1].strip()\n",
    "            entry[\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\"] = wish_translation\n",
    "\n",
    "# Save the modified data with translations\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Translated data saved to {output_file_path}\")\n"
   ],
   "id": "350720ac15b19d48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extract Topics",
   "id": "2f7a86d82ca0d334"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(s_root + 'Data/survey_results_trans.json', 'r') as f:\n",
    "    db = json.load(f)\n",
    "\n",
    "entry = db[0]\n",
    "\n",
    "# Extract important information from the 2nd and 3rd keys\n",
    "review_text = entry[\"Please tell us why you chose the rating above:\"]\n",
    "additional_feedback = entry[\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\"]\n",
    "\n",
    "# Combine both into a single review input for the prompt\n",
    "combined_review = f\"{review_text} {additional_feedback}\"\n",
    "\n",
    "# Format the prompt for the LLM\n",
    "prompt_topic = prompt_template_topic.format(review=combined_review)\n",
    "\n",
    "# Make the OpenAI API call\n",
    "response = client.chat.completions.create(\n",
    "    model=chat_model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant expertised in game review analysis. Respond in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_topic},\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    response_format={\n",
    "    \"type\": \"json_object\"\n",
    "  }\n",
    ")\n",
    "\n",
    "# Print the response content\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "add531e9c4e67be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "combined_review",
   "id": "26432a89b2f7ed05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Analysis",
   "id": "5cb23e822d8e08ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "topic_response = response.choices[0].message.content\n",
    "topics = json.loads(topic_response)"
   ],
   "id": "4f09281c21010295"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "topics",
   "id": "807a4eb7dbcba890"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Iterate over each topic in the nested structure\n",
    "for topic in topics['Topics']:\n",
    "    topic_text = topic[\"Topic\"]\n",
    "    topic_context = topic[\"Context\"]\n",
    "    \n",
    "    # Format the prompt for sentiment analysis\n",
    "    prompt_sentiment = prompt_template_topic_view.format(review=topic_context, topic=topic_text)\n",
    "    \n",
    "    # Call the API for sentiment analysis\n",
    "    sentiment_response = client.chat.completions.create(\n",
    "        model=chat_model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant expertised in sentiment analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_sentiment},\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    # Extract the sentiment from the response\n",
    "    sentiment = sentiment_response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Ensure 'topics' key is initialized in the entry\n",
    "    if \"topics\" not in entry:\n",
    "        entry[\"topics\"] = []\n",
    "\n",
    "    # Append the topic information with sentiment to the \"topics\" list\n",
    "    entry[\"topics\"].append({\n",
    "        \"topic\": topic_text,\n",
    "        \"context\": topic_context,\n",
    "        \"category\": topic[\"Category\"],  # Add category from original data\n",
    "        \"sentiment\": sentiment\n",
    "    })\n",
    "\n",
    "    # Print for confirmation\n",
    "    print(f\"Topic: {topic_text}\\nContext: {topic_context}\\nCategory: {topic['Category']}\\nSentiment: {sentiment}\\n\")\n"
   ],
   "id": "9a53ca586683ed01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_file_path = s_root + 'Data/survey_results_with_topics.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(db, json_file, indent=4, ensure_ascii=False)"
   ],
   "id": "13fdcb62902d1161"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Everything put together \n",
    "### in a loop"
   ],
   "id": "ba57a0d7c403912"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import logging\n",
    "from langchain.prompts import PromptTemplate\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "# Initialize the language detector\n",
    "detector = LanguageDetectorBuilder.from_languages(\n",
    "    Language.ENGLISH, Language.SPANISH, Language.CHINESE, Language.GERMAN, Language.FRENCH\n",
    ").build()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize token counters\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "\n",
    "# Load the survey results JSON file\n",
    "input_file_path = s_root + 'Data/sample_size.json'\n",
    "output_file_path = s_root + 'Data/sample_survey_results_with_topics.json'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    db = json.load(f)\n",
    "\n",
    "# Initialize a counter for unique IDs\n",
    "id_counter = 1\n",
    "\n",
    "# Loop through each entry in the survey results\n",
    "for entry in db:\n",
    "    # Add a unique ID to the entry\n",
    "    entry[\"ID\"] = id_counter\n",
    "    logging.info(f\"Processing entry ID: {entry['ID']}\")\n",
    "\n",
    "    # Step 1: Detect language for each field and decide if translation is needed\n",
    "    reason_text = entry.get(\"Please tell us why you chose the rating above:\", \"\")\n",
    "    wish_text = entry.get(\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\", \"\")\n",
    "    \n",
    "    # Initialize detected language as unknown\n",
    "    detected_language = \"unknown\"\n",
    "    \n",
    "    # Detect language only for fields with actual text\n",
    "    if isinstance(reason_text, str) and reason_text.strip():\n",
    "        detected_language_reason = detector.detect_language_of(reason_text).name.lower()\n",
    "    else:\n",
    "        detected_language_reason = \"none\"\n",
    "\n",
    "    if isinstance(wish_text, str) and wish_text.strip():\n",
    "        detected_language_wish = detector.detect_language_of(wish_text).name.lower()\n",
    "    else:\n",
    "        detected_language_wish = \"none\"\n",
    "    \n",
    "    # Set the overall detected language based on valid fields\n",
    "    if detected_language_reason != \"none\" and detected_language_reason == detected_language_wish:\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_reason != \"none\" and detected_language_reason != detected_language_wish:\n",
    "        detected_language = \"mixed\"\n",
    "    elif detected_language_reason != \"none\":\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_wish != \"none\":\n",
    "        detected_language = detected_language_wish\n",
    "    \n",
    "    # Save the detected language in the JSON entry\n",
    "    entry[\"language\"] = detected_language\n",
    "\n",
    "    # Only proceed to translation if:\n",
    "    # - There is text in either reason_text or wish_text\n",
    "    # - The detected language is not English\n",
    "    if detected_language not in [\"english\", \"none\"]:\n",
    "        # Prepare the translation prompt with only the fields that have text\n",
    "        reason_text_for_prompt = reason_text if detected_language_reason != \"none\" else \"N/A\"\n",
    "        wish_text_for_prompt = wish_text if detected_language_wish != \"none\" else \"N/A\"\n",
    "        \n",
    "        prompt_translation = prompt_template_translation.format(\n",
    "            reason=reason_text_for_prompt, \n",
    "            wish=wish_text_for_prompt\n",
    "        )\n",
    "\n",
    "        # Make the OpenAI API call to translate the review\n",
    "        translation_response = client.chat.completions.create(\n",
    "            model=chat_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for translation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_translation},\n",
    "            ],\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        \n",
    "        # Track tokens used\n",
    "        prompt_tokens += translation_response.usage.prompt_tokens\n",
    "        completion_tokens += translation_response.usage.completion_tokens\n",
    "        logging.info(f\"Translation API call: Prompt tokens used: {translation_response.usage.prompt_tokens}, Completion tokens used: {translation_response.usage.completion_tokens}\")\n",
    "\n",
    "        # Parse and update entry with translated text, preserving original language info\n",
    "        translation_text = translation_response.choices[0].message.content\n",
    "\n",
    "        if \"REASON:\" in translation_text and detected_language_reason != \"none\":\n",
    "            reason_translation = translation_text.split(\"REASON:\")[1].split(\"WISH:\")[0].strip()\n",
    "            entry[\"Please tell us why you chose the rating above:\"] = reason_translation\n",
    "\n",
    "        if \"WISH:\" in translation_text and detected_language_wish != \"none\":\n",
    "            wish_translation = translation_text.split(\"WISH:\")[1].strip()\n",
    "            entry[\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\"] = wish_translation\n",
    "\n",
    "    # Rebuild combined_review with the updated (translated) values\n",
    "    combined_review = f\"{entry.get('Please tell us why you chose the rating above:', '')} {entry.get('If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?', '')}\"\n",
    "\n",
    "    # Step 2: Topic Extraction using the translated text\n",
    "    prompt_topic = prompt_template_topic.format(review=combined_review)\n",
    "    topic_response = client.chat.completions.create(\n",
    "        model=chat_model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for game review analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_topic},\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "          }\n",
    "    )\n",
    "\n",
    "    # Track tokens used\n",
    "    prompt_tokens += topic_response.usage.prompt_tokens\n",
    "    completion_tokens += topic_response.usage.completion_tokens\n",
    "    logging.info(f\"Topic Extraction API call: Prompt tokens used: {topic_response.usage.prompt_tokens}, Completion tokens used: {topic_response.usage.completion_tokens}\")\n",
    "    \n",
    "    # Parse topics from JSON response\n",
    "    topics = json.loads(topic_response.choices[0].message.content)  # JSON parse the response content\n",
    "\n",
    "    # Initialize the \"topics\" key in the entry if it doesn't exist\n",
    "    entry[\"topics\"] = []\n",
    "\n",
    "    # Step 3: Sentiment Analysis on each extracted topic\n",
    "    for topic in topics[\"Topics\"]:\n",
    "        topic_text = topic[\"Topic\"]\n",
    "        topic_context = topic[\"Context\"]\n",
    "        topic_category = topic[\"Category\"]  # Preserve \"fact\" or \"request\"\n",
    "\n",
    "        prompt_sentiment = prompt_template_topic_view.format(review=topic_context, topic=topic_text)\n",
    "        sentiment_response = client.chat.completions.create(\n",
    "            model=chat_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant expertised in sentiment analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_sentiment},\n",
    "            ],\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        # Track tokens used\n",
    "        prompt_tokens += sentiment_response.usage.prompt_tokens\n",
    "        completion_tokens += sentiment_response.usage.completion_tokens\n",
    "        logging.info(f\"Sentiment Analysis API call for topic '{topic_text}': Prompt tokens used: {sentiment_response.usage.prompt_tokens}, Completion tokens used: {sentiment_response.usage.completion_tokens}\")\n",
    "\n",
    "        sentiment = sentiment_response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Append the topic information with sentiment and category to the \"topics\" list\n",
    "        entry[\"topics\"].append({\n",
    "            \"topic\": topic_text,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"category\": topic_category,\n",
    "            \"sentence\": topic_context\n",
    "        })\n",
    "        \n",
    "    logging.info(f\"Completed processing entry ID: {entry['ID']}\")\n",
    "    \n",
    "    # Increment the ID counter for the next entry\n",
    "    id_counter += 1\n",
    "\n",
    "# Save the final JSON with topics and sentiments\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(db, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print or log the total tokens used\n",
    "logging.info(f\"Total prompt tokens used: {prompt_tokens}\")\n",
    "logging.info(f\"Total completion tokens used: {completion_tokens}\")\n",
    "logging.info(f\"Processed data with topics, sentiments, language info, and IDs saved to {output_file_path}\")\n"
   ],
   "id": "623a3a61886255a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import logging\n",
    "from langchain.prompts import PromptTemplate\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "# Initialize the language detector\n",
    "detector = LanguageDetectorBuilder.from_languages(\n",
    "    Language.ENGLISH, Language.SPANISH, Language.CHINESE, Language.GERMAN, Language.FRENCH\n",
    ").build()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize token counters\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "\n",
    "# Track corrupted entries\n",
    "corrupted_entries = []\n",
    "\n",
    "# Load the survey results JSON file\n",
    "input_file_path = s_root + 'Data/sample_size.json'\n",
    "output_file_path = s_root + 'Data/sample_survey_results_with_topics.json'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    db = json.load(f)\n",
    "\n",
    "# Initialize a counter for unique IDs\n",
    "id_counter = 1\n",
    "\n",
    "# Loop through each entry in the survey results\n",
    "for entry in db:\n",
    "    # Add a unique ID to the entry\n",
    "    entry[\"ID\"] = id_counter\n",
    "    logging.info(f\"Processing entry ID: {entry['ID']}\")\n",
    "\n",
    "    # Step 1: Detect language for each field and decide if translation is needed\n",
    "    reason_text = entry.get(\"Please tell us why you chose the rating above:\", \"\")\n",
    "    wish_text = entry.get(\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\", \"\")\n",
    "\n",
    "    # Detect language only for fields with actual text\n",
    "    detected_language_reason = detector.detect_language_of(reason_text).name.lower() if isinstance(reason_text, str) and reason_text.strip() else \"none\"\n",
    "    detected_language_wish = detector.detect_language_of(wish_text).name.lower() if isinstance(wish_text, str) and wish_text.strip() else \"none\"\n",
    "\n",
    "    # Determine the overall detected language\n",
    "    if detected_language_reason != \"none\" and detected_language_reason == detected_language_wish:\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_reason != \"none\" and detected_language_reason != detected_language_wish:\n",
    "        detected_language = \"mixed\"\n",
    "    elif detected_language_reason != \"none\":\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_wish != \"none\":\n",
    "        detected_language = detected_language_wish\n",
    "    else:\n",
    "        detected_language = \"unknown\"\n",
    "\n",
    "    # Save detected language in the JSON entry\n",
    "    entry[\"language\"] = detected_language\n",
    "\n",
    "    # Translation step if necessary\n",
    "    if detected_language not in [\"english\", \"none\"]:\n",
    "        try:\n",
    "            reason_text_for_prompt = reason_text if detected_language_reason != \"none\" else \"N/A\"\n",
    "            wish_text_for_prompt = wish_text if detected_language_wish != \"none\" else \"N/A\"\n",
    "            \n",
    "            prompt_translation = prompt_template_translation.format(\n",
    "                reason=reason_text_for_prompt,\n",
    "                wish=wish_text_for_prompt\n",
    "            )\n",
    "\n",
    "            # Make translation API call\n",
    "            translation_response = client.chat.completions.create(\n",
    "                model=chat_model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for translation.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_translation},\n",
    "                ],\n",
    "                max_tokens=1024\n",
    "            )\n",
    "\n",
    "            # Track tokens\n",
    "            prompt_tokens += translation_response.usage.prompt_tokens\n",
    "            completion_tokens += translation_response.usage.completion_tokens\n",
    "\n",
    "            # Parse and update entry with translated text\n",
    "            translation_text = translation_response.choices[0].message.content\n",
    "            if \"REASON:\" in translation_text and detected_language_reason != \"none\":\n",
    "                reason_translation = translation_text.split(\"REASON:\")[1].split(\"WISH:\")[0].strip()\n",
    "                entry[\"Please tell us why you chose the rating above:\"] = reason_translation\n",
    "            if \"WISH:\" in translation_text and detected_language_wish != \"none\":\n",
    "                wish_translation = translation_text.split(\"WISH:\")[1].strip()\n",
    "                entry[\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\"] = wish_translation\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error translating entry ID: {entry['ID']}: {e}\")\n",
    "            corrupted_entries.append(entry)\n",
    "            id_counter += 1\n",
    "            continue\n",
    "\n",
    "    # Rebuild combined_review with updated values\n",
    "    combined_review = f\"{entry.get('Please tell us why you chose the rating above:', '')} {entry.get('If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?', '')}\"\n",
    "\n",
    "    # Step 2: Topic Extraction\n",
    "    try:\n",
    "        prompt_topic = prompt_template_topic.format(review=combined_review)\n",
    "        topic_response = client.chat.completions.create(\n",
    "            model=chat_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for game review analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_topic},\n",
    "            ],\n",
    "            max_tokens=1024,        \n",
    "            response_format={\n",
    "                \"type\": \"json_object\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Track tokens used\n",
    "        prompt_tokens += topic_response.usage.prompt_tokens\n",
    "        completion_tokens += topic_response.usage.completion_tokens\n",
    "\n",
    "        # Ensure JSON response parsing works correctly\n",
    "        try:\n",
    "            topics = json.loads(topic_response.choices[0].message.content)\n",
    "        except json.JSONDecodeError as json_err:\n",
    "            logging.error(f\"JSON parsing error for entry ID {entry['ID']}: {json_err}\")\n",
    "            corrupted_entries.append(entry)\n",
    "            id_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Initialize the \"topics\" key\n",
    "        entry[\"topics\"] = []\n",
    "\n",
    "        # Step 3: Sentiment Analysis for each topic\n",
    "        for topic in topics[\"Topics\"]:\n",
    "            topic_text = topic[\"Topic\"]\n",
    "            topic_context = topic[\"Context\"]\n",
    "            topic_category = topic[\"Category\"]\n",
    "\n",
    "            prompt_sentiment = prompt_template_topic_view.format(review=topic_context, topic=topic_text)\n",
    "            try:\n",
    "                sentiment_response = client.chat.completions.create(\n",
    "                    model=chat_model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant expertised in sentiment analysis.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt_sentiment},\n",
    "                    ],\n",
    "                    max_tokens=1024\n",
    "                )\n",
    "\n",
    "                # Track tokens used\n",
    "                prompt_tokens += sentiment_response.usage.prompt_tokens\n",
    "                completion_tokens += sentiment_response.usage.completion_tokens\n",
    "\n",
    "                # Extract and store sentiment result\n",
    "                sentiment = sentiment_response.choices[0].message.content.strip()\n",
    "                entry[\"topics\"].append({\n",
    "                    \"topic\": topic_text,\n",
    "                    \"sentiment\": sentiment,\n",
    "                    \"category\": topic_category,\n",
    "                    \"sentence\": topic_context\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing sentiment analysis for entry ID {entry['ID']} topic '{topic_text}': {e}\")\n",
    "                corrupted_entries.append(entry)\n",
    "                break  # Continue to next entry if sentiment analysis fails for any topic\n",
    "\n",
    "        logging.info(f\"Completed processing entry ID: {entry['ID']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting topics for entry ID: {entry['ID']}: {e}\")\n",
    "        corrupted_entries.append(entry)\n",
    "        id_counter += 1\n",
    "        continue  # Move to next entry if topic extraction fails\n",
    "\n",
    "    # Increment ID counter for the next entry\n",
    "    id_counter += 1\n",
    "\n",
    "# Save final JSON with topics and sentiments\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(db, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save corrupted entries separately\n",
    "with open(s_root + 'Data/corrupted_entries.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(corrupted_entries, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Log the total tokens used\n",
    "logging.info(f\"Total prompt tokens used: {prompt_tokens}\")\n",
    "logging.info(f\"Total completion tokens used: {completion_tokens}\")\n",
    "logging.info(f\"Processed data with topics, sentiments, language info, and IDs saved to {output_file_path}\")\n",
    "logging.info(f\"Corrupted entries saved to {s_root + 'Data/corrupted_entries.json'}\")\n"
   ],
   "id": "7c545daed73d8fbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import logging\n",
    "from langchain.prompts import PromptTemplate\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "\n",
    "# Initialize the language detector\n",
    "detector = LanguageDetectorBuilder.from_languages(\n",
    "    Language.ENGLISH, Language.SPANISH, Language.CHINESE, Language.GERMAN, Language.FRENCH\n",
    ").build()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize token counters\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "\n",
    "# Track corrupted entries\n",
    "corrupted_entries = []\n",
    "\n",
    "# Load the survey results JSON file\n",
    "input_file_path = s_root + 'Data/sample_size.json'\n",
    "output_file_path = s_root + 'Data/sample_survey_results_with_topics.json'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    db = json.load(f)\n",
    "\n",
    "# Initialize a counter for unique IDs\n",
    "id_counter = 1\n",
    "\n",
    "# Loop through each entry in the survey results\n",
    "for entry in db:\n",
    "    # Add a unique ID to the entry\n",
    "    entry[\"ID\"] = id_counter\n",
    "    logging.info(f\"Processing entry ID: {entry['ID']}\")\n",
    "\n",
    "    # Step 1: Detect language for each field and decide if translation is needed\n",
    "    reason_text = entry.get(\"Please tell us why you chose the rating above:\", \"\")\n",
    "    wish_text = entry.get(\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\", \"\")\n",
    "\n",
    "    # Detect language only for fields with actual text and skip if either field is None or empty\n",
    "    if isinstance(reason_text, str) and reason_text.strip():\n",
    "        try:\n",
    "            detected_language_reason = detector.detect_language_of(reason_text)\n",
    "            detected_language_reason = detected_language_reason.name.lower() if detected_language_reason else \"none\"\n",
    "        except AttributeError:\n",
    "            detected_language_reason = \"none\"\n",
    "    else:\n",
    "        detected_language_reason = \"none\"\n",
    "\n",
    "    if isinstance(wish_text, str) and wish_text.strip():\n",
    "        try:\n",
    "            detected_language_wish = detector.detect_language_of(wish_text)\n",
    "            detected_language_wish = detected_language_wish.name.lower() if detected_language_wish else \"none\"\n",
    "        except AttributeError:\n",
    "            detected_language_wish = \"none\"\n",
    "    else:\n",
    "        detected_language_wish = \"none\"\n",
    "\n",
    "    # Set the overall detected language based on valid fields\n",
    "    if detected_language_reason != \"none\" and detected_language_reason == detected_language_wish:\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_reason != \"none\" and detected_language_reason != detected_language_wish:\n",
    "        detected_language = \"mixed\"\n",
    "    elif detected_language_reason != \"none\":\n",
    "        detected_language = detected_language_reason\n",
    "    elif detected_language_wish != \"none\":\n",
    "        detected_language = detected_language_wish\n",
    "    else:\n",
    "        detected_language = \"unknown\"\n",
    "\n",
    "    # Save detected language in the JSON entry\n",
    "    entry[\"language\"] = detected_language\n",
    "\n",
    "    # Translation step if necessary\n",
    "    if detected_language not in [\"english\", \"none\"]:\n",
    "        logging.info(f\"Entry ID {entry['ID']}: Translation required (Language: {detected_language})\")\n",
    "        try:\n",
    "            reason_text_for_prompt = reason_text if detected_language_reason != \"none\" else \"N/A\"\n",
    "            wish_text_for_prompt = wish_text if detected_language_wish != \"none\" else \"N/A\"\n",
    "            \n",
    "            prompt_translation = prompt_template_translation.format(\n",
    "                reason=reason_text_for_prompt,\n",
    "                wish=wish_text_for_prompt\n",
    "            )\n",
    "\n",
    "            # Make translation API call\n",
    "            translation_response = client.chat.completions.create(\n",
    "                model=chat_model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for translation.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_translation},\n",
    "                ],\n",
    "                max_tokens=1024\n",
    "            )\n",
    "\n",
    "            # Track tokens\n",
    "            prompt_tokens += translation_response.usage.prompt_tokens\n",
    "            completion_tokens += translation_response.usage.completion_tokens\n",
    "\n",
    "            # Parse and update entry with translated text\n",
    "            translation_text = translation_response.choices[0].message.content\n",
    "            if \"REASON:\" in translation_text and detected_language_reason != \"none\":\n",
    "                reason_translation = translation_text.split(\"REASON:\")[1].split(\"WISH:\")[0].strip()\n",
    "                entry[\"Please tell us why you chose the rating above:\"] = reason_translation\n",
    "            if \"WISH:\" in translation_text and detected_language_wish != \"none\":\n",
    "                wish_translation = translation_text.split(\"WISH:\")[1].strip()\n",
    "                entry[\"If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?\"] = wish_translation\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error translating entry ID: {entry['ID']}: {e}\")\n",
    "            corrupted_entries.append(entry)\n",
    "            id_counter += 1\n",
    "            continue\n",
    "    else:\n",
    "        logging.info(f\"Entry ID {entry['ID']}: No translation needed (Language: {detected_language})\")\n",
    "\n",
    "    # Rebuild combined_review with updated values\n",
    "    combined_review = f\"{entry.get('Please tell us why you chose the rating above:', '')} {entry.get('If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?', '')}\"\n",
    "\n",
    "    # Step 2: Topic Extraction\n",
    "    logging.info(f\"Entry ID {entry['ID']}: Starting topic extraction\")\n",
    "    try:\n",
    "        prompt_topic = prompt_template_topic.format(review=combined_review)\n",
    "        topic_response = client.chat.completions.create(\n",
    "            model=chat_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for game review analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_topic},\n",
    "            ],\n",
    "            max_tokens=1024,        \n",
    "            response_format={\n",
    "                \"type\": \"json_object\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Track tokens used\n",
    "        prompt_tokens += topic_response.usage.prompt_tokens\n",
    "        completion_tokens += topic_response.usage.completion_tokens\n",
    "\n",
    "        # Ensure JSON response parsing works correctly\n",
    "        try:\n",
    "            topics = json.loads(topic_response.choices[0].message.content)\n",
    "        except json.JSONDecodeError as json_err:\n",
    "            logging.error(f\"JSON parsing error for entry ID {entry['ID']}: {json_err}\")\n",
    "            corrupted_entries.append(entry)\n",
    "            id_counter += 1\n",
    "            continue\n",
    "\n",
    "        # Initialize the \"topics\" key\n",
    "        entry[\"topics\"] = []\n",
    "\n",
    "        # Step 3: Sentiment Analysis for each topic\n",
    "        for topic in topics[\"Topics\"]:\n",
    "            logging.info(f\"Entry ID {entry['ID']}: Starting sentiment analysis for topic '{topic['Topic']}'\")\n",
    "            topic_text = topic[\"Topic\"]\n",
    "            topic_context = topic[\"Context\"]\n",
    "            topic_category = topic[\"Category\"]\n",
    "\n",
    "            prompt_sentiment = prompt_template_topic_view.format(review=topic_context, topic=topic_text)\n",
    "            try:\n",
    "                sentiment_response = client.chat.completions.create(\n",
    "                    model=chat_model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant expertised in sentiment analysis.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt_sentiment},\n",
    "                    ],\n",
    "                    max_tokens=1024\n",
    "                )\n",
    "\n",
    "                # Track tokens used\n",
    "                prompt_tokens += sentiment_response.usage.prompt_tokens\n",
    "                completion_tokens += sentiment_response.usage.completion_tokens\n",
    "\n",
    "                # Extract and store sentiment result\n",
    "                sentiment = sentiment_response.choices[0].message.content.strip()\n",
    "                entry[\"topics\"].append({\n",
    "                    \"topic\": topic_text,\n",
    "                    \"sentiment\": sentiment,\n",
    "                    \"category\": topic_category,\n",
    "                    \"sentence\": topic_context\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing sentiment analysis for entry ID {entry['ID']} topic '{topic_text}': {e}\")\n",
    "                corrupted_entries.append(entry)\n",
    "                break  # Continue to next entry if sentiment analysis fails for any topic\n",
    "\n",
    "        logging.info(f\"Completed topic and sentiment analysis for entry ID: {entry['ID']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting topics for entry ID: {entry['ID']}: {e}\")\n",
    "        corrupted_entries.append(entry)\n",
    "        id_counter += 1\n",
    "        continue  # Move to next entry if topic extraction fails\n",
    "\n",
    "    # Increment ID counter for the next entry\n",
    "    id_counter += 1\n",
    "\n",
    "# Save final JSON with topics and sentiments\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(db, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save corrupted entries separately\n",
    "with open(s_root + 'Data/corrupted_entries.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(corrupted_entries, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Log the total tokens used\n",
    "logging.info(f\"Total prompt tokens used: {prompt_tokens}\")\n",
    "logging.info(f\"Total completion tokens used: {completion_tokens}\")\n",
    "logging.info(f\"Processed data with topics, sentiments, language info, and IDs saved to {output_file_path}\")\n",
    "logging.info(f\"Corrupted entries saved to {s_root + 'Data/corrupted_entries.json'}\")\n"
   ],
   "id": "f14149a5582fb526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    topics = json.loads(topic_response.choices[0].message.content)\n",
    "except json.JSONDecodeError as json_err:\n",
    "    logging.error(f\"JSON parsing error for entry ID {entry['ID']}: {json_err}\")\n",
    "    corrupted_entries.append(entry)\n",
    "    id_counter += 1\n"
   ],
   "id": "69882820684c720c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "topic_response.choices[0].message.content",
   "id": "efd1f13953b75600"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cost Calculation",
   "id": "79ada8e387398bd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Usage and pricing for GPT4o-mini\n",
    "total_prompt_cost = (prompt_tokens / 1_000_000) * 0.15\n",
    "total_completion_cost = (completion_tokens / 1_000_000) * 0.6\n",
    "\n",
    "\n",
    "print(f\"Total prompt tokens used: {prompt_tokens}\")\n",
    "print(f\"Total completion tokens used: {completion_tokens}\")\n",
    "print(f\"Total prompt token cost: ${total_prompt_cost:.4f}\")\n",
    "print(f\"Total completion token cost: ${total_completion_cost:.4f}\")\n",
    "\n"
   ],
   "id": "e7cda2114d7ded00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read in the JSON file with survey results\n",
    "with open(s_root + 'Data/sample_survey_results_with_topics.json', 'r', encoding='utf-8') as f:\n",
    "    db = json.load(f)\n",
    "    "
   ],
   "id": "c253a83e63b7a488"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embed the reviews\n",
    "### Only the topics of the reviews"
   ],
   "id": "33ebf13438dce7a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ],
   "id": "a0e79e912a6702d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embed_MiniLM = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "embed_MPNET = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "s_db_json = 'Data/survey_results_with_topics.json'\n",
    "s_db_json_sample = 'Data/sample_survey_results_with_topics.json'"
   ],
   "id": "731e2f7cec481241"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T22:04:25.920333Z",
     "start_time": "2024-11-13T22:04:25.908316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def index_embedding(text, model_name=embed_MiniLM):\n",
    "    text = text.encode(encoding='ASCII', errors='ignore').decode()\n",
    "    embed_model = LangchainEmbedding(\n",
    "        HuggingFaceEmbeddings(model_name=model_name)\n",
    "    )\n",
    "    vector = embed_model.get_text_embedding(text)\n",
    "    return vector"
   ],
   "id": "9ac1053284eef493",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T22:05:21.545949Z",
     "start_time": "2024-11-13T22:05:18.632095Z"
    }
   },
   "cell_type": "code",
   "source": "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=embed_MiniLM))",
   "id": "b3731c77b4174e63",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 11:05:18,633 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-14 11:05:18,633 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T22:18:28.944614Z",
     "start_time": "2024-11-13T22:07:21.300399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "batch_size = 10  # Adjust based on available memory and dataset size\n",
    "b_embedding = True\n",
    "\n",
    "if b_embedding:\n",
    "    with open(s_root + s_db_json_sample, 'r', encoding='utf-8') as f:\n",
    "        d_review_output = json.load(f)\n",
    "        print('Loaded JSON data')\n",
    "\n",
    "    for batch_start in range(0, len(d_review_output), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(d_review_output))\n",
    "        batch = d_review_output[batch_start:batch_end]\n",
    "        \n",
    "        for i, review_entry in enumerate(batch):\n",
    "            print(f\"Processing Review {i + batch_start + 1}\")\n",
    "            \n",
    "            if isinstance(review_entry, dict) and 'topics' in review_entry and isinstance(review_entry['topics'], list):\n",
    "                d_topics = review_entry['topics']\n",
    "                \n",
    "                for d_topic in d_topics:\n",
    "                    if isinstance(d_topic, dict):\n",
    "                        if 'embedding' not in d_topic or b_override:\n",
    "                            if 'topic' in d_topic:\n",
    "                                d_topic['embedding'] = embed_model.get_text_embedding(d_topic['topic'])\n",
    "                                \n",
    "                                # Release memory\n",
    "                                torch.cuda.empty_cache()\n",
    "                                gc.collect()\n",
    "                            else:\n",
    "                                d_topic['embedding'] = 0\n",
    "                print('.', end='')\n",
    "        print(f\"\\nBatch {batch_start // batch_size + 1} processed.\")\n",
    "\n",
    "    # Save updated JSON with embeddings\n",
    "    with open(s_root + s_db_embed_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(d_review_output, f)\n",
    "    print(\"Embeddings saved.\")\n"
   ],
   "id": "2c9371e624b6f3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded JSON data\n",
      "Processing Review 1\n",
      ".Processing Review 2\n",
      ".Processing Review 3\n",
      ".Processing Review 4\n",
      ".Processing Review 5\n",
      ".Processing Review 6\n",
      ".Processing Review 7\n",
      ".Processing Review 8\n",
      ".Processing Review 9\n",
      ".Processing Review 10\n",
      ".\n",
      "Batch 1 processed.\n",
      "Processing Review 11\n",
      ".Processing Review 12\n",
      ".Processing Review 13\n",
      ".Processing Review 14\n",
      ".Processing Review 15\n",
      ".Processing Review 16\n",
      ".Processing Review 17\n",
      ".Processing Review 18\n",
      ".Processing Review 19\n",
      ".Processing Review 20\n",
      ".\n",
      "Batch 2 processed.\n",
      "Processing Review 21\n",
      ".Processing Review 22\n",
      ".Processing Review 23\n",
      ".Processing Review 24\n",
      ".Processing Review 25\n",
      ".Processing Review 26\n",
      ".Processing Review 27\n",
      ".Processing Review 28\n",
      ".Processing Review 29\n",
      ".Processing Review 30\n",
      ".\n",
      "Batch 3 processed.\n",
      "Processing Review 31\n",
      ".Processing Review 32\n",
      ".Processing Review 33\n",
      ".Processing Review 34\n",
      ".Processing Review 35\n",
      ".Processing Review 36\n",
      ".Processing Review 37\n",
      ".Processing Review 38\n",
      ".Processing Review 39\n",
      ".Processing Review 40\n",
      ".\n",
      "Batch 4 processed.\n",
      "Processing Review 41\n",
      ".Processing Review 42\n",
      ".Processing Review 43\n",
      ".Processing Review 44\n",
      ".Processing Review 45\n",
      ".Processing Review 46\n",
      ".Processing Review 47\n",
      ".Processing Review 48\n",
      ".Processing Review 49\n",
      ".Processing Review 50\n",
      ".\n",
      "Batch 5 processed.\n",
      "Processing Review 51\n",
      ".Processing Review 52\n",
      ".Processing Review 53\n",
      ".Processing Review 54\n",
      ".Processing Review 55\n",
      ".Processing Review 56\n",
      ".Processing Review 57\n",
      ".Processing Review 58\n",
      ".Processing Review 59\n",
      ".Processing Review 60\n",
      ".\n",
      "Batch 6 processed.\n",
      "Processing Review 61\n",
      ".Processing Review 62\n",
      ".Processing Review 63\n",
      ".Processing Review 64\n",
      ".Processing Review 65\n",
      ".Processing Review 66\n",
      ".Processing Review 67\n",
      ".Processing Review 68\n",
      ".Processing Review 69\n",
      ".Processing Review 70\n",
      ".\n",
      "Batch 7 processed.\n",
      "Processing Review 71\n",
      ".Processing Review 72\n",
      ".Processing Review 73\n",
      ".Processing Review 74\n",
      ".Processing Review 75\n",
      ".Processing Review 76\n",
      ".Processing Review 77\n",
      ".Processing Review 78\n",
      ".Processing Review 79\n",
      ".Processing Review 80\n",
      ".\n",
      "Batch 8 processed.\n",
      "Processing Review 81\n",
      ".Processing Review 82\n",
      ".Processing Review 83\n",
      ".Processing Review 84\n",
      ".Processing Review 85\n",
      ".Processing Review 86\n",
      ".Processing Review 87\n",
      ".Processing Review 88\n",
      ".Processing Review 89\n",
      ".Processing Review 90\n",
      ".\n",
      "Batch 9 processed.\n",
      "Processing Review 91\n",
      ".Processing Review 92\n",
      ".Processing Review 93\n",
      ".Processing Review 94\n",
      ".Processing Review 95\n",
      ".Processing Review 96\n",
      ".Processing Review 97\n",
      ".Processing Review 98\n",
      ".Processing Review 99\n",
      ".Processing Review 100\n",
      ".\n",
      "Batch 10 processed.\n",
      "Processing Review 101\n",
      ".Processing Review 102\n",
      ".Processing Review 103\n",
      ".Processing Review 104\n",
      ".Processing Review 105\n",
      ".Processing Review 106\n",
      ".Processing Review 107\n",
      ".Processing Review 108\n",
      ".Processing Review 109\n",
      ".Processing Review 110\n",
      ".\n",
      "Batch 11 processed.\n",
      "Processing Review 111\n",
      ".Processing Review 112\n",
      ".Processing Review 113\n",
      ".Processing Review 114\n",
      ".Processing Review 115\n",
      ".Processing Review 116\n",
      ".Processing Review 117\n",
      ".Processing Review 118\n",
      ".Processing Review 119\n",
      ".Processing Review 120\n",
      ".\n",
      "Batch 12 processed.\n",
      "Processing Review 121\n",
      ".Processing Review 122\n",
      ".Processing Review 123\n",
      ".Processing Review 124\n",
      ".Processing Review 125\n",
      ".Processing Review 126\n",
      ".Processing Review 127\n",
      ".Processing Review 128\n",
      ".Processing Review 129\n",
      ".Processing Review 130\n",
      ".\n",
      "Batch 13 processed.\n",
      "Processing Review 131\n",
      ".Processing Review 132\n",
      ".Processing Review 133\n",
      ".Processing Review 134\n",
      ".Processing Review 135\n",
      ".Processing Review 136\n",
      ".Processing Review 137\n",
      ".Processing Review 138\n",
      ".Processing Review 139\n",
      ".Processing Review 140\n",
      ".\n",
      "Batch 14 processed.\n",
      "Processing Review 141\n",
      ".Processing Review 142\n",
      ".Processing Review 143\n",
      ".Processing Review 144\n",
      ".Processing Review 145\n",
      ".Processing Review 146\n",
      ".Processing Review 147\n",
      ".Processing Review 148\n",
      ".Processing Review 149\n",
      ".Processing Review 150\n",
      ".\n",
      "Batch 15 processed.\n",
      "Processing Review 151\n",
      ".Processing Review 152\n",
      ".Processing Review 153\n",
      ".Processing Review 154\n",
      ".Processing Review 155\n",
      ".Processing Review 156\n",
      ".Processing Review 157\n",
      ".Processing Review 158\n",
      ".Processing Review 159\n",
      ".Processing Review 160\n",
      ".\n",
      "Batch 16 processed.\n",
      "Processing Review 161\n",
      ".Processing Review 162\n",
      ".Processing Review 163\n",
      ".Processing Review 164\n",
      ".Processing Review 165\n",
      ".Processing Review 166\n",
      ".Processing Review 167\n",
      ".Processing Review 168\n",
      ".Processing Review 169\n",
      ".Processing Review 170\n",
      ".\n",
      "Batch 17 processed.\n",
      "Processing Review 171\n",
      ".Processing Review 172\n",
      ".Processing Review 173\n",
      ".Processing Review 174\n",
      ".Processing Review 175\n",
      ".Processing Review 176\n",
      ".Processing Review 177\n",
      ".Processing Review 178\n",
      ".Processing Review 179\n",
      ".Processing Review 180\n",
      ".\n",
      "Batch 18 processed.\n",
      "Processing Review 181\n",
      ".Processing Review 182\n",
      ".Processing Review 183\n",
      ".Processing Review 184\n",
      ".Processing Review 185\n",
      ".Processing Review 186\n",
      ".Processing Review 187\n",
      ".Processing Review 188\n",
      ".Processing Review 189\n",
      ".Processing Review 190\n",
      ".\n",
      "Batch 19 processed.\n",
      "Processing Review 191\n",
      ".Processing Review 192\n",
      ".Processing Review 193\n",
      ".Processing Review 194\n",
      ".Processing Review 195\n",
      ".Processing Review 196\n",
      ".Processing Review 197\n",
      ".Processing Review 198\n",
      ".Processing Review 199\n",
      ".Processing Review 200\n",
      ".\n",
      "Batch 20 processed.\n",
      "Processing Review 201\n",
      ".Processing Review 202\n",
      ".Processing Review 203\n",
      ".Processing Review 204\n",
      ".Processing Review 205\n",
      ".Processing Review 206\n",
      ".Processing Review 207\n",
      ".Processing Review 208\n",
      ".Processing Review 209\n",
      ".Processing Review 210\n",
      ".\n",
      "Batch 21 processed.\n",
      "Processing Review 211\n",
      ".Processing Review 212\n",
      ".Processing Review 213\n",
      ".Processing Review 214\n",
      ".Processing Review 215\n",
      ".Processing Review 216\n",
      ".Processing Review 217\n",
      ".Processing Review 218\n",
      ".Processing Review 219\n",
      ".Processing Review 220\n",
      ".\n",
      "Batch 22 processed.\n",
      "Processing Review 221\n",
      ".Processing Review 222\n",
      ".Processing Review 223\n",
      ".Processing Review 224\n",
      ".Processing Review 225\n",
      ".Processing Review 226\n",
      ".Processing Review 227\n",
      ".Processing Review 228\n",
      ".Processing Review 229\n",
      ".Processing Review 230\n",
      ".\n",
      "Batch 23 processed.\n",
      "Processing Review 231\n",
      ".Processing Review 232\n",
      ".Processing Review 233\n",
      ".Processing Review 234\n",
      ".Processing Review 235\n",
      ".Processing Review 236\n",
      ".Processing Review 237\n",
      ".Processing Review 238\n",
      ".Processing Review 239\n",
      ".Processing Review 240\n",
      ".\n",
      "Batch 24 processed.\n",
      "Processing Review 241\n",
      ".Processing Review 242\n",
      ".Processing Review 243\n",
      ".Processing Review 244\n",
      ".Processing Review 245\n",
      ".Processing Review 246\n",
      ".Processing Review 247\n",
      ".Processing Review 248\n",
      ".Processing Review 249\n",
      ".Processing Review 250\n",
      ".\n",
      "Batch 25 processed.\n",
      "Processing Review 251\n",
      ".Processing Review 252\n",
      ".Processing Review 253\n",
      ".Processing Review 254\n",
      ".Processing Review 255\n",
      ".Processing Review 256\n",
      ".Processing Review 257\n",
      ".Processing Review 258\n",
      ".Processing Review 259\n",
      ".Processing Review 260\n",
      ".\n",
      "Batch 26 processed.\n",
      "Processing Review 261\n",
      ".Processing Review 262\n",
      ".Processing Review 263\n",
      ".Processing Review 264\n",
      ".Processing Review 265\n",
      ".Processing Review 266\n",
      ".Processing Review 267\n",
      ".Processing Review 268\n",
      ".Processing Review 269\n",
      ".Processing Review 270\n",
      ".\n",
      "Batch 27 processed.\n",
      "Processing Review 271\n",
      ".Processing Review 272\n",
      ".Processing Review 273\n",
      ".Processing Review 274\n",
      ".Processing Review 275\n",
      ".Processing Review 276\n",
      ".Processing Review 277\n",
      ".Processing Review 278\n",
      ".Processing Review 279\n",
      ".Processing Review 280\n",
      ".\n",
      "Batch 28 processed.\n",
      "Processing Review 281\n",
      ".Processing Review 282\n",
      ".Processing Review 283\n",
      ".Processing Review 284\n",
      ".Processing Review 285\n",
      ".Processing Review 286\n",
      ".Processing Review 287\n",
      ".Processing Review 288\n",
      ".Processing Review 289\n",
      ".Processing Review 290\n",
      ".\n",
      "Batch 29 processed.\n",
      "Processing Review 291\n",
      ".Processing Review 292\n",
      ".Processing Review 293\n",
      ".Processing Review 294\n",
      ".Processing Review 295\n",
      ".Processing Review 296\n",
      ".Processing Review 297\n",
      ".Processing Review 298\n",
      ".Processing Review 299\n",
      ".Processing Review 300\n",
      ".\n",
      "Batch 30 processed.\n",
      "Processing Review 301\n",
      ".Processing Review 302\n",
      ".Processing Review 303\n",
      ".Processing Review 304\n",
      ".Processing Review 305\n",
      ".Processing Review 306\n",
      ".Processing Review 307\n",
      ".Processing Review 308\n",
      ".Processing Review 309\n",
      ".Processing Review 310\n",
      ".\n",
      "Batch 31 processed.\n",
      "Processing Review 311\n",
      ".Processing Review 312\n",
      ".Processing Review 313\n",
      ".Processing Review 314\n",
      ".Processing Review 315\n",
      ".Processing Review 316\n",
      ".Processing Review 317\n",
      ".Processing Review 318\n",
      ".Processing Review 319\n",
      ".Processing Review 320\n",
      ".\n",
      "Batch 32 processed.\n",
      "Processing Review 321\n",
      ".Processing Review 322\n",
      ".Processing Review 323\n",
      ".Processing Review 324\n",
      ".Processing Review 325\n",
      ".Processing Review 326\n",
      ".Processing Review 327\n",
      ".Processing Review 328\n",
      ".Processing Review 329\n",
      ".Processing Review 330\n",
      ".\n",
      "Batch 33 processed.\n",
      "Processing Review 331\n",
      ".Processing Review 332\n",
      ".Processing Review 333\n",
      ".Processing Review 334\n",
      ".Processing Review 335\n",
      ".Processing Review 336\n",
      ".Processing Review 337\n",
      ".Processing Review 338\n",
      ".Processing Review 339\n",
      ".Processing Review 340\n",
      ".\n",
      "Batch 34 processed.\n",
      "Processing Review 341\n",
      ".Processing Review 342\n",
      ".Processing Review 343\n",
      ".Processing Review 344\n",
      ".Processing Review 345\n",
      ".Processing Review 346\n",
      ".Processing Review 347\n",
      ".Processing Review 348\n",
      ".Processing Review 349\n",
      ".Processing Review 350\n",
      ".\n",
      "Batch 35 processed.\n",
      "Processing Review 351\n",
      ".Processing Review 352\n",
      ".Processing Review 353\n",
      ".Processing Review 354\n",
      ".Processing Review 355\n",
      ".Processing Review 356\n",
      ".Processing Review 357\n",
      ".Processing Review 358\n",
      ".Processing Review 359\n",
      ".Processing Review 360\n",
      ".\n",
      "Batch 36 processed.\n",
      "Processing Review 361\n",
      ".Processing Review 362\n",
      ".Processing Review 363\n",
      ".Processing Review 364\n",
      ".Processing Review 365\n",
      ".Processing Review 366\n",
      ".Processing Review 367\n",
      ".Processing Review 368\n",
      ".Processing Review 369\n",
      ".Processing Review 370\n",
      ".\n",
      "Batch 37 processed.\n",
      "Processing Review 371\n",
      ".Processing Review 372\n",
      ".Processing Review 373\n",
      ".Processing Review 374\n",
      ".Processing Review 375\n",
      ".Processing Review 376\n",
      ".Processing Review 377\n",
      ".Processing Review 378\n",
      ".Processing Review 379\n",
      ".Processing Review 380\n",
      ".\n",
      "Batch 38 processed.\n",
      "Processing Review 381\n",
      ".Processing Review 382\n",
      ".Processing Review 383\n",
      ".Processing Review 384\n",
      ".Processing Review 385\n",
      ".Processing Review 386\n",
      ".Processing Review 387\n",
      ".Processing Review 388\n",
      ".Processing Review 389\n",
      ".Processing Review 390\n",
      ".\n",
      "Batch 39 processed.\n",
      "Processing Review 391\n",
      ".Processing Review 392\n",
      ".Processing Review 393\n",
      ".Processing Review 394\n",
      ".Processing Review 395\n",
      ".Processing Review 396\n",
      ".Processing Review 397\n",
      ".Processing Review 398\n",
      ".Processing Review 399\n",
      ".Processing Review 400\n",
      ".\n",
      "Batch 40 processed.\n",
      "Processing Review 401\n",
      ".Processing Review 402\n",
      ".Processing Review 403\n",
      ".Processing Review 404\n",
      ".Processing Review 405\n",
      ".Processing Review 406\n",
      ".Processing Review 407\n",
      ".Processing Review 408\n",
      ".Processing Review 409\n",
      ".Processing Review 410\n",
      ".\n",
      "Batch 41 processed.\n",
      "Processing Review 411\n",
      ".Processing Review 412\n",
      ".Processing Review 413\n",
      ".Processing Review 414\n",
      ".Processing Review 415\n",
      ".Processing Review 416\n",
      ".Processing Review 417\n",
      ".Processing Review 418\n",
      ".Processing Review 419\n",
      ".Processing Review 420\n",
      ".\n",
      "Batch 42 processed.\n",
      "Processing Review 421\n",
      ".Processing Review 422\n",
      ".Processing Review 423\n",
      ".Processing Review 424\n",
      ".Processing Review 425\n",
      ".Processing Review 426\n",
      ".Processing Review 427\n",
      ".Processing Review 428\n",
      ".Processing Review 429\n",
      ".Processing Review 430\n",
      ".\n",
      "Batch 43 processed.\n",
      "Processing Review 431\n",
      ".Processing Review 432\n",
      ".Processing Review 433\n",
      ".Processing Review 434\n",
      ".Processing Review 435\n",
      ".Processing Review 436\n",
      ".Processing Review 437\n",
      ".Processing Review 438\n",
      ".Processing Review 439\n",
      ".Processing Review 440\n",
      ".\n",
      "Batch 44 processed.\n",
      "Processing Review 441\n",
      ".Processing Review 442\n",
      ".Processing Review 443\n",
      ".Processing Review 444\n",
      ".Processing Review 445\n",
      ".Processing Review 446\n",
      ".Processing Review 447\n",
      ".Processing Review 448\n",
      ".Processing Review 449\n",
      ".Processing Review 450\n",
      ".\n",
      "Batch 45 processed.\n",
      "Processing Review 451\n",
      ".Processing Review 452\n",
      ".Processing Review 453\n",
      ".Processing Review 454\n",
      ".Processing Review 455\n",
      ".Processing Review 456\n",
      ".Processing Review 457\n",
      ".Processing Review 458\n",
      ".Processing Review 459\n",
      ".Processing Review 460\n",
      ".\n",
      "Batch 46 processed.\n",
      "Processing Review 461\n",
      ".Processing Review 462\n",
      ".Processing Review 463\n",
      ".Processing Review 464\n",
      ".Processing Review 465\n",
      ".Processing Review 466\n",
      ".Processing Review 467\n",
      ".Processing Review 468\n",
      ".Processing Review 469\n",
      ".Processing Review 470\n",
      ".\n",
      "Batch 47 processed.\n",
      "Processing Review 471\n",
      ".Processing Review 472\n",
      ".Processing Review 473\n",
      ".Processing Review 474\n",
      ".Processing Review 475\n",
      ".Processing Review 476\n",
      ".Processing Review 477\n",
      ".Processing Review 478\n",
      ".Processing Review 479\n",
      ".Processing Review 480\n",
      ".\n",
      "Batch 48 processed.\n",
      "Processing Review 481\n",
      ".Processing Review 482\n",
      ".Processing Review 483\n",
      ".Processing Review 484\n",
      ".Processing Review 485\n",
      ".Processing Review 486\n",
      ".Processing Review 487\n",
      ".Processing Review 488\n",
      ".Processing Review 489\n",
      ".Processing Review 490\n",
      ".\n",
      "Batch 49 processed.\n",
      "Processing Review 491\n",
      ".Processing Review 492\n",
      ".Processing Review 493\n",
      ".Processing Review 494\n",
      ".Processing Review 495\n",
      ".Processing Review 496\n",
      ".Processing Review 497\n",
      ".Processing Review 498\n",
      ".Processing Review 499\n",
      ".Processing Review 500\n",
      ".\n",
      "Batch 50 processed.\n",
      "Processing Review 501\n",
      ".Processing Review 502\n",
      ".Processing Review 503\n",
      ".Processing Review 504\n",
      ".Processing Review 505\n",
      ".Processing Review 506\n",
      ".Processing Review 507\n",
      ".Processing Review 508\n",
      ".Processing Review 509\n",
      ".Processing Review 510\n",
      ".\n",
      "Batch 51 processed.\n",
      "Processing Review 511\n",
      ".Processing Review 512\n",
      ".Processing Review 513\n",
      ".Processing Review 514\n",
      ".Processing Review 515\n",
      ".Processing Review 516\n",
      ".Processing Review 517\n",
      ".Processing Review 518\n",
      ".Processing Review 519\n",
      ".Processing Review 520\n",
      ".\n",
      "Batch 52 processed.\n",
      "Processing Review 521\n",
      ".Processing Review 522\n",
      ".Processing Review 523\n",
      ".Processing Review 524\n",
      ".Processing Review 525\n",
      ".Processing Review 526\n",
      ".Processing Review 527\n",
      ".Processing Review 528\n",
      ".Processing Review 529\n",
      ".Processing Review 530\n",
      ".\n",
      "Batch 53 processed.\n",
      "Processing Review 531\n",
      ".Processing Review 532\n",
      ".Processing Review 533\n",
      ".Processing Review 534\n",
      ".Processing Review 535\n",
      ".Processing Review 536\n",
      ".Processing Review 537\n",
      ".Processing Review 538\n",
      ".Processing Review 539\n",
      ".Processing Review 540\n",
      ".\n",
      "Batch 54 processed.\n",
      "Processing Review 541\n",
      ".Processing Review 542\n",
      ".Processing Review 543\n",
      ".Processing Review 544\n",
      ".Processing Review 545\n",
      ".Processing Review 546\n",
      ".Processing Review 547\n",
      ".Processing Review 548\n",
      ".Processing Review 549\n",
      ".Processing Review 550\n",
      ".\n",
      "Batch 55 processed.\n",
      "Processing Review 551\n",
      ".Processing Review 552\n",
      ".Processing Review 553\n",
      ".Processing Review 554\n",
      ".Processing Review 555\n",
      ".Processing Review 556\n",
      ".Processing Review 557\n",
      ".Processing Review 558\n",
      ".Processing Review 559\n",
      ".Processing Review 560\n",
      ".\n",
      "Batch 56 processed.\n",
      "Processing Review 561\n",
      ".Processing Review 562\n",
      ".Processing Review 563\n",
      ".Processing Review 564\n",
      ".Processing Review 565\n",
      ".Processing Review 566\n",
      ".Processing Review 567\n",
      ".Processing Review 568\n",
      ".Processing Review 569\n",
      ".Processing Review 570\n",
      ".\n",
      "Batch 57 processed.\n",
      "Processing Review 571\n",
      ".Processing Review 572\n",
      ".Processing Review 573\n",
      ".Processing Review 574\n",
      ".Processing Review 575\n",
      ".Processing Review 576\n",
      ".Processing Review 577\n",
      ".Processing Review 578\n",
      ".Processing Review 579\n",
      ".Processing Review 580\n",
      ".\n",
      "Batch 58 processed.\n",
      "Processing Review 581\n",
      ".Processing Review 582\n",
      ".Processing Review 583\n",
      ".Processing Review 584\n",
      ".Processing Review 585\n",
      ".Processing Review 586\n",
      ".Processing Review 587\n",
      ".Processing Review 588\n",
      ".Processing Review 589\n",
      ".Processing Review 590\n",
      ".\n",
      "Batch 59 processed.\n",
      "Processing Review 591\n",
      ".Processing Review 592\n",
      ".Processing Review 593\n",
      ".Processing Review 594\n",
      ".Processing Review 595\n",
      ".Processing Review 596\n",
      ".Processing Review 597\n",
      ".Processing Review 598\n",
      ".Processing Review 599\n",
      ".Processing Review 600\n",
      ".\n",
      "Batch 60 processed.\n",
      "Embeddings saved.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T23:31:26.606111Z",
     "start_time": "2024-11-13T23:31:26.595911Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "d0cce26e67baa3ad",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Convert to Table\n",
    "#### One review can have multiple topics. If we want a table structure, we need to have every topic in one row, essentially duplicating the review information."
   ],
   "id": "2cff3c935b08e1ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T23:31:37.756867Z",
     "start_time": "2024-11-13T23:31:31.405494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "b_to_table = True\n",
    "\n",
    "if b_to_table:\n",
    "    # Initialize an empty DataFrame to hold all topics\n",
    "    df_total = pd.DataFrame()\n",
    "    \n",
    "    # Load the JSON data with embeddings\n",
    "    with open(s_root + s_db_embed_json, 'r', encoding='utf-8') as f:\n",
    "        d_review_output = json.load(f)\n",
    "        print(\"Loaded JSON with embeddings\")\n",
    "\n",
    "    # Iterate over each review entry in the JSON data\n",
    "    for review_entry in d_review_output:\n",
    "        # Check if 'topics' exists in each review entry\n",
    "        if 'topics' in review_entry and isinstance(review_entry['topics'], list):\n",
    "            # Create a DataFrame for the current review's topics\n",
    "            df_gp = pd.DataFrame(review_entry['topics'])\n",
    "            \n",
    "            # Add additional columns from the review entry\n",
    "            for key, value in review_entry.items():\n",
    "                if key != 'topics':  # Skip the topics column itself\n",
    "                    df_gp[key] = value  # Assign each additional field to each row in df_gp\n",
    "\n",
    "            # Concatenate this review's DataFrame to the total DataFrame\n",
    "            df_total = pd.concat([df_total, df_gp], ignore_index=True)\n",
    "    \n",
    "    # Save the combined DataFrame to JSON and Excel\n",
    "    df_total.to_json(s_root + s_db_table_json, orient='records')\n",
    "    df_total.to_excel(s_root + s_db_table_xlsx, index=False)\n",
    "    print(\"Data saved to JSON and Excel\")\n"
   ],
   "id": "c6329951970368d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded JSON with embeddings\n",
      "Data saved to JSON and Excel\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PCA",
   "id": "fd1cf82d8b9c761b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "b_pca_kmeans = True\n",
    "b_update_kmeans_centers = True\n",
    "\n",
    "if b_pca_kmeans:\n",
    "    keyword = 'zombie'\n",
    "    \n",
    "    # Load existing K-means centers if available\n",
    "    if os.path.isfile(s_root + s_kmeans_centers):\n",
    "        with open(s_root + s_kmeans_centers, 'r') as f:\n",
    "            d_kmeans_centers = json.load(f)\n",
    "    else:\n",
    "        d_kmeans_centers = {}\n",
    "\n",
    "    # Load the DataFrame with embeddings\n",
    "    df_total = pd.read_json(s_root + s_db_table_json, orient='records')\n",
    "    df_total = df_total[df_total['embedding'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "    \n",
    "    # Convert embeddings to matrix\n",
    "    mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "    # Embed the keyword and compute similarity for each entry\n",
    "    keyword_embed = index_embedding(keyword)\n",
    "    df_total['similarity'] = mat @ keyword_embed  # Dot product for cosine similarity\n",
    "\n",
    "    # Define number of clusters for main clustering and sub-clustering\n",
    "    n_clusters = 20\n",
    "    n_clusters_sub = 2\n",
    "    s_key = str(n_clusters)\n",
    "    \n",
    "    # Initialize K-means clusters or use precomputed centers\n",
    "    if s_key in d_kmeans_centers:\n",
    "        mat_init = np.array(d_kmeans_centers[s_key])\n",
    "    else:\n",
    "        mat_init = np.zeros((n_clusters, mat.shape[1]))\n",
    "        np.fill_diagonal(mat_init, 1.0)\n",
    "    \n",
    "    # Main K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=mat_init, n_init=1).fit(mat)\n",
    "    df_total['kmeans'] = kmeans.labels_\n",
    "    \n",
    "    if b_update_kmeans_centers:\n",
    "        d_kmeans_centers[s_key] = kmeans.cluster_centers_.tolist()\n",
    "\n",
    "    # Sub-clustering for larger clusters\n",
    "    df_total['kmeans_sub'] = 0\n",
    "    for i_c in range(n_clusters):\n",
    "        cluster_indices = np.where(df_total['kmeans'] == i_c)[0]\n",
    "        if len(cluster_indices) > len(df_total) / n_clusters:\n",
    "            mat_i_c = mat[cluster_indices, :]\n",
    "            s_key_sub = f\"{n_clusters}_{i_c}_{n_clusters_sub}\"\n",
    "    \n",
    "            # Adjust n_clusters_sub if fewer samples than sub-clusters\n",
    "            adjusted_n_clusters_sub = min(n_clusters_sub, len(mat_i_c))\n",
    "    \n",
    "            if s_key_sub in d_kmeans_centers and adjusted_n_clusters_sub == n_clusters_sub:\n",
    "                mat_init_sub = np.array(d_kmeans_centers[s_key_sub])\n",
    "            else:\n",
    "                mat_init_sub = np.zeros((adjusted_n_clusters_sub, mat.shape[1]))\n",
    "                np.fill_diagonal(mat_init_sub, 1.0)\n",
    "    \n",
    "            # Apply KMeans with adjusted number of clusters\n",
    "            kmeans_sub = KMeans(n_clusters=adjusted_n_clusters_sub, init=mat_init_sub, n_init=1).fit(mat_i_c)\n",
    "            df_total.loc[cluster_indices, 'kmeans_sub'] = kmeans_sub.labels_\n",
    "    \n",
    "            if b_update_kmeans_centers and adjusted_n_clusters_sub == n_clusters_sub:\n",
    "                d_kmeans_centers[s_key_sub] = kmeans_sub.cluster_centers_.tolist()\n",
    "\n",
    "\n",
    "    # Dimensionality Reduction Techniques\n",
    "    methods = [\n",
    "        ('PCA', PCA(n_components=3)),\n",
    "        ('t-SNE', TSNE(n_components=2)),\n",
    "        ('UMAP', UMAP(n_components=2))\n",
    "    ]\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    for i, (name, model) in enumerate(methods):\n",
    "        print(name)\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "\n",
    "        if name == 'PCA':\n",
    "            X_embedded = model.fit_transform(mat)\n",
    "            df_total['first_dim_PCA'] = X_embedded[:, 0]\n",
    "            df_total['second_dim_PCA'] = X_embedded[:, 1]\n",
    "            df_total['third_dim_PCA'] = X_embedded[:, 2]\n",
    "            plt.scatter(X_embedded[:, 1], X_embedded[:, 2], c=df_total['kmeans'], cmap='tab20')\n",
    "        else:\n",
    "            X_embedded = model.fit_transform(mat)\n",
    "            df_total[f'first_dim_{name}'] = X_embedded[:, 0]\n",
    "            df_total[f'second_dim_{name}'] = X_embedded[:, 1]\n",
    "            plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=df_total['kmeans'], cmap='tab20')\n",
    "\n",
    "        plt.title(f\"{name} Visualization\")\n",
    "        plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save K-means centers and DataFrame with clustering results\n",
    "    with open(s_root + s_kmeans_centers, 'w') as f:\n",
    "        json.dump(d_kmeans_centers, f)\n",
    "\n",
    "    df_total.to_json(s_root + s_db_table_pca_json, orient='records')\n",
    "    df_total.to_excel(s_root + s_db_table_pca_xlsx, index=False)\n",
    "    print(\"Clustering and dimensionality reduction results saved.\")\n"
   ],
   "id": "add640814eaeb24a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_total.head()",
   "id": "2743c78ec6f2d4f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HDBSCAN",
   "id": "4c24e1b0cfcca19c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T23:32:13.089894Z",
     "start_time": "2024-11-13T23:31:56.491144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "# Paths and parameters\n",
    "s_root = r'C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/'\n",
    "s_db_table_json = 'Data/review_db_table.json'  # Input JSON with embeddings\n",
    "s_db_table_hdbscan_json = 'Data/review_db_table_hdbscan.json'  # Output JSON with UMAP + HDBSCAN clusters\n",
    "keyword = 'zombie'  # Keyword for similarity calculation\n",
    "\n",
    "# Load DataFrame with embeddings\n",
    "df_total = pd.read_json(s_root + s_db_table_json, orient='records')\n",
    "df_total = df_total[df_total['embedding'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "# Convert embeddings to matrix\n",
    "mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "# Embed the keyword and compute similarity for each entry\n",
    "def index_embedding(keyword):\n",
    "    # Random embedding for the keyword\n",
    "    return np.random.rand(mat.shape[1])\n",
    "\n",
    "keyword_embed = index_embedding(keyword)\n",
    "df_total['similarity'] = mat @ keyword_embed  # Dot product for cosine similarity\n",
    "\n",
    "# UMAP Dimensionality Reduction (3D)\n",
    "umap_model = umap.UMAP(n_components=3, random_state=42)\n",
    "umap_embeddings = umap_model.fit_transform(mat)\n",
    "df_total['umap_x'] = umap_embeddings[:, 0]\n",
    "df_total['umap_y'] = umap_embeddings[:, 1]\n",
    "df_total['umap_z'] = umap_embeddings[:, 2]  # Third dimension for optional 3D visualization\n",
    "\n",
    "# HDBSCAN Clustering\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3, cluster_selection_epsilon=0.5)\n",
    "cluster_labels = clusterer.fit_predict(umap_embeddings)\n",
    "df_total['cluster_id'] = cluster_labels\n",
    "\n",
    "# Save DataFrame with UMAP and HDBSCAN results to JSON\n",
    "df_total.to_json(s_root + s_db_table_hdbscan_json, orient='records')\n",
    "print(f\"3D UMAP and HDBSCAN clustering results with similarity saved to {s_root + s_db_table_hdbscan_json}\")\n"
   ],
   "id": "fdbe09910c43b59e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbohm\\Documents\\Venvironments\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D UMAP and HDBSCAN clustering results with similarity saved to C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/Data/review_db_table_hdbscan.json\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T23:32:16.020225Z",
     "start_time": "2024-11-13T23:32:15.996883Z"
    }
   },
   "cell_type": "code",
   "source": "df_total.head()",
   "id": "fd4dcdd834c412af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Please rate your overall experience playing Into the Dead: Our Darkest Days  \\\n",
       "0                                                 10                             \n",
       "1                                                 10                             \n",
       "2                                                 10                             \n",
       "3                                                  8                             \n",
       "4                                                  8                             \n",
       "\n",
       "      Please tell us why you chose the rating above:  \\\n",
       "0                               Nice story,nice game   \n",
       "1                               Nice story,nice game   \n",
       "2                               Nice story,nice game   \n",
       "3  Reminds me a lot of This War of Mine, a game I...   \n",
       "4  Reminds me a lot of This War of Mine, a game I...   \n",
       "\n",
       "  If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?  \\\n",
       "0  when move to another shelter, suplies which di...                                                               \n",
       "1  when move to another shelter, suplies which di...                                                               \n",
       "2  when move to another shelter, suplies which di...                                                               \n",
       "3  Need a tutorial or guide because I got stuck a...                                                               \n",
       "4  Need a tutorial or guide because I got stuck a...                                                               \n",
       "\n",
       "  Had you heard of Into the Dead before this demo? What is your age group  \\\n",
       "0                                      Heard of it                  18-24   \n",
       "1                                      Heard of it                  18-24   \n",
       "2                                      Heard of it                  18-24   \n",
       "3                                Never heard of it                  45-54   \n",
       "4                                Never heard of it                  45-54   \n",
       "\n",
       "  What is your gender?  \\\n",
       "0                  Man   \n",
       "1                  Man   \n",
       "2                  Man   \n",
       "3                Woman   \n",
       "4                Woman   \n",
       "\n",
       "  What are your favourite Steam games you have played in the last 3 months?  \\\n",
       "0          CS2, callisto protocal, lockdown protocal                          \n",
       "1          CS2, callisto protocal, lockdown protocal                          \n",
       "2          CS2, callisto protocal, lockdown protocal                          \n",
       "3           Cozy Grove, Death Stranding, I am Future                          \n",
       "4           Cozy Grove, Death Stranding, I am Future                          \n",
       "\n",
       "   ID language     topic sentiment category  \\\n",
       "0   2  english     Story  Positive     fact   \n",
       "1   2  english   Shelter  Negative     fact   \n",
       "2   2  english  Supplies  Negative  request   \n",
       "3   3  english  Graphics  Positive     fact   \n",
       "4   3  english  Tutorial  Negative  request   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                             Nice story, nice game.   \n",
       "1  When moving to another shelter, supplies which...   \n",
       "2  They should be added to player's storage since...   \n",
       "3  Your game has more detailed graphics and in co...   \n",
       "4  Need a tutorial or guide because I got stuck a...   \n",
       "\n",
       "                                           embedding  similarity     umap_x  \\\n",
       "0  [0.0332841799, 0.0589793995, 0.0548812076, 0.0...    0.402232  11.984763   \n",
       "1  [0.0102353394, 0.1051071808, 0.0266069323, 0.1...   -0.209047   9.949238   \n",
       "2  [-0.027585186100000002, 0.029896769700000003, ...   -0.091792   8.315664   \n",
       "3  [-0.012894654600000001, -0.0043537645, -0.0294...    0.491395   5.229348   \n",
       "4  [-0.0567355193, -0.0038657677000000002, -0.053...    0.813603   8.222432   \n",
       "\n",
       "      umap_y     umap_z  cluster_id  \n",
       "0  11.016189  10.425184          86  \n",
       "1   6.057717  11.481344          92  \n",
       "2   6.911914   8.666392          90  \n",
       "3   3.590627  31.203501           8  \n",
       "4  -6.434734   8.722463          11  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Please rate your overall experience playing Into the Dead: Our Darkest Days</th>\n",
       "      <th>Please tell us why you chose the rating above:</th>\n",
       "      <th>If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?</th>\n",
       "      <th>Had you heard of Into the Dead before this demo?</th>\n",
       "      <th>What is your age group</th>\n",
       "      <th>What is your gender?</th>\n",
       "      <th>What are your favourite Steam games you have played in the last 3 months?</th>\n",
       "      <th>ID</th>\n",
       "      <th>language</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>category</th>\n",
       "      <th>sentence</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "      <th>umap_x</th>\n",
       "      <th>umap_y</th>\n",
       "      <th>umap_z</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Nice story,nice game</td>\n",
       "      <td>when move to another shelter, suplies which di...</td>\n",
       "      <td>Heard of it</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Man</td>\n",
       "      <td>CS2, callisto protocal, lockdown protocal</td>\n",
       "      <td>2</td>\n",
       "      <td>english</td>\n",
       "      <td>Story</td>\n",
       "      <td>Positive</td>\n",
       "      <td>fact</td>\n",
       "      <td>Nice story, nice game.</td>\n",
       "      <td>[0.0332841799, 0.0589793995, 0.0548812076, 0.0...</td>\n",
       "      <td>0.402232</td>\n",
       "      <td>11.984763</td>\n",
       "      <td>11.016189</td>\n",
       "      <td>10.425184</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Nice story,nice game</td>\n",
       "      <td>when move to another shelter, suplies which di...</td>\n",
       "      <td>Heard of it</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Man</td>\n",
       "      <td>CS2, callisto protocal, lockdown protocal</td>\n",
       "      <td>2</td>\n",
       "      <td>english</td>\n",
       "      <td>Shelter</td>\n",
       "      <td>Negative</td>\n",
       "      <td>fact</td>\n",
       "      <td>When moving to another shelter, supplies which...</td>\n",
       "      <td>[0.0102353394, 0.1051071808, 0.0266069323, 0.1...</td>\n",
       "      <td>-0.209047</td>\n",
       "      <td>9.949238</td>\n",
       "      <td>6.057717</td>\n",
       "      <td>11.481344</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Nice story,nice game</td>\n",
       "      <td>when move to another shelter, suplies which di...</td>\n",
       "      <td>Heard of it</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Man</td>\n",
       "      <td>CS2, callisto protocal, lockdown protocal</td>\n",
       "      <td>2</td>\n",
       "      <td>english</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>Negative</td>\n",
       "      <td>request</td>\n",
       "      <td>They should be added to player's storage since...</td>\n",
       "      <td>[-0.027585186100000002, 0.029896769700000003, ...</td>\n",
       "      <td>-0.091792</td>\n",
       "      <td>8.315664</td>\n",
       "      <td>6.911914</td>\n",
       "      <td>8.666392</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Reminds me a lot of This War of Mine, a game I...</td>\n",
       "      <td>Need a tutorial or guide because I got stuck a...</td>\n",
       "      <td>Never heard of it</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Woman</td>\n",
       "      <td>Cozy Grove, Death Stranding, I am Future</td>\n",
       "      <td>3</td>\n",
       "      <td>english</td>\n",
       "      <td>Graphics</td>\n",
       "      <td>Positive</td>\n",
       "      <td>fact</td>\n",
       "      <td>Your game has more detailed graphics and in co...</td>\n",
       "      <td>[-0.012894654600000001, -0.0043537645, -0.0294...</td>\n",
       "      <td>0.491395</td>\n",
       "      <td>5.229348</td>\n",
       "      <td>3.590627</td>\n",
       "      <td>31.203501</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Reminds me a lot of This War of Mine, a game I...</td>\n",
       "      <td>Need a tutorial or guide because I got stuck a...</td>\n",
       "      <td>Never heard of it</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Woman</td>\n",
       "      <td>Cozy Grove, Death Stranding, I am Future</td>\n",
       "      <td>3</td>\n",
       "      <td>english</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>Negative</td>\n",
       "      <td>request</td>\n",
       "      <td>Need a tutorial or guide because I got stuck a...</td>\n",
       "      <td>[-0.0567355193, -0.0038657677000000002, -0.053...</td>\n",
       "      <td>0.813603</td>\n",
       "      <td>8.222432</td>\n",
       "      <td>-6.434734</td>\n",
       "      <td>8.722463</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Kmeans",
   "id": "864d21786605d50e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T00:00:03.328340Z",
     "start_time": "2024-11-13T23:59:32.450765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Paths and parameters\n",
    "s_root = r'C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/'\n",
    "s_db_table_json = 'Data/review_db_table.json'  # Input JSON with embeddings\n",
    "s_db_table_hdbscan_json = 'Data/review_db_table_hdbscan.json'  # Output JSON with UMAP + HDBSCAN clusters\n",
    "keyword = 'zombie'  # Keyword for similarity calculation\n",
    "n_kmeans_clusters = 20\n",
    "\n",
    "# Load DataFrame with embeddings\n",
    "df_total = pd.read_json(s_root + s_db_table_json, orient='records')\n",
    "df_total = df_total[df_total['embedding'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "# Convert embeddings to matrix\n",
    "mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "# Embed the keyword and compute similarity for each entry\n",
    "def index_embedding(keyword):\n",
    "    return np.random.rand(mat.shape[1])  # Example embedding; replace with real embedding function\n",
    "\n",
    "keyword_embed = index_embedding(keyword)\n",
    "df_total['similarity'] = mat @ keyword_embed  # Dot product for cosine similarity\n",
    "\n",
    "# Dimensionality Reduction Techniques\n",
    "# UMAP (3D)\n",
    "umap_model = umap.UMAP(n_components=3, random_state=42)\n",
    "umap_embeddings = umap_model.fit_transform(mat)\n",
    "df_total['umap_x'] = umap_embeddings[:, 0]\n",
    "df_total['umap_y'] = umap_embeddings[:, 1]\n",
    "df_total['umap_z'] = umap_embeddings[:, 2]\n",
    "\n",
    "# PCA (3D)\n",
    "pca_model = PCA(n_components=3)\n",
    "pca_embeddings = pca_model.fit_transform(mat)\n",
    "df_total['pca_x'] = pca_embeddings[:, 0]\n",
    "df_total['pca_y'] = pca_embeddings[:, 1]\n",
    "df_total['pca_z'] = pca_embeddings[:, 2]\n",
    "\n",
    "# t-SNE (3D)\n",
    "tsne_model = TSNE(n_components=3, random_state=42)\n",
    "tsne_embeddings = tsne_model.fit_transform(mat)\n",
    "df_total['tsne_x'] = tsne_embeddings[:, 0]\n",
    "df_total['tsne_y'] = tsne_embeddings[:, 1]\n",
    "df_total['tsne_z'] = tsne_embeddings[:, 2]\n",
    "\n",
    "# HDBSCAN Clustering\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3, cluster_selection_epsilon=0.5)\n",
    "hdbscan_labels = hdbscan_clusterer.fit_predict(umap_embeddings)  # Clustering on UMAP reduced space\n",
    "df_total['hdbscan_cluster_id'] = hdbscan_labels\n",
    "\n",
    "# KMeans Clustering (on original embeddings)\n",
    "kmeans_model = KMeans(n_clusters=n_kmeans_clusters, random_state=42)\n",
    "kmeans_labels = kmeans_model.fit_predict(mat)\n",
    "df_total['kmeans_cluster_id'] = kmeans_labels\n",
    "\n",
    "# Save DataFrame with all results to JSON\n",
    "output_path = s_root + s_db_table_hdbscan_json\n",
    "df_total.to_json(output_path, orient='records')\n",
    "print(f\"3D UMAP, PCA, t-SNE and clustering results (HDBSCAN and KMeans) saved to {output_path}\")\n"
   ],
   "id": "9cb280a7eb1baad3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fbohm\\Documents\\Venvironments\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\fbohm\\Documents\\Venvironments\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D UMAP, PCA, t-SNE and clustering results (HDBSCAN and KMeans) saved to C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/Data/review_db_table_hdbscan.json\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Name the Clusters",
   "id": "13535c2c7c8f1131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T00:47:39.305206Z",
     "start_time": "2024-11-14T00:47:24.254102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Define paths and parameters\n",
    "s_root = r'C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/'\n",
    "s_db_table_hdbscan_json = 'Data/review_db_table_hdbscan.json'\n",
    "keyword = 'zombie'  # Example keyword for similarity calculation\n",
    "\n",
    "# Load precomputed data\n",
    "df_total = pd.read_json(s_root + s_db_table_hdbscan_json, orient='records')\n",
    "mat = np.array(df_total['embedding'].tolist())\n",
    "\n",
    "# Step 1: Find Representative Topics for Each Cluster\n",
    "cluster_names = {}\n",
    "unique_clusters = df_total['kmeans_cluster_id'].unique()\n",
    "\n",
    "# Define a prompt template for cluster naming\n",
    "prompt_template_cluster_naming = PromptTemplate.from_template(\n",
    "'''Based on the following topics, generate a concise name (5 words or fewer) that best describes the general theme of this cluster.\n",
    "\n",
    "TOPICS: {topics}\n",
    "CLUSTER NAME: '''\n",
    ")\n",
    "\n",
    "def find_representative_topics(cluster_id, df, mat, max_topics=8):\n",
    "    \"\"\"Finds up to max_topics representative topics based on centroid proximity.\"\"\"\n",
    "    cluster_data = df[df['kmeans_cluster_id'] == cluster_id]\n",
    "    cluster_embeddings = np.array(cluster_data['embedding'].tolist())\n",
    "    centroid = np.mean(cluster_embeddings, axis=0)\n",
    "    distances = cosine_distances([centroid], cluster_embeddings).flatten()\n",
    "    closest_indices = np.argsort(distances)[:max_topics]\n",
    "    return cluster_data.iloc[closest_indices]['topic'].tolist()\n",
    "\n",
    "# Step 2: Generate Cluster Names Using LLM\n",
    "for cluster_id in unique_clusters:\n",
    "    topics = find_representative_topics(cluster_id, df_total, mat)\n",
    "    prompt = f\"Generate a concise name (5 words or fewer) for a cluster with these topics: {', '.join(topics)}\"\n",
    "    \n",
    "    # API call to generate cluster names\n",
    "    def generate_cluster_name(topics_list):\n",
    "        # Format topics as a comma-separated string\n",
    "        topics = \", \".join(topics_list)\n",
    "    \n",
    "        # Generate the prompt for the cluster name\n",
    "        prompt_cluster_naming = prompt_template_cluster_naming.format(topics=topics)\n",
    "        \n",
    "        # API call to OpenAI's completion model\n",
    "        cluster_name_response = client.chat.completions.create(\n",
    "            model=chat_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at summarizing topics into concise names.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_cluster_naming},\n",
    "            ],\n",
    "            max_tokens=100  # Adjust tokens to limit response length\n",
    "        )\n",
    "    \n",
    "        # Extract and return the generated cluster name\n",
    "        cluster_name = cluster_name_response.choices[0].message.content.strip()\n",
    "        return cluster_name\n",
    "    \n",
    "    cluster_name = generate_cluster_name(prompt)\n",
    "    cluster_names[cluster_id] = cluster_name\n",
    "\n",
    "# Step 3: Save Cluster Names to JSON\n",
    "df_total['cluster_name'] = df_total['kmeans_cluster_id'].map(cluster_names)\n",
    "df_total.to_json(s_root + s_db_table_hdbscan_json, orient='records')\n",
    "print(f\"Cluster names saved to {s_root + s_db_table_hdbscan_json}\")\n"
   ],
   "id": "da8268ca3380ca9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 13:47:25,322 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:25,823 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:26,292 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:26,945 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:27,436 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:27,883 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:28,462 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:28,918 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:31,298 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:31,935 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:32,423 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:32,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:33,379 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:33,958 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:34,460 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:35,038 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:35,827 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:36,442 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:38,667 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-14 13:47:39,165 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster names saved to C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/Data/review_db_table_hdbscan.json\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T00:46:49.091348Z",
     "start_time": "2024-11-14T00:46:49.073437Z"
    }
   },
   "cell_type": "code",
   "source": "df_total.head()",
   "id": "c3574d56cf87324b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Please rate your overall experience playing Into the Dead: Our Darkest Days  \\\n",
       "0                                                 10                             \n",
       "1                                                 10                             \n",
       "2                                                 10                             \n",
       "3                                                  8                             \n",
       "4                                                  8                             \n",
       "\n",
       "      Please tell us why you chose the rating above:  \\\n",
       "0                               Nice story,nice game   \n",
       "1                               Nice story,nice game   \n",
       "2                               Nice story,nice game   \n",
       "3  Reminds me a lot of This War of Mine, a game I...   \n",
       "4  Reminds me a lot of This War of Mine, a game I...   \n",
       "\n",
       "  If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?  \\\n",
       "0  when move to another shelter, suplies which di...                                                               \n",
       "1  when move to another shelter, suplies which di...                                                               \n",
       "2  when move to another shelter, suplies which di...                                                               \n",
       "3  Need a tutorial or guide because I got stuck a...                                                               \n",
       "4  Need a tutorial or guide because I got stuck a...                                                               \n",
       "\n",
       "  Had you heard of Into the Dead before this demo? What is your age group  \\\n",
       "0                                      Heard of it                  18-24   \n",
       "1                                      Heard of it                  18-24   \n",
       "2                                      Heard of it                  18-24   \n",
       "3                                Never heard of it                  45-54   \n",
       "4                                Never heard of it                  45-54   \n",
       "\n",
       "  What is your gender?  \\\n",
       "0                  Man   \n",
       "1                  Man   \n",
       "2                  Man   \n",
       "3                Woman   \n",
       "4                Woman   \n",
       "\n",
       "  What are your favourite Steam games you have played in the last 3 months?  \\\n",
       "0          CS2, callisto protocal, lockdown protocal                          \n",
       "1          CS2, callisto protocal, lockdown protocal                          \n",
       "2          CS2, callisto protocal, lockdown protocal                          \n",
       "3           Cozy Grove, Death Stranding, I am Future                          \n",
       "4           Cozy Grove, Death Stranding, I am Future                          \n",
       "\n",
       "   ID language     topic  ...     umap_y     umap_z     pca_x     pca_y  \\\n",
       "0   2  english     Story  ...  11.016189  10.425184  0.028941 -0.027230   \n",
       "1   2  english   Shelter  ...   6.057717  11.481344  0.084692 -0.370481   \n",
       "2   2  english  Supplies  ...   6.911914   8.666392 -0.027137 -0.029577   \n",
       "3   3  english  Graphics  ...   3.590627  31.203501  0.199391  0.378013   \n",
       "4   3  english  Tutorial  ...  -6.434734   8.722463  0.026106  0.075713   \n",
       "\n",
       "      pca_z     tsne_x     tsne_y     tsne_z  hdbscan_cluster_id  \\\n",
       "0  0.110504  11.269978   9.573381 -10.489752                  86   \n",
       "1  0.150167  -1.712859 -23.507380   8.990909                  92   \n",
       "2  0.237652 -16.929251   2.470290   4.071044                  90   \n",
       "3  0.071529  15.819509  17.815710   6.263166                   8   \n",
       "4  0.176829   7.549045  20.893099  22.751272                  11   \n",
       "\n",
       "   kmeans_cluster_id  \n",
       "0                  5  \n",
       "1                  5  \n",
       "2                 13  \n",
       "3                  1  \n",
       "4                 14  \n",
       "\n",
       "[5 rows x 26 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Please rate your overall experience playing Into the Dead: Our Darkest Days</th>\n",
       "      <th>Please tell us why you chose the rating above:</th>\n",
       "      <th>If you had a magic wand and you could change, add, or remove anything from the game, what would it be and why?</th>\n",
       "      <th>Had you heard of Into the Dead before this demo?</th>\n",
       "      <th>What is your age group</th>\n",
       "      <th>What is your gender?</th>\n",
       "      <th>What are your favourite Steam games you have played in the last 3 months?</th>\n",
       "      <th>ID</th>\n",
       "      <th>language</th>\n",
       "      <th>topic</th>\n",
       "      <th>...</th>\n",
       "      <th>umap_y</th>\n",
       "      <th>umap_z</th>\n",
       "      <th>pca_x</th>\n",
       "      <th>pca_y</th>\n",
       "      <th>pca_z</th>\n",
       "      <th>tsne_x</th>\n",
       "      <th>tsne_y</th>\n",
       "      <th>tsne_z</th>\n",
       "      <th>hdbscan_cluster_id</th>\n",
       "      <th>kmeans_cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Nice story,nice game</td>\n",
       "      <td>when move to another shelter, suplies which di...</td>\n",
       "      <td>Heard of it</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Man</td>\n",
       "      <td>CS2, callisto protocal, lockdown protocal</td>\n",
       "      <td>2</td>\n",
       "      <td>english</td>\n",
       "      <td>Story</td>\n",
       "      <td>...</td>\n",
       "      <td>11.016189</td>\n",
       "      <td>10.425184</td>\n",
       "      <td>0.028941</td>\n",
       "      <td>-0.027230</td>\n",
       "      <td>0.110504</td>\n",
       "      <td>11.269978</td>\n",
       "      <td>9.573381</td>\n",
       "      <td>-10.489752</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Nice story,nice game</td>\n",
       "      <td>when move to another shelter, suplies which di...</td>\n",
       "      <td>Heard of it</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Man</td>\n",
       "      <td>CS2, callisto protocal, lockdown protocal</td>\n",
       "      <td>2</td>\n",
       "      <td>english</td>\n",
       "      <td>Shelter</td>\n",
       "      <td>...</td>\n",
       "      <td>6.057717</td>\n",
       "      <td>11.481344</td>\n",
       "      <td>0.084692</td>\n",
       "      <td>-0.370481</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>-1.712859</td>\n",
       "      <td>-23.507380</td>\n",
       "      <td>8.990909</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Nice story,nice game</td>\n",
       "      <td>when move to another shelter, suplies which di...</td>\n",
       "      <td>Heard of it</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Man</td>\n",
       "      <td>CS2, callisto protocal, lockdown protocal</td>\n",
       "      <td>2</td>\n",
       "      <td>english</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>...</td>\n",
       "      <td>6.911914</td>\n",
       "      <td>8.666392</td>\n",
       "      <td>-0.027137</td>\n",
       "      <td>-0.029577</td>\n",
       "      <td>0.237652</td>\n",
       "      <td>-16.929251</td>\n",
       "      <td>2.470290</td>\n",
       "      <td>4.071044</td>\n",
       "      <td>90</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Reminds me a lot of This War of Mine, a game I...</td>\n",
       "      <td>Need a tutorial or guide because I got stuck a...</td>\n",
       "      <td>Never heard of it</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Woman</td>\n",
       "      <td>Cozy Grove, Death Stranding, I am Future</td>\n",
       "      <td>3</td>\n",
       "      <td>english</td>\n",
       "      <td>Graphics</td>\n",
       "      <td>...</td>\n",
       "      <td>3.590627</td>\n",
       "      <td>31.203501</td>\n",
       "      <td>0.199391</td>\n",
       "      <td>0.378013</td>\n",
       "      <td>0.071529</td>\n",
       "      <td>15.819509</td>\n",
       "      <td>17.815710</td>\n",
       "      <td>6.263166</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Reminds me a lot of This War of Mine, a game I...</td>\n",
       "      <td>Need a tutorial or guide because I got stuck a...</td>\n",
       "      <td>Never heard of it</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Woman</td>\n",
       "      <td>Cozy Grove, Death Stranding, I am Future</td>\n",
       "      <td>3</td>\n",
       "      <td>english</td>\n",
       "      <td>Tutorial</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.434734</td>\n",
       "      <td>8.722463</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>0.075713</td>\n",
       "      <td>0.176829</td>\n",
       "      <td>7.549045</td>\n",
       "      <td>20.893099</td>\n",
       "      <td>22.751272</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embedd the sentence not the topic \"title\"",
   "id": "d6b6133df7fc954"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Paths and parameters\n",
    "s_root = r'C:\\Users\\fbohm\\Desktop\\Projects\\DataScience\\cluster_analysis/'\n",
    "s_db_json_sample = 'Data/sample_survey_results_with_topics.json'  # Input JSON file\n",
    "s_db_embed_json = 'Data/review_db_table_with_sentence_embeddings.json'  # Output JSON file with embeddings\n",
    "batch_size = 10  # Adjust based on available memory and dataset size\n",
    "b_embedding = True\n",
    "b_override = True  # Set to True if you want to overwrite existing embeddings\n",
    "\n",
    "# Define your sentence embedding function\n",
    "def index_embedding(sentence):\n",
    "    # Placeholder for sentence embedding; replace with actual embedding model as needed\n",
    "    return np.random.rand(300).tolist()  # Assuming 300 dimensions for embedding\n",
    "\n",
    "# Load data and process in batches\n",
    "if b_embedding:\n",
    "    with open(os.path.join(s_root, s_db_json_sample), 'r', encoding='utf-8') as f:\n",
    "        d_review_output = json.load(f)\n",
    "        print('Loaded JSON data')\n",
    "\n",
    "    for batch_start in range(0, len(d_review_output), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(d_review_output))\n",
    "        batch = d_review_output[batch_start:batch_end]\n",
    "        \n",
    "        for i, review_entry in enumerate(batch):\n",
    "            print(f\"Processing Review {i + batch_start + 1}\")\n",
    "            \n",
    "            if isinstance(review_entry, dict) and 'topics' in review_entry and isinstance(review_entry['topics'], list):\n",
    "                d_topics = review_entry['topics']\n",
    "                \n",
    "                for d_topic in d_topics:\n",
    "                    if isinstance(d_topic, dict):\n",
    "                        if 'embedding' not in d_topic or b_override:\n",
    "                            if 'sentence' in d_topic:  # Embed the sentence instead of the topic\n",
    "                                d_topic['embedding'] = index_embedding(d_topic['sentence'])\n",
    "                                \n",
    "                                # Release memory\n",
    "                                torch.cuda.empty_cache()\n",
    "                                gc.collect()\n",
    "                            else:\n",
    "                                d_topic['embedding'] = 0\n",
    "                print('.', end='')\n",
    "        print(f\"\\nBatch {batch_start // batch_size + 1} processed.\")\n",
    "\n",
    "    # Save updated JSON with embeddings\n",
    "    with open(os.path.join(s_root, s_db_embed_json), 'w', encoding='utf-8') as f:\n",
    "        json.dump(d_review_output, f)\n",
    "    print(\"Embeddings saved.\")\n"
   ],
   "id": "9f730089bedeb383"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#Check the results (why were they so fast?)",
   "id": "9b43b023e89a7ee3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
